{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae67808-af4b-471d-b3ee-735adea97509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier  # 분류분석\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import adjusted_rand_score, homogeneity_score, completeness_score, v_measure_score, mutual_info_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "filepath=r'E:\\ai\\Downloads\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79164f-7bb1-44f9-82c1-6eda781343b8",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0587ba45-2468-470b-87cd-87404cc84e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80158, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG_vectors = np.load(os.path.join(filepath,'VGG_vectors.npy'))\n",
    "VGG_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47dea60f-8c13-4c81-9ff9-837166009946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Realism/vincent-van-gogh_pine-trees-in-the-fen...</td>\n",
       "      <td>pine-trees-in-the-fen-1884</td>\n",
       "      <td>vincent-van-gogh</td>\n",
       "      <td>landscape</td>\n",
       "      <td>Realism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baroque/rembrandt_the-angel-appearing-to-the-s...</td>\n",
       "      <td>the-angel-appearing-to-the-shepherds-1634</td>\n",
       "      <td>rembrandt</td>\n",
       "      <td>religious painting</td>\n",
       "      <td>Baroque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post_Impressionism/paul-cezanne_portrait-of-th...</td>\n",
       "      <td>portrait-of-the-artist-s-son</td>\n",
       "      <td>paul-cezanne</td>\n",
       "      <td>portrait</td>\n",
       "      <td>Post_Impressionism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Impressionism/pierre-auguste-renoir_young-girl...</td>\n",
       "      <td>young-girl-seated-in-a-meadow-1916</td>\n",
       "      <td>pierre-auguste-renoir</td>\n",
       "      <td>genre painting</td>\n",
       "      <td>Impressionism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Romanticism/ivan-aivazovsky_morning-1851.jpg</td>\n",
       "      <td>morning-1851</td>\n",
       "      <td>ivan-aivazovsky</td>\n",
       "      <td>marina</td>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  Realism/vincent-van-gogh_pine-trees-in-the-fen...   \n",
       "1  Baroque/rembrandt_the-angel-appearing-to-the-s...   \n",
       "2  Post_Impressionism/paul-cezanne_portrait-of-th...   \n",
       "3  Impressionism/pierre-auguste-renoir_young-girl...   \n",
       "4       Romanticism/ivan-aivazovsky_morning-1851.jpg   \n",
       "\n",
       "                                       title                 artist  \\\n",
       "0                 pine-trees-in-the-fen-1884       vincent-van-gogh   \n",
       "1  the-angel-appearing-to-the-shepherds-1634              rembrandt   \n",
       "2               portrait-of-the-artist-s-son           paul-cezanne   \n",
       "3         young-girl-seated-in-a-meadow-1916  pierre-auguste-renoir   \n",
       "4                               morning-1851        ivan-aivazovsky   \n",
       "\n",
       "                genre               style  \n",
       "0           landscape             Realism  \n",
       "1  religious painting             Baroque  \n",
       "2            portrait  Post_Impressionism  \n",
       "3      genre painting       Impressionism  \n",
       "4              marina         Romanticism  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "painting = pd.read_csv(os.path.join(filepath,'painting.csv'))\n",
    "painting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ea5e77-010b-4f5f-b990-aaebf63f2b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = VGG_vectors\n",
    "y = painting['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab5e630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X, y = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1031850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# y가 numpy.ndarray일 경우, np.unique()를 사용하여 고유 값들을 가져옵니다\n",
    "unique_classes = np.unique(y)\n",
    "\n",
    "# 각 클래스별로 샘플 수를 200개로 설정\n",
    "rus = RandomUnderSampler(sampling_strategy={class_name: 200 for class_name in unique_classes}, random_state=42)\n",
    "\n",
    "# X와 y에 대해 언더샘플링 적용\n",
    "X,y = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3157139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a.y.-jackson            200\n",
       "martin-barre            200\n",
       "masaccio                200\n",
       "mary-fedden             200\n",
       "mary-cassatt            200\n",
       "                       ... \n",
       "gerrit-dou              200\n",
       "geta-bratescu           200\n",
       "gheorghe-tattarescu     200\n",
       "giacomo-balla           200\n",
       "zinaida-serebriakova    200\n",
       "Name: artist, Length: 1104, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdb81ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(filepath,'X.npy'),X)\n",
    "np.save(os.path.join(filepath,'y.npy'),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffcc8e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.load(os.path.join(filepath,'X.npy'))\n",
    "y = np.load(os.path.join(filepath,'y.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec3016d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154560, 512), (66240, 512), (154560,), (66240,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    shuffle=True,  # 분할 전 데이터 섞기\n",
    "                                                    stratify=y     # 층화추출\n",
    "                                                   )\n",
    "train_X.shape, test_X.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4890ef41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(filepath,'train_x.npy'),train_X)\n",
    "np.save(os.path.join(filepath,'test_x.npy'),test_X)\n",
    "np.save(os.path.join(filepath,'train_y.npy'),train_y)\n",
    "np.save(os.path.join(filepath,'test_y.npy'),test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ace501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X = np.load(os.path.join(filepath,'train_x.npy')).astype('float32')\n",
    "test_X = np.load(os.path.join(filepath,'test_x.npy')).astype('float32')\n",
    "train_y = np.load(os.path.join(filepath,'train_y.npy'), allow_pickle=True)\n",
    "test_y = np.load(os.path.join(filepath,'test_y.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a47899-da31-42cc-9320-bccf8ca5cd5b",
   "metadata": {},
   "source": [
    "# 분류분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e6bbb",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce585247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, max_features='sqrt', random_state=1, n_jobs=-1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f82d19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\ai\\\\Downloads\\\\Data\\\\rf_model.joblib']\n"
     ]
    }
   ],
   "source": [
    "# 대용량 모형일 때 : joblib 파일로 저장 (joblib 라이브러리 사용)\n",
    "print(joblib.dump(rf_model, os.path.join(filepath,'rf_model.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74734e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 load\n",
    "rf_model = joblib.load(os.path.join(filepath,'rf_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1411cd6-94ba-4524-b8ea-075b94ae92b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09726751207729468"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f43984-62cd-4553-aa64-13f8019de2b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <th>aaron-siskind</th>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <th>abidin-dino</th>\n",
       "      <th>adnan-coker</th>\n",
       "      <th>adolf-fleischmann</th>\n",
       "      <th>adriaen-van-de-velde</th>\n",
       "      <th>adriaen-van-de-venne</th>\n",
       "      <th>agnes-martin</th>\n",
       "      <th>agnolo-bronzino</th>\n",
       "      <th>...</th>\n",
       "      <th>william-baziotes</th>\n",
       "      <th>william-blake</th>\n",
       "      <th>william-congdon</th>\n",
       "      <th>william-merritt-chase</th>\n",
       "      <th>william-shayer</th>\n",
       "      <th>wu-guanzhong</th>\n",
       "      <th>xu-beihong</th>\n",
       "      <th>yayoi-kusama</th>\n",
       "      <th>yves-gaucher</th>\n",
       "      <th>yves-klein</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-siskind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abidin-dino</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abraham-manievich</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiannis-tsaroychis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-gaucher</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-klein</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zinaida-serebriakova</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                  a.y.-jackson  aaron-siskind  abdullah-suriosubroto  \\\n",
       "row_0                                                                       \n",
       "a.y.-jackson                      2              0                      0   \n",
       "aaron-siskind                     0              0                      0   \n",
       "abdullah-suriosubroto             0              0                      0   \n",
       "abidin-dino                       0              0                      0   \n",
       "abraham-manievich                 0              0                      0   \n",
       "...                             ...            ...                    ...   \n",
       "yiannis-tsaroychis                0              0                      0   \n",
       "yov-kondzelevych                  0              0                      0   \n",
       "yves-gaucher                      0              0                      0   \n",
       "yves-klein                        0              0                      0   \n",
       "zinaida-serebriakova              0              0                      0   \n",
       "\n",
       "col_0                  abidin-dino  adnan-coker  adolf-fleischmann  \\\n",
       "row_0                                                                \n",
       "a.y.-jackson                     0            0                  0   \n",
       "aaron-siskind                    0            0                  0   \n",
       "abdullah-suriosubroto            0            0                  0   \n",
       "abidin-dino                      8            0                  0   \n",
       "abraham-manievich                0            0                  0   \n",
       "...                            ...          ...                ...   \n",
       "yiannis-tsaroychis               0            0                  0   \n",
       "yov-kondzelevych                 0            0                  0   \n",
       "yves-gaucher                     0            0                  0   \n",
       "yves-klein                       0            0                  0   \n",
       "zinaida-serebriakova             0            1                  0   \n",
       "\n",
       "col_0                  adriaen-van-de-velde  adriaen-van-de-venne  \\\n",
       "row_0                                                               \n",
       "a.y.-jackson                              0                     0   \n",
       "aaron-siskind                             0                     0   \n",
       "abdullah-suriosubroto                     0                     0   \n",
       "abidin-dino                               0                     0   \n",
       "abraham-manievich                         0                     0   \n",
       "...                                     ...                   ...   \n",
       "yiannis-tsaroychis                        0                     0   \n",
       "yov-kondzelevych                          0                     0   \n",
       "yves-gaucher                              0                     0   \n",
       "yves-klein                                0                     0   \n",
       "zinaida-serebriakova                      0                     0   \n",
       "\n",
       "col_0                  agnes-martin  agnolo-bronzino  ...  william-baziotes  \\\n",
       "row_0                                                 ...                     \n",
       "a.y.-jackson                      0                0  ...                 0   \n",
       "aaron-siskind                     0                0  ...                 0   \n",
       "abdullah-suriosubroto             0                0  ...                 0   \n",
       "abidin-dino                       0                0  ...                 0   \n",
       "abraham-manievich                 0                0  ...                 0   \n",
       "...                             ...              ...  ...               ...   \n",
       "yiannis-tsaroychis                0                0  ...                 0   \n",
       "yov-kondzelevych                  0                0  ...                 0   \n",
       "yves-gaucher                      0                0  ...                 0   \n",
       "yves-klein                        0                0  ...                 0   \n",
       "zinaida-serebriakova              0                0  ...                 0   \n",
       "\n",
       "col_0                  william-blake  william-congdon  william-merritt-chase  \\\n",
       "row_0                                                                          \n",
       "a.y.-jackson                       0                0                      0   \n",
       "aaron-siskind                      0                6                      0   \n",
       "abdullah-suriosubroto              0               22                      0   \n",
       "abidin-dino                        0                0                      0   \n",
       "abraham-manievich                  0                1                      0   \n",
       "...                              ...              ...                    ...   \n",
       "yiannis-tsaroychis                 0                2                      0   \n",
       "yov-kondzelevych                   0                0                      0   \n",
       "yves-gaucher                       0                0                      0   \n",
       "yves-klein                         0                0                      0   \n",
       "zinaida-serebriakova               0                0                      0   \n",
       "\n",
       "col_0                  william-shayer  wu-guanzhong  xu-beihong  yayoi-kusama  \\\n",
       "row_0                                                                           \n",
       "a.y.-jackson                        0             0           0             0   \n",
       "aaron-siskind                       0             0           0             0   \n",
       "abdullah-suriosubroto               0             0           0             0   \n",
       "abidin-dino                         0             0           0             0   \n",
       "abraham-manievich                   0             0           0             0   \n",
       "...                               ...           ...         ...           ...   \n",
       "yiannis-tsaroychis                  0             0           0             0   \n",
       "yov-kondzelevych                    0             0           0             0   \n",
       "yves-gaucher                        0             0           0             0   \n",
       "yves-klein                          0             0           0             0   \n",
       "zinaida-serebriakova                1             0           0             0   \n",
       "\n",
       "col_0                  yves-gaucher  yves-klein  \n",
       "row_0                                            \n",
       "a.y.-jackson                      0           0  \n",
       "aaron-siskind                     0           0  \n",
       "abdullah-suriosubroto             0           0  \n",
       "abidin-dino                       0           0  \n",
       "abraham-manievich                 0           0  \n",
       "...                             ...         ...  \n",
       "yiannis-tsaroychis                0           0  \n",
       "yov-kondzelevych                  0           0  \n",
       "yves-gaucher                      8           0  \n",
       "yves-klein                        0           1  \n",
       "zinaida-serebriakova              0           0  \n",
       "\n",
       "[1104 rows x 615 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred_y = rf_model.predict(test_X)\n",
    "pd.crosstab(test_y, rf_pred_y)  # 실제값, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ad0bf1-b8ea-4148-bb49-26431d46dcad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                    a.y.-jackson     0.0317    0.0333    0.0325        60\n",
      "                   aaron-siskind     0.0000    0.0000    0.0000        60\n",
      "           abdullah-suriosubroto     0.0000    0.0000    0.0000        60\n",
      "                     abidin-dino     0.8889    0.1333    0.2319        60\n",
      "               abraham-manievich     0.0000    0.0000    0.0000        60\n",
      "                    ad-reinhardt     0.0000    0.0000    0.0000        60\n",
      "                    adam-baltatu     0.0000    0.0000    0.0000        60\n",
      "                     adnan-coker     0.0876    0.2833    0.1339        60\n",
      "               adolf-fleischmann     0.2979    0.2333    0.2617        60\n",
      "                    adolf-hitler     0.0000    0.0000    0.0000        60\n",
      "adolphe-joseph-thomas-monticelli     0.0000    0.0000    0.0000        60\n",
      "                 adriaen-brouwer     0.0000    0.0000    0.0000        60\n",
      "            adriaen-van-de-velde     1.0000    0.0833    0.1538        60\n",
      "            adriaen-van-de-venne     1.0000    0.0167    0.0328        60\n",
      "              adriaen-van-ostade     0.0000    0.0000    0.0000        60\n",
      "                    aelbert-cuyp     0.0000    0.0000    0.0000        60\n",
      "                            afro     0.0000    0.0000    0.0000        60\n",
      "                    agnes-martin     1.0000    0.1500    0.2609        60\n",
      "                 agnolo-bronzino     1.0000    0.0167    0.0328        60\n",
      "               agostino-carracci     0.0000    0.0000    0.0000        60\n",
      "                      aki-kuroda     1.0000    0.1833    0.3099        60\n",
      "           akseli-gallen-kallela     1.0000    0.0667    0.1250        60\n",
      "                         al-held     0.6081    0.7500    0.6716        60\n",
      "         aladar-korosfoi-kriesch     0.0000    0.0000    0.0000        60\n",
      "                albert-bierstadt     0.0000    0.0000    0.0000        60\n",
      "                    albert-bloch     1.0000    0.0167    0.0328        60\n",
      "                  albert-gleizes     0.0000    0.0000    0.0000        60\n",
      "                     albert-huie     1.0000    0.0833    0.1538        60\n",
      "                  albert-marquet     1.0000    0.0333    0.0645        60\n",
      "            albert-pinkham-ryder     0.0000    0.0000    0.0000        60\n",
      "                   alberto-burri     0.2727    1.0000    0.4286        60\n",
      "                alberto-carneiro     0.6842    0.2167    0.3291        60\n",
      "                alberto-magnelli     0.3333    0.1333    0.1905        60\n",
      "              albrecht-altdorfer     1.0000    0.0167    0.0328        60\n",
      "                  albrecht-durer     0.0000    0.0000    0.0000        60\n",
      "                 aldemir-martins     1.0000    0.1000    0.1818        60\n",
      "                    aldo-mondino     0.9333    0.2333    0.3733        60\n",
      "              alekos-kontopoulos     0.0000    0.0000    0.0000        60\n",
      "               aleksandra-ekster     0.8947    0.5667    0.6939        60\n",
      "                aleksey-antropov     0.0000    0.0000    0.0000        60\n",
      "                aleksey-savrasov     0.1429    0.0167    0.0299        60\n",
      "                        alex-hay     0.5333    0.1333    0.2133        60\n",
      "                alexander-calder     0.8571    0.2000    0.3243        60\n",
      "                alexander-ivanov     1.0000    0.0333    0.0645        60\n",
      "              alexander-liberman     0.0000    0.0000    0.0000        60\n",
      "              alexander-orlowski     1.0000    0.0500    0.0952        60\n",
      "                alexander-shilov     0.0000    0.0000    0.0000        60\n",
      "                alexandre-benois     0.4500    0.1500    0.2250        60\n",
      "               alexey-bogolyubov     0.0000    0.0000    0.0000        60\n",
      "              alexey-venetsianov     0.0000    0.0000    0.0000        60\n",
      "                    alexey-zubov     0.0000    0.0000    0.0000        60\n",
      "                   alfred-jensen     0.6250    1.0000    0.7692        60\n",
      "                    alfred-kubin     0.0000    0.0000    0.0000        60\n",
      "                alfred-manessier     0.0120    0.7667    0.0236        60\n",
      "                   alfred-sisley     0.0000    0.0000    0.0000        60\n",
      "                  alfred-stevens     0.0000    0.0000    0.0000        60\n",
      "                     alice-baber     0.8571    0.1000    0.1791        60\n",
      "                alighiero-boetti     0.5455    0.2000    0.2927        60\n",
      "                    allan-ramsay     0.0000    0.0000    0.0000        60\n",
      "                     allen-jones     0.0000    0.0000    0.0000        60\n",
      "             alma-woodsey-thomas     0.2174    0.0833    0.1205        60\n",
      "                     alonzo-cano     0.1250    0.0167    0.0294        60\n",
      "                  alphonse-mucha     1.0000    0.0167    0.0328        60\n",
      "                     alvaro-lapa     0.0864    0.4333    0.1440        60\n",
      "         amadeo-de-souza-cardoso     1.0000    0.0667    0.1250        60\n",
      "                 amedee-ozenfant     0.0000    0.0000    0.0000        60\n",
      "               amedeo-modigliani     0.6667    0.0333    0.0635        60\n",
      "                 amrita-sher-gil     1.0000    0.0167    0.0328        60\n",
      "                     anders-zorn     1.0000    0.0333    0.0645        60\n",
      "                  andre-bauchant     1.0000    0.0167    0.0328        60\n",
      "                    andre-derain     0.0000    0.0000    0.0000        60\n",
      "       andre-dunoyer-de-segonzac     0.5556    0.0833    0.1449        60\n",
      "                   andre-lanskoy     0.4471    0.6333    0.5241        60\n",
      "                    andre-masson     0.0000    0.0000    0.0000        60\n",
      "              andre-pierre-arnal     0.6667    0.2000    0.3077        60\n",
      "             andrea-del-castagno     0.8000    0.0667    0.1231        60\n",
      "                andrea-del-sarto     0.0000    0.0000    0.0000        60\n",
      "           andrea-del-verrocchio     0.1429    0.0833    0.1053        60\n",
      "                 andrea-mantegna     1.0000    0.0333    0.0645        60\n",
      "                  andrea-solario     0.0000    0.0000    0.0000        60\n",
      "               andrei-ryabushkin     0.0000    0.0000    0.0000        60\n",
      "                     andy-warhol     0.0000    0.0000    0.0000        60\n",
      "                 angelo-de-sousa     0.7500    0.0500    0.0938        60\n",
      "                  anita-malfatti     0.0000    0.0000    0.0000        60\n",
      "        anna-ostroumova-lebedeva     0.2000    0.0167    0.0308        60\n",
      "                    anne-appleby     1.0000    0.0167    0.0328        60\n",
      "                     anne-truitt     0.0000    0.0000    0.0000        60\n",
      "               annibale-carracci     1.0000    0.0500    0.0952        60\n",
      "                anthony-van-dyck     0.0000    0.0000    0.0000        60\n",
      "               antoine-blanchard     0.7500    0.0500    0.0938        60\n",
      "                   antoine-pesne     1.0000    0.0667    0.1250        60\n",
      "                 antoine-watteau     0.0000    0.0000    0.0000        60\n",
      "                      anton-azbe     0.0000    0.0000    0.0000        60\n",
      "                    anton-melbye     0.0000    0.0000    0.0000        60\n",
      "            antonello-da-messina     0.0000    0.0000    0.0000        60\n",
      "                   antoni-tapies     0.2027    1.0000    0.3371        60\n",
      "                antonio-carneiro     0.0000    0.0000    0.0000        60\n",
      "                 antonio-ligabue     0.0000    0.0000    0.0000        60\n",
      "                  antonio-palolo     1.0000    0.0833    0.1538        60\n",
      "              aristarkh-lentulov     0.7059    0.2000    0.3117        60\n",
      "                 arkhip-kuindzhi     0.0000    0.0000    0.0000        60\n",
      "                           arman     0.3279    1.0000    0.4938        60\n",
      "                 arman-manookian     1.0000    0.0333    0.0645        60\n",
      "               armand-guillaumin     1.0000    0.0500    0.0952        60\n",
      "                   arshile-gorky     1.0000    0.0167    0.0328        60\n",
      "           artemisia-gentileschi     0.7500    0.0500    0.0938        60\n",
      "                     arthur-dove     0.8421    0.5333    0.6531        60\n",
      "                   arthur-hughes     0.0000    0.0000    0.0000        60\n",
      "                     arthur-lowe     0.1290    1.0000    0.2286        60\n",
      "                 arthur-pinajian     0.0500    0.0167    0.0250        60\n",
      "                    arthur-segal     0.9375    0.2500    0.3947        60\n",
      "                   arthur-verona     0.0000    0.0000    0.0000        60\n",
      "                    arturo-souto     0.0000    0.0000    0.0000        60\n",
      "                asgrimur-jonsson     0.7500    0.0500    0.0938        60\n",
      "                   atsuko-tanaka     0.1023    0.1500    0.1216        60\n",
      "                aubrey-beardsley     0.3333    0.0167    0.0317        60\n",
      "                    audrey-flack     0.8182    0.3000    0.4390        60\n",
      "                    august-macke     0.0000    0.0000    0.0000        60\n",
      "                  auguste-herbin     0.0000    0.0000    0.0000        60\n",
      "                   auguste-rodin     0.3750    0.1000    0.1579        60\n",
      "                   augustus-john     1.0000    0.0333    0.0645        60\n",
      "                     aurel-cojan     0.1412    0.4167    0.2110        60\n",
      "                         balthus     0.0000    0.0000    0.0000        60\n",
      "                  barnett-newman     0.0000    0.0000    0.0000        60\n",
      "               bartolome-bermejo     0.0000    0.0000    0.0000        60\n",
      "       bartolome-esteban-murillo     0.0000    0.0000    0.0000        60\n",
      "                   basil-beattie     0.9091    0.1667    0.2817        60\n",
      "                 basuki-abdullah     0.6000    0.0500    0.0923        60\n",
      "                     bela-czobel     0.0000    0.0000    0.0000        60\n",
      "                      bela-kadar     0.0000    0.0000    0.0000        60\n",
      "                   ben-nicholson     0.0000    0.0000    0.0000        60\n",
      "                   benjamin-west     0.0000    0.0000    0.0000        60\n",
      "                 benozzo-gozzoli     0.2857    0.0333    0.0597        60\n",
      "               bernardo-bellotto     0.1190    0.0833    0.0980        60\n",
      "                bernardo-strozzi     0.0000    0.0000    0.0000        60\n",
      "                bernhard-strigel     0.7500    0.1000    0.1765        60\n",
      "                    bertalan-por     0.0000    0.0000    0.0000        60\n",
      "                  berthe-morisot     0.0000    0.0000    0.0000        60\n",
      "                   betty-parsons     1.0000    0.0333    0.0645        60\n",
      "                     billy-apple     0.5455    0.1000    0.1690        60\n",
      "                  billy-childish     0.0000    0.0000    0.0000        60\n",
      "                  blinky-palermo     0.0000    0.0000    0.0000        60\n",
      "                 boris-grigoriev     0.0000    0.0000    0.0000        60\n",
      "                 boris-kustodiev     0.0000    0.0000    0.0000        60\n",
      "           bradley-walker-tomlin     1.0000    0.0167    0.0328        60\n",
      "                  brett-whiteley     0.1000    0.0667    0.0800        60\n",
      "                    brice-marden     0.0000    0.0000    0.0000        60\n",
      "                   bridget-riley     0.5500    0.5500    0.5500        60\n",
      "                    bruce-nauman     0.8750    0.2333    0.3684        60\n",
      "                   bui-xuan-phai     1.0000    0.1333    0.2353        60\n",
      "                 burhan-dogancay     0.5385    0.1167    0.1918        60\n",
      "                 camille-bombois     0.6667    0.2667    0.3810        60\n",
      "                   camille-corot     0.0000    0.0000    0.0000        60\n",
      "                camille-pissarro     0.0000    0.0000    0.0000        60\n",
      "                       canaletto     1.0000    0.0167    0.0328        60\n",
      "               candido-portinari     0.6667    0.0333    0.0635        60\n",
      "                      caravaggio     0.0000    0.0000    0.0000        60\n",
      "                      carl-bloch     0.0192    0.5000    0.0369        60\n",
      "                    carl-larsson     0.0000    0.0000    0.0000        60\n",
      "  carl-ludwig-johann-christineck     0.0000    0.0000    0.0000        60\n",
      "                  carlo-crivelli     0.0000    0.0000    0.0000        60\n",
      "                  carlos-botelho     0.0000    0.0000    0.0000        60\n",
      "                   carlos-merida     0.8000    0.0667    0.1231        60\n",
      "            carlos-orozco-romero     0.0000    0.0000    0.0000        60\n",
      "          carlos-saenz-de-tejada     0.0000    0.0000    0.0000        60\n",
      "          caspar-david-friedrich     0.5000    0.0167    0.0323        60\n",
      "                   chaim-soutine     0.0000    0.0000    0.0000        60\n",
      "                  charles-cottet     0.0000    0.0000    0.0000        60\n",
      "                  charles-demuth     0.0000    0.0000    0.0000        60\n",
      "       charles-francois-daubigny     0.0000    0.0000    0.0000        60\n",
      "                 charles-hermans     0.0000    0.0000    0.0000        60\n",
      "                  charles-hinman     0.3015    0.6833    0.4184        60\n",
      "                charles-lapicque     0.1034    0.0500    0.0674        60\n",
      "                 charles-reiffel     1.0000    0.0667    0.1250        60\n",
      "                 charles-sheeler     1.0000    0.2000    0.3333        60\n",
      "                   childe-hassam     0.0000    0.0000    0.0000        60\n",
      "                  christen-kobke     0.0000    0.0000    0.0000        60\n",
      "                 christian-schad     0.0154    1.0000    0.0304        60\n",
      "                 christo-coetzee     0.0000    0.0000    0.0000        60\n",
      "  christoffer-wilhelm-eckersberg     0.9524    1.0000    0.9756        60\n",
      "               chronis-botsoglou     0.0000    0.0000    0.0000        60\n",
      "                     chuck-close     0.4000    0.0667    0.1143        60\n",
      "              cima-da-conegliano     0.0000    0.0000    0.0000        60\n",
      "                 claes-oldenburg     0.2083    0.1667    0.1852        60\n",
      "                  claude-lorrain     0.8750    0.1167    0.2059        60\n",
      "                    claude-monet     0.0000    0.0000    0.0000        60\n",
      "               claude-tousignant     0.2949    0.3833    0.3333        60\n",
      "                  claude-viallat     0.7179    0.4667    0.5657        60\n",
      "                   claudio-tozzi     0.7273    0.2667    0.3902        60\n",
      "                  clyfford-still     0.0000    0.0000    0.0000        60\n",
      "      columbano-bordalo-pinheiro     0.0000    0.0000    0.0000        60\n",
      "              conrad-marca-relli     0.0000    0.0000    0.0000        60\n",
      "                 constant-troyon     0.6667    0.0333    0.0635        60\n",
      "            constantin-artachino     0.0000    0.0000    0.0000        60\n",
      "              constantin-blendea     0.8750    0.1167    0.2059        60\n",
      "             constantin-brancusi     1.0000    0.2000    0.3333        60\n",
      "     constantin-daniel-rosenthal     0.0800    0.0667    0.0727        60\n",
      "              constantin-flondor     0.0000    0.0000    0.0000        60\n",
      "                 constantin-guys     1.0000    0.0667    0.1250        60\n",
      "              constantin-piliuta     0.4000    0.0667    0.1143        60\n",
      "                constantin-stahi     1.0000    0.0333    0.0645        60\n",
      "              constantine-maleas     0.0000    0.0000    0.0000        60\n",
      "                       corneille     0.0421    0.5167    0.0778        60\n",
      "                 cornelis-de-vos     0.0000    0.0000    0.0000        60\n",
      "   cornelis-norbertus-gysbrechts     0.1818    0.1667    0.1739        60\n",
      "               cornelis-springer     0.0517    0.1000    0.0682        60\n",
      "           cornelis-vreedenburgh     0.1429    0.0167    0.0299        60\n",
      "                   corneliu-baba     0.0000    0.0000    0.0000        60\n",
      "            corneliu-michailescu     0.0000    0.0000    0.0000        60\n",
      "                       correggio     0.0000    0.0000    0.0000        60\n",
      "                 costas-niarchos     1.0000    0.0500    0.0952        60\n",
      "                  craig-kauffman     0.0000    0.0000    0.0000        60\n",
      "         cristovao-de-figueiredo     0.0000    0.0000    0.0000        60\n",
      "                      cy-twombly     0.0000    0.0000    0.0000        60\n",
      "                       dadamaino     0.5750    0.7667    0.6571        60\n",
      "                    damien-hirst     0.9677    1.0000    0.9836        60\n",
      "                 dan-christensen     0.0000    0.0000    0.0000        60\n",
      "                      dan-flavin     1.0000    0.0333    0.0645        60\n",
      "                    daniel-buren     0.2593    0.3500    0.2979        60\n",
      "                  daniel-dezeuze     0.0000    0.0000    0.0000        60\n",
      "          dante-gabriel-rossetti     0.0000    0.0000    0.0000        60\n",
      "          david-alfaro-siqueiros     1.0000    0.0167    0.0328        60\n",
      "                 david-batchelor     0.7778    0.1167    0.2029        60\n",
      "                   david-bomberg     0.0000    0.0000    0.0000        60\n",
      "                   david-burliuk     0.0000    0.0000    0.0000        60\n",
      "                     david-smith     1.0000    0.0833    0.1538        60\n",
      "       david-teniers-the-younger     0.0000    0.0000    0.0000        60\n",
      "                    david-wilkie     0.0000    0.0000    0.0000        60\n",
      "                    denise-green     1.0000    0.1000    0.1818        60\n",
      "                   derek-boshier     0.0000    0.0000    0.0000        60\n",
      "                    diego-rivera     0.0000    0.0000    0.0000        60\n",
      "                 diego-velazquez     0.0000    0.0000    0.0000        60\n",
      "               dimitrie-paciurea     0.4225    1.0000    0.5941        60\n",
      "                dimitris-mytaras     1.0000    0.0167    0.0328        60\n",
      "                    ding-yanyong     0.5000    0.1333    0.2105        60\n",
      "                      dirk-bouts     0.0000    0.0000    0.0000        60\n",
      "                 dmitry-levitzky     0.0000    0.0000    0.0000        60\n",
      "            domenico-ghirlandaio     0.0000    0.0000    0.0000        60\n",
      "              domenico-veneziano     0.5000    0.0500    0.0909        60\n",
      "               domingos-sequeira     0.1200    0.1000    0.1091        60\n",
      "                   donald-sultan     0.7805    0.5333    0.6337        60\n",
      "                       donatello     0.8636    0.3167    0.4634        60\n",
      "                     dosso-dossi     0.0000    0.0000    0.0000        60\n",
      "                     doug-ohlson     0.1340    0.2167    0.1656        60\n",
      "                    doug-wheeler     0.0263    1.0000    0.0512        60\n",
      "                 douglas-huebler     0.0000    0.0000    0.0000        60\n",
      "                 dumitru-ghiatza     0.7143    0.0833    0.1493        60\n",
      "                     edgar-degas     0.0000    0.0000    0.0000        60\n",
      "                  edith-vonnegut     0.0000    0.0000    0.0000        60\n",
      "          edmund-charles-tarbell     0.0000    0.0000    0.0000        60\n",
      "                    edmund-dulac     0.1250    0.0167    0.0294        60\n",
      "                  edouard-cortes     1.0000    0.0167    0.0328        60\n",
      "                   edouard-manet     0.0000    0.0000    0.0000        60\n",
      "                edouard-vuillard     1.0000    0.0333    0.0645        60\n",
      "             eduard-von-gebhardt     0.0000    0.0000    0.0000        60\n",
      "                eduardo-paolozzi     0.0000    0.0000    0.0000        60\n",
      "                   eduardo-viana     0.0000    0.0000    0.0000        60\n",
      "                    edvard-munch     0.0000    0.0000    0.0000        60\n",
      "                edward-avedisian     0.0000    0.0000    0.0000        60\n",
      "              edward-burne-jones     0.3333    0.0167    0.0317        60\n",
      "                  edward-corbett     0.0000    0.0000    0.0000        60\n",
      "                   edward-hopper     0.0000    0.0000    0.0000        60\n",
      "                   edward-ruscha     1.0000    0.0500    0.0952        60\n",
      "            edwin-henry-landseer     0.6667    0.0667    0.1212        60\n",
      "                     efim-volkov     1.0000    0.0167    0.0328        60\n",
      "                    egon-schiele     0.0000    0.0000    0.0000        60\n",
      "                     eileen-agar     0.9545    0.3500    0.5122        60\n",
      "                        el-greco     0.0000    0.0000    0.0000        60\n",
      "                    el-lissitzky     0.0000    0.0000    0.0000        60\n",
      "               elaine-de-kooning     0.0000    0.0000    0.0000        60\n",
      "                 eliseu-visconti     0.0000    0.0000    0.0000        60\n",
      "                 ellsworth-kelly     0.0000    0.0000    0.0000        60\n",
      "                      emil-nolde     1.0000    0.0333    0.0645        60\n",
      "                     emile-claus     1.0000    0.0833    0.1538        60\n",
      "                      emily-carr     1.0000    0.0667    0.1250        60\n",
      "                 emmanuel-zairis     0.2045    0.1500    0.1731        60\n",
      "                    endre-balint     1.0000    0.0500    0.0952        60\n",
      "                    endre-bartos     0.4762    0.1667    0.2469        60\n",
      "               enrico-castellani     1.0000    0.3000    0.4615        60\n",
      "                     eric-fischl     1.0000    0.0167    0.0328        60\n",
      "           ernst-ludwig-kirchner     0.0000    0.0000    0.0000        60\n",
      "                            erro     0.6429    0.1500    0.2432        60\n",
      "             esaias-van-de-velde     0.0000    0.0000    0.0000        60\n",
      "                 esteban-vicente     1.0000    0.1167    0.2090        60\n",
      "                   eugene-boudin     0.0000    0.0000    0.0000        60\n",
      "                eugene-delacroix     0.0000    0.0000    0.0000        60\n",
      "               eustache-le-sueur     0.0000    0.0000    0.0000        60\n",
      "                    eva-gonzales     0.0000    0.0000    0.0000        60\n",
      "                       eva-hesse     1.0000    0.0167    0.0328        60\n",
      "                   evelyne-axell     1.0000    0.1333    0.2353        60\n",
      "                fairfield-porter     0.0000    0.0000    0.0000        60\n",
      "           federico-zandomeneghi     0.0000    0.0000    0.0000        60\n",
      "                   felicien-rops     0.0000    0.0000    0.0000        60\n",
      "           felix-gonzalez-torres     0.0000    0.0000    0.0000        60\n",
      "                 felix-vallotton     0.0000    0.0000    0.0000        60\n",
      "                ferdinand-hodler     0.0000    0.0000    0.0000        60\n",
      "              ferdynand-ruszczyc     0.0385    0.0500    0.0435        60\n",
      "                   fern-coppedge     0.0000    0.0000    0.0000        60\n",
      "                 fernand-khnopff     1.0000    0.0667    0.1250        60\n",
      "                   fernand-leger     0.5000    0.0167    0.0323        60\n",
      "                 fernando-botero     1.0000    0.0167    0.0328        60\n",
      "                 fernando-calhau     0.5455    0.1000    0.1690        60\n",
      "                 fernando-lanhas     0.6897    0.3333    0.4494        60\n",
      "             fikret-mualla-saygi     1.0000    0.0167    0.0328        60\n",
      "                 filipp-malyavin     0.0000    0.0000    0.0000        60\n",
      "            filippo-brunelleschi     1.0000    0.1500    0.2609        60\n",
      "                   filippo-lippi     0.0000    0.0000    0.0000        60\n",
      "                ford-madox-brown     0.0000    0.0000    0.0000        60\n",
      "                    forrest-bess     1.0000    0.1000    0.1818        60\n",
      "                    fra-angelico     0.3750    0.0500    0.0882        60\n",
      "                francesco-guardi     0.0000    0.0000    0.0000        60\n",
      "                 francesco-hayez     0.0000    0.0000    0.0000        60\n",
      "              francesco-solimena     0.0000    0.0000    0.0000        60\n",
      "                 francis-picabia     0.0000    0.0000    0.0000        60\n",
      "                 francisc-sirato     0.3333    0.0667    0.1111        60\n",
      "        francisco-bayeu-y-subias     0.0000    0.0000    0.0000        60\n",
      "           francisco-de-zurbaran     0.0000    0.0000    0.0000        60\n",
      "                  francisco-goya     0.0000    0.0000    0.0000        60\n",
      "                francois-boucher     1.0000    0.0667    0.1250        60\n",
      "                  frank-auerbach     0.0000    0.0000    0.0000        60\n",
      "                   frank-bowling     0.0000    0.0000    0.0000        60\n",
      "                  frank-johnston     1.0000    0.1000    0.1818        60\n",
      "                   frank-lobdell     0.0000    0.0000    0.0000        60\n",
      "                    frank-stella     0.5000    0.0500    0.0909        60\n",
      "             franklin-carmichael     1.0000    0.2000    0.3333        60\n",
      "                      frans-hals     0.0000    0.0000    0.0000        60\n",
      "                   frans-snyders     0.0000    0.0000    0.0000        60\n",
      "                 frantisek-kupka     0.0000    0.0000    0.0000        60\n",
      "                     franz-kline     1.0000    0.0500    0.0952        60\n",
      "                      franz-marc     0.5000    0.0167    0.0323        60\n",
      "       franz-richard-unterberger     0.0000    0.0000    0.0000        60\n",
      "                     franz-stuck     0.0000    0.0000    0.0000        60\n",
      "        franz-xaver-winterhalter     0.0161    0.3000    0.0306        60\n",
      "                   fred-sandback     0.2500    0.1167    0.1591        60\n",
      "                   fred-williams     0.0000    0.0000    0.0000        60\n",
      "                      fred-yates     1.0000    0.0500    0.0952        60\n",
      "                frederic-bazille     0.0000    0.0000    0.0000        60\n",
      "           frederic-edwin-church     0.0000    0.0000    0.0000        60\n",
      "              frederic-remington     0.0000    0.0000    0.0000        60\n",
      "            frederick-hammersley     0.7838    0.4833    0.5979        60\n",
      "                     frida-kahlo     0.3750    1.0000    0.5455        60\n",
      "                  friedel-dzubas     0.5000    0.0333    0.0625        60\n",
      "                   frits-thaulow     1.0000    0.0500    0.0952        60\n",
      "                fyodor-bronnikov     0.6000    0.0500    0.0923        60\n",
      "                  fyodor-rokotov     0.0000    0.0000    0.0000        60\n",
      "                 fyodor-solntsev     0.1262    0.2167    0.1595        60\n",
      "                 fyodor-vasilyev     1.0000    0.0167    0.0328        60\n",
      "                   gabriel-metsu     0.0000    0.0000    0.0000        60\n",
      "                      gene-davis     1.0000    0.0833    0.1538        60\n",
      "                  genevieve-asse     0.0000    0.0000    0.0000        60\n",
      "                george-bouzianis     0.0000    0.0000    0.0000        60\n",
      "                   george-catlin     0.1500    0.0500    0.0750        60\n",
      "          george-frederick-watts     0.0000    0.0000    0.0000        60\n",
      "                   george-inness     0.0408    1.0000    0.0785        60\n",
      "                     george-luks     0.0000    0.0000    0.0000        60\n",
      "                george-mavroides     0.0000    0.0000    0.0000        60\n",
      "                  george-morland     0.0000    0.0000    0.0000        60\n",
      "                    george-pemba     0.0000    0.0000    0.0000        60\n",
      "                    george-segal     1.0000    0.0333    0.0645        60\n",
      "                   george-stubbs     1.0000    0.0167    0.0328        60\n",
      "                  georges-braque     0.0000    0.0000    0.0000        60\n",
      "                  georges-lemmen     1.0000    0.1000    0.1818        60\n",
      "     georges-ribemont-dessaignes     0.0000    0.0000    0.0000        60\n",
      "                 georges-rouault     0.0000    0.0000    0.0000        60\n",
      "                  georges-seurat     0.0000    0.0000    0.0000        60\n",
      "            georges-vantongerloo     0.0000    0.0000    0.0000        60\n",
      "              georgios-jakobides     0.0000    0.0000    0.0000        60\n",
      "                    gerard-david     0.0000    0.0000    0.0000        60\n",
      "                gerard-fromanger     1.0000    0.0500    0.0952        60\n",
      "                   gerard-sekoto     1.0000    0.0333    0.0645        60\n",
      "                 gerard-terborch     0.0000    0.0000    0.0000        60\n",
      "                 gerardo-dottori     0.0110    1.0000    0.0218        60\n",
      "                 gerhard-richter     0.6667    0.0333    0.0635        60\n",
      "                      gerrit-dou     0.0000    0.0000    0.0000        60\n",
      "                   geta-bratescu     1.0000    0.0333    0.0645        60\n",
      "             gheorghe-tattarescu     0.7500    0.1500    0.2500        60\n",
      "                   giacomo-balla     0.8750    0.2333    0.3684        60\n",
      "            gian-lorenzo-bernini     0.8571    0.1000    0.1791        60\n",
      "           gianfranco-baruchello     0.0000    0.0000    0.0000        60\n",
      "              gil-teixeira-lopes     0.3204    0.5500    0.4049        60\n",
      "                  gilles-aillaud     0.2353    0.1333    0.1702        60\n",
      "                   gino-severini     0.0000    0.0000    0.0000        60\n",
      "                 giorgio-morandi     0.0000    0.0000    0.0000        60\n",
      "                  giorgio-vasari     0.8182    0.1500    0.2535        60\n",
      "                       giorgione     0.0000    0.0000    0.0000        60\n",
      "                giovanni-anselmo     0.0000    0.0000    0.0000        60\n",
      "     giovanni-antonio-boltraffio     0.0000    0.0000    0.0000        60\n",
      "       giovanni-battista-tiepolo     0.0000    0.0000    0.0000        60\n",
      "                giovanni-bellini     0.0000    0.0000    0.0000        60\n",
      "                giovanni-boldini     0.0000    0.0000    0.0000        60\n",
      "       giovanni-domenico-tiepolo     0.0000    0.0000    0.0000        60\n",
      "                giovanni-fattori     0.1667    0.0167    0.0303        60\n",
      "           giovanni-paolo-panini     0.1667    0.0500    0.0769        60\n",
      "             giuseppe-arcimboldo     0.0000    0.0000    0.0000        60\n",
      "              giuseppe-de-nittis     0.0000    0.0000    0.0000        60\n",
      "            gosta-adrian-nilsson     1.0000    0.0500    0.0952        60\n",
      "               gotthard-graubner     0.6667    0.0333    0.0635        60\n",
      "          grace-cossington-smith     0.8571    0.1000    0.1791        60\n",
      "                   grandma-moses     0.0127    0.0333    0.0184        60\n",
      "              gregoire-boonzaier     0.0000    0.0000    0.0000        60\n",
      "                  gregorio-lopes     0.0000    0.0000    0.0000        60\n",
      "             grigoriy-myasoyedov     0.0000    0.0000    0.0000        60\n",
      "                  guido-molinari     0.6250    0.0833    0.1471        60\n",
      "                      guido-reni     0.0000    0.0000    0.0000        60\n",
      "                   gulacsy-lajos     0.0563    0.0667    0.0611        60\n",
      "                guntis-strupulis     0.4839    1.0000    0.6522        60\n",
      "                    gustav-klimt     0.0000    0.0000    0.0000        60\n",
      "             gustave-caillebotte     1.0000    0.0167    0.0328        60\n",
      "                 gustave-courbet     1.0000    0.0167    0.0328        60\n",
      "                    gustave-dore     0.0000    0.0000    0.0000        60\n",
      "                 gustave-loiseau     0.0000    0.0000    0.0000        60\n",
      "                  gustave-moreau     1.0000    0.0167    0.0328        60\n",
      "                        guy-rose     0.2857    0.0333    0.0597        60\n",
      "                       gwen-john     0.0000    0.0000    0.0000        60\n",
      "                    hans-baldung     0.0000    0.0000    0.0000        60\n",
      "                    hans-bellmer     0.2071    0.5833    0.3057        60\n",
      "                    hans-hofmann     0.2500    0.0333    0.0588        60\n",
      "        hans-holbein-the-younger     0.0000    0.0000    0.0000        60\n",
      "                    hans-memling     0.0000    0.0000    0.0000        60\n",
      "                    hans-richter     1.0000    0.2167    0.3562        60\n",
      "                 hans-von-aachen     0.0000    0.0000    0.0000        60\n",
      "                    harry-clarke     0.0000    0.0000    0.0000        60\n",
      "                    hassel-smith     0.0000    0.0000    0.0000        60\n",
      "                    hedda-sterne     0.8333    0.0833    0.1515        60\n",
      "             helen-frankenthaler     1.0000    0.0167    0.0328        60\n",
      "              helene-schjerfbeck     0.0000    0.0000    0.0000        60\n",
      "                  helio-oiticica     0.0000    0.0000    0.0000        60\n",
      "        hendrick-cornelisz-vroom     0.0647    0.1833    0.0957        60\n",
      "            hendrick-terbrugghen     0.0000    0.0000    0.0000        60\n",
      "                    henk-peeters     0.0000    0.0000    0.0000        60\n",
      "                   henri-catargi     0.0000    0.0000    0.0000        60\n",
      "       henri-de-toulouse-lautrec     0.0000    0.0000    0.0000        60\n",
      "              henri-edmond-cross     0.0000    0.0000    0.0000        60\n",
      "             henri-fantin-latour     0.0000    0.0000    0.0000        60\n",
      "                   henri-laurens     0.1618    0.1833    0.1719        60\n",
      "             henri-le-fauconnier     0.0000    0.0000    0.0000        60\n",
      "                    henri-martin     0.0000    0.0000    0.0000        60\n",
      "                   henri-matisse     0.0000    0.0000    0.0000        60\n",
      "                   henri-michaux     1.0000    0.2667    0.4211        60\n",
      "                  henri-rousseau     0.0000    0.0000    0.0000        60\n",
      "                 henrique-pousao     0.0000    0.0000    0.0000        60\n",
      "        henry-herbert-la-thangue     0.0000    0.0000    0.0000        60\n",
      "                     henry-moore     0.8077    0.3500    0.4884        60\n",
      "                   henry-raeburn     1.0000    0.0333    0.0645        60\n",
      "              henryk-siemiradzki     0.0000    0.0000    0.0000        60\n",
      "                  heorhiy-narbut     0.0000    0.0000    0.0000        60\n",
      "                hercules-seghers     0.0000    0.0000    0.0000        60\n",
      "                hieronymus-bosch     0.0000    0.0000    0.0000        60\n",
      "                  hilma-af-klint     0.9630    0.4333    0.5977        60\n",
      "                   hiro-yamagata     0.1967    0.2000    0.1983        60\n",
      "                       hiroshige     1.0000    0.0167    0.0328        60\n",
      "                   hoca-ali-riza     1.0000    0.0333    0.0645        60\n",
      "                    homer-watson     0.0175    0.0167    0.0171        60\n",
      "                   horace-pippin     0.0000    0.0000    0.0000        60\n",
      "                    horia-bernea     0.1000    0.0167    0.0286        60\n",
      "                    horia-damian     0.7353    0.4167    0.5319        60\n",
      "                  howard-finster     0.9048    0.6333    0.7451        60\n",
      "                  howard-hodgkin     1.0000    0.1333    0.2353        60\n",
      "                  howard-mehring     0.0639    0.7000    0.1172        60\n",
      "                   hubert-robert     0.3750    0.0500    0.0882        60\n",
      "                    hugo-simberg     0.0000    0.0000    0.0000        60\n",
      "               hugo-van-der-goes     0.3333    0.0167    0.0317        60\n",
      "                   ian-davenport     1.0000    0.1000    0.1818        60\n",
      "                       ilka-gedo     0.0000    0.0000    0.0000        60\n",
      "                    ilya-mashkov     0.1429    0.0167    0.0299        60\n",
      "                      ilya-repin     0.0000    0.0000    0.0000        60\n",
      "                     imi-knoebel     1.0000    0.1167    0.2090        60\n",
      "              ioannis-altamouras     1.0000    0.0333    0.0645        60\n",
      "                   ion-andreescu     0.8750    0.1167    0.2059        60\n",
      "                     ion-nicodim     1.0000    0.2167    0.3562        60\n",
      "                       ion-pacea     0.2000    0.0167    0.0308        60\n",
      "            ion-theodorescu-sion     0.0000    0.0000    0.0000        60\n",
      "                   ion-tuculescu     0.1765    0.1500    0.1622        60\n",
      "                      iosif-iser     0.0000    0.0000    0.0000        60\n",
      "                  ipolit-strambu     0.0000    0.0000    0.0000        60\n",
      "                      irma-stern     0.0000    0.0000    0.0000        60\n",
      "                     isa-genzken     0.9836    1.0000    0.9917        60\n",
      "                   isaac-levitan     0.0000    0.0000    0.0000        60\n",
      "                isaac-van-ostade     0.0000    0.0000    0.0000        60\n",
      "                   istvan-farkas     0.0000    0.0000    0.0000        60\n",
      "            istvan-ilosvai-varga     0.3333    0.0167    0.0317        60\n",
      "                     istvan-nagy     0.0000    0.0000    0.0000        60\n",
      "                     ito-jakuchu     0.0000    0.0000    0.0000        60\n",
      "                     ito-shinsui     0.0192    1.0000    0.0376        60\n",
      "                 ivan-aivazovsky     0.0000    0.0000    0.0000        60\n",
      "                   ivan-albright     1.0000    0.0667    0.1250        60\n",
      "                    ivan-bilibin     0.0000    0.0000    0.0000        60\n",
      "                  ivan-generalic     0.0000    0.0000    0.0000        60\n",
      "                     ivan-grohar     0.0000    0.0000    0.0000        60\n",
      "                   ivan-kramskoy     0.0000    0.0000    0.0000        60\n",
      "                      ivan-milev     0.8571    0.1000    0.1791        60\n",
      "                    ivan-nikitin     0.0160    0.4833    0.0309        60\n",
      "                  ivan-rutkovych     0.8000    0.0667    0.1231        60\n",
      "                   ivan-shishkin     0.0000    0.0000    0.0000        60\n",
      "                 ivan-vladimirov     1.0000    0.0500    0.0952        60\n",
      "              j.-e.-h.-macdonald     0.0000    0.0000    0.0000        60\n",
      "                jacek-malczewski     0.0000    0.0000    0.0000        60\n",
      "                       jack-bush     0.0000    0.0000    0.0000        60\n",
      "                    jack-tworkov     1.0000    0.1000    0.1818        60\n",
      "                 jack-youngerman     0.6154    0.1333    0.2192        60\n",
      "                 jackson-pollock     0.0000    0.0000    0.0000        60\n",
      "   jacob-isaakszoon-van-ruisdael     0.2632    0.0833    0.1266        60\n",
      "                  jacob-jordaens     0.0000    0.0000    0.0000        60\n",
      "           jacoba-van-heemskerck     0.6842    0.2167    0.3291        60\n",
      "                  jacopo-bellini     1.0000    0.0833    0.1538        60\n",
      "                 jacopo-pontormo     0.0000    0.0000    0.0000        60\n",
      "                  jacques-stella     1.0000    0.1167    0.2090        60\n",
      "                  jacques-villon     0.0000    0.0000    0.0000        60\n",
      "                     james-ensor     0.0000    0.0000    0.0000        60\n",
      "          james-mcneill-whistler     0.0000    0.0000    0.0000        60\n",
      "                    james-tissot     0.0000    0.0000    0.0000        60\n",
      "                   james-turrell     0.4000    1.0000    0.5714        60\n",
      "                     jamie-wyeth     1.0000    0.0167    0.0328        60\n",
      "                     jan-matejko     0.0000    0.0000    0.0000        60\n",
      "                    jan-provoost     1.0000    0.0333    0.0645        60\n",
      "                  jan-siberechts     0.0000    0.0000    0.0000        60\n",
      "                    jan-sluyters     1.0000    0.0500    0.0952        60\n",
      "                       jan-steen     0.0000    0.0000    0.0000        60\n",
      "                      jan-toorop     0.6667    0.0667    0.1212        60\n",
      "                    jan-van-eyck     1.0000    0.0167    0.0328        60\n",
      "                jan-van-hemessen     1.0000    0.0500    0.0952        60\n",
      "                      jane-frank     1.0000    0.1000    0.1818        60\n",
      "                      janet-fish     0.4286    0.0500    0.0896        60\n",
      "            janos-mattis-teutsch     0.0000    0.0000    0.0000        60\n",
      "                   janos-tornyai     0.0000    0.0000    0.0000        60\n",
      "                       jay-defeo     1.0000    0.0500    0.0952        60\n",
      "         jean-alexandru-steriadi     0.0000    0.0000    0.0000        60\n",
      "             jean-baptiste-oudry     0.0000    0.0000    0.0000        60\n",
      "    jean-baptiste-simeon-chardin     0.5000    0.0167    0.0323        60\n",
      "                      jean-david     1.0000    0.0167    0.0328        60\n",
      "                   jean-degottex     0.0000    0.0000    0.0000        60\n",
      "                   jean-dubuffet     0.0000    0.0000    0.0000        60\n",
      "                   jean-fautrier     0.0000    0.0000    0.0000        60\n",
      "                    jean-fouquet     0.0000    0.0000    0.0000        60\n",
      "            jean-francois-millet     1.0000    0.0167    0.0328        60\n",
      "                     jean-helion     0.0000    0.0000    0.0000        60\n",
      "                        jean-hey     0.0000    0.0000    0.0000        60\n",
      "           jean-honore-fragonard     0.0000    0.0000    0.0000        60\n",
      "                       jean-hugo     0.0000    0.0000    0.0000        60\n",
      "                jean-leon-gerome     0.0000    0.0000    0.0000        60\n",
      "               jean-marc-nattier     0.0000    0.0000    0.0000        60\n",
      "                  jean-metzinger     0.0000    0.0000    0.0000        60\n",
      "               jean-paul-lemieux     0.0980    0.0833    0.0901        60\n",
      "              jean-paul-riopelle     0.0000    0.0000    0.0000        60\n",
      "               jean-rene-bazaine     0.0000    0.0000    0.0000        60\n",
      "            jehan-georges-vibert     0.0000    0.0000    0.0000        60\n",
      "                        jim-dine     0.2338    0.3000    0.2628        60\n",
      "                      jim-lambie     0.3241    0.5833    0.4167        60\n",
      "                     jimmy-ernst     1.0000    0.0333    0.0645        60\n",
      "                  jiro-yoshihara     0.9444    0.2833    0.4359        60\n",
      "                         jo-baer     1.0000    0.1333    0.2353        60\n",
      "                 joachim-patinir     0.0000    0.0000    0.0000        60\n",
      "                 joachim-wtewael     0.0000    0.0000    0.0000        60\n",
      "           joan-hernandez-pijuan     1.0000    0.0167    0.0328        60\n",
      "                       joan-miro     0.2000    0.0167    0.0308        60\n",
      "                   joan-mitchell     1.0000    0.0167    0.0328        60\n",
      "                       joan-ponc     0.2812    0.7500    0.4091        60\n",
      "                     joan-snyder     1.0000    0.0167    0.0328        60\n",
      "                     joao-vieira     0.7241    0.7000    0.7119        60\n",
      "                  jock-macdonald     1.0000    0.2833    0.4416        60\n",
      "                       joe-goode     0.0000    0.0000    0.0000        60\n",
      "            johan-christian-dahl     0.0335    0.4000    0.0619        60\n",
      "      johan-hendrik-weissenbruch     0.4286    0.1500    0.2222        60\n",
      "                    johann-koler     1.0000    0.0500    0.0952        60\n",
      "                  johannes-itten     0.0000    0.0000    0.0000        60\n",
      "      johannes-sveinsson-kjarval     0.0000    0.0000    0.0000        60\n",
      "                johannes-vermeer     0.0000    0.0000    0.0000        60\n",
      "          john-atkinson-grimshaw     0.0000    0.0000    0.0000        60\n",
      "                john-chamberlain     0.0306    0.4833    0.0575        60\n",
      "                    john-collier     1.0000    0.0167    0.0328        60\n",
      "                  john-constable     0.0000    0.0000    0.0000        60\n",
      "                      john-crome     0.0000    0.0000    0.0000        60\n",
      "           john-duncan-fergusson     0.0000    0.0000    0.0000        60\n",
      "            john-everett-millais     0.0000    0.0000    0.0000        60\n",
      "                     john-ferren     0.0000    0.0000    0.0000        60\n",
      "               john-french-sloan     0.5000    0.0167    0.0323        60\n",
      "            john-henry-twachtman     1.0000    0.0333    0.0645        60\n",
      "                    john-hoppner     0.0000    0.0000    0.0000        60\n",
      "                    john-hoyland     0.5000    0.0167    0.0323        60\n",
      "                     john-lavery     0.1429    0.0333    0.0541        60\n",
      "              john-lewis-krimmel     0.0000    0.0000    0.0000        60\n",
      "                      john-marin     0.0000    0.0000    0.0000        60\n",
      "                  john-mccracken     0.0000    0.0000    0.0000        60\n",
      "                 john-mclaughlin     0.0000    0.0000    0.0000        60\n",
      "                     john-miller     0.0000    0.0000    0.0000        60\n",
      "    john-roddam-spencer-stanhope     0.0000    0.0000    0.0000        60\n",
      "                    john-russell     0.0000    0.0000    0.0000        60\n",
      "             john-singer-sargent     0.0000    0.0000    0.0000        60\n",
      "           john-singleton-copley     0.1692    0.1833    0.1760        60\n",
      "                   john-trumbull     0.0000    0.0000    0.0000        60\n",
      "         john-william-waterhouse     0.0000    0.0000    0.0000        60\n",
      "                   jorge-martins     0.7500    0.0500    0.0938        60\n",
      "            jose-clemente-orozco     0.0679    0.2500    0.1068        60\n",
      "        jose-de-almada-negreiros     1.0000    0.1333    0.2353        60\n",
      "               jose-de-guimaraes     0.0000    0.0000    0.0000        60\n",
      "                   jose-guerrero     0.9091    0.1667    0.2817        60\n",
      "           jose-gutierrez-solana     0.0000    0.0000    0.0000        60\n",
      "                     jose-malhoa     0.2857    0.1000    0.1481        60\n",
      "                    josef-albers     0.0000    0.0000    0.0000        60\n",
      "                    josef-herman     0.1429    0.0167    0.0299        60\n",
      "                josefa-de-obidos     0.0000    0.0000    0.0000        60\n",
      "                   joseph-wright     0.0000    0.0000    0.0000        60\n",
      "                 joshua-reynolds     0.0000    0.0000    0.0000        60\n",
      "              jozsef-rippl-ronai     0.0000    0.0000    0.0000        60\n",
      "             juan-de-valdes-leal     0.0000    0.0000    0.0000        60\n",
      "                       juan-gris     0.0000    0.0000    0.0000        60\n",
      "                  judith-leyster     0.0000    0.0000    0.0000        60\n",
      "                    judy-chicago     0.7059    0.2000    0.3117        60\n",
      "                    jules-cheret     0.0000    0.0000    0.0000        60\n",
      "           jules-joseph-lefebvre     0.7500    0.3000    0.4286        60\n",
      "                   jules-lefranc     0.4762    0.1667    0.2469        60\n",
      "                    jules-pascin     0.3846    0.0833    0.1370        60\n",
      "                   jules-perahim     0.0185    0.5000    0.0357        60\n",
      "               julian-alden-weir     0.0000    0.0000    0.0000        60\n",
      "                  julio-gonzalez     0.1818    0.0667    0.0976        60\n",
      "                     julio-pomar     0.0000    0.0000    0.0000        60\n",
      "          julius-leblanc-stewart     0.0000    0.0000    0.0000        60\n",
      "                   jury-annenkov     0.0000    0.0000    0.0000        60\n",
      "                jusepe-de-ribera     0.4545    0.0833    0.1408        60\n",
      "                   karl-benjamin     1.0000    0.2000    0.3333        60\n",
      "                     karl-bodmer     0.0000    0.0000    0.0000        60\n",
      "                   karl-bryullov     1.0000    0.0333    0.0645        60\n",
      "                     karl-schrag     0.0000    0.0000    0.0000        60\n",
      "                 karoly-ferenczy     0.0000    0.0000    0.0000        60\n",
      "                kateryna-bilokur     0.6667    0.0333    0.0635        60\n",
      "                  kathe-kollwitz     0.0000    0.0000    0.0000        60\n",
      "              katsushika-hokusai     0.0000    0.0000    0.0000        60\n",
      "                     kay-nielsen     0.0000    0.0000    0.0000        60\n",
      "                kazimir-malevich     0.6667    0.0333    0.0635        60\n",
      "                  kazuo-nakamura     1.0000    0.0167    0.0328        60\n",
      "                 kees-van-dongen     0.6667    0.1000    0.1739        60\n",
      "                    keisai-eisen     0.0000    0.0000    0.0000        60\n",
      "                    keith-haring     1.0000    0.1000    0.1818        60\n",
      "                   keith-sonnier     0.7500    0.0500    0.0938        60\n",
      "                     kenzo-okada     0.0000    0.0000    0.0000        60\n",
      "                     kimon-loghi     0.0000    0.0000    0.0000        60\n",
      "                kitagawa-utamaro     0.0000    0.0000    0.0000        60\n",
      "                  klavdy-lebedev     0.6667    0.0333    0.0635        60\n",
      "                   koloman-moser     0.0000    0.0000    0.0000        60\n",
      "                     konrad-witz     0.0000    0.0000    0.0000        60\n",
      "            konstantin-bogaevsky     0.0000    0.0000    0.0000        60\n",
      "              konstantin-korovin     0.0000    0.0000    0.0000        60\n",
      "             konstantin-makovsky     0.0000    0.0000    0.0000        60\n",
      "                konstantin-somov     0.0000    0.0000    0.0000        60\n",
      "             konstantin-vasilyev     1.0000    0.0167    0.0328        60\n",
      "          konstantinos-parthenis     0.1667    0.0167    0.0303        60\n",
      "          konstantinos-volanakis     0.0000    0.0000    0.0000        60\n",
      "                 kurt-schwitters     0.1198    1.0000    0.2139        60\n",
      "             kuzma-petrov-vodkin     0.0000    0.0000    0.0000        60\n",
      "                   lajos-tihanyi     0.0000    0.0000    0.0000        60\n",
      "                      larry-bell     0.2647    0.1500    0.1915        60\n",
      "                       larry-zox     0.2614    0.3833    0.3108        60\n",
      "                    lasar-segall     0.8421    0.2667    0.4051        60\n",
      "              laszlo-mednyanszky     0.0000    0.0000    0.0000        60\n",
      "              laszlo-moholy-nagy     0.2970    1.0000    0.4580        60\n",
      "                 lavinia-fontana     0.3333    0.0167    0.0317        60\n",
      "                   lawren-harris     0.4667    0.2333    0.3111        60\n",
      "                    le-corbusier     0.0000    0.0000    0.0000        60\n",
      "                le-nain-brothers     1.0000    0.0500    0.0952        60\n",
      "                          le-pho     0.0000    0.0000    0.0000        60\n",
      "                     lee-krasner     0.6000    0.0500    0.0923        60\n",
      "                        lee-ufan     0.8000    0.0667    0.1231        60\n",
      "                   lennart-rodhe     0.2900    0.4833    0.3625        60\n",
      "                   leo-villareal     0.5588    0.3167    0.4043        60\n",
      "                      leon-bakst     1.0000    0.0167    0.0328        60\n",
      "                  leon-berkowitz     0.3000    0.0500    0.0857        60\n",
      "                     leon-bonnat     0.0000    0.0000    0.0000        60\n",
      "                       leon-dabo     0.0000    0.0000    0.0000        60\n",
      "                 leon-spilliaert     0.0000    0.0000    0.0000        60\n",
      "               leonardo-da-vinci     0.0000    0.0000    0.0000        60\n",
      "                 leopold-survage     1.0000    0.1167    0.2090        60\n",
      "                    leroy-neiman     0.3333    0.0167    0.0317        60\n",
      "                     lev-lagorio     0.0909    0.0167    0.0282        60\n",
      "                    li-yuan-chia     0.1944    0.2333    0.2121        60\n",
      "                   ligia-macovei     0.0000    0.0000    0.0000        60\n",
      "                   lorenzo-lotto     1.0000    0.0167    0.0328        60\n",
      "                lorser-feitelson     1.0000    1.0000    1.0000        60\n",
      "                   louay-kayyali     0.3750    0.0500    0.0882        60\n",
      "                      louis-cane     0.7857    0.1833    0.2973        60\n",
      "                    louis-janmot     0.0000    0.0000    0.0000        60\n",
      "                louis-marcoussis     1.0000    0.0167    0.0328        60\n",
      "                  louis-schanker     0.0000    0.0000    0.0000        60\n",
      "                    louis-valtat     0.0000    0.0000    0.0000        60\n",
      "                     louis-vivin     0.0490    0.0833    0.0617        60\n",
      "  louise-elisabeth-vigee-le-brun     0.0000    0.0000    0.0000        60\n",
      "                 louise-nevelson     0.9167    0.1833    0.3056        60\n",
      "                  lourdes-castro     0.0000    0.0000    0.0000        60\n",
      "                   lovis-corinth     0.0000    0.0000    0.0000        60\n",
      "                 luca-signorelli     0.4444    0.0667    0.1159        60\n",
      "         lucas-cranach-the-elder     0.0000    0.0000    0.0000        60\n",
      "      lucia-demetriade-balacescu     0.6667    0.0667    0.1212        60\n",
      "                    lucian-freud     0.0000    0.0000    0.0000        60\n",
      "               lucian-grigorescu     0.0000    0.0000    0.0000        60\n",
      "               luciano-bartolini     1.0000    0.1500    0.2609        60\n",
      "                   luigi-russolo     0.0000    0.0000    0.0000        60\n",
      "                      luis-feito     0.0000    0.0000    0.0000        60\n",
      "                      lygia-pape     0.1523    1.0000    0.2643        60\n",
      "                       lynd-ward     0.0000    0.0000    0.0000        60\n",
      "                lyonel-feininger     0.0000    0.0000    0.0000        60\n",
      "                   lyubov-popova     0.1495    0.2667    0.1916        60\n",
      "                      m.-h.-maxy     0.0000    0.0000    0.0000        60\n",
      "                     m.c.-escher     0.6250    0.0833    0.1471        60\n",
      "                          mabuse     0.0000    0.0000    0.0000        60\n",
      "          maerten-van-heemskerck     0.1667    0.0167    0.0303        60\n",
      "                         man-ray     0.7000    0.1167    0.2000        60\n",
      "                     manabu-mabe     0.6154    0.1333    0.2192        60\n",
      "                     manuel-neri     0.0000    0.0000    0.0000        60\n",
      "                    marc-chagall     0.0000    0.0000    0.0000        60\n",
      "                  marcel-barbeau     0.1143    1.0000    0.2051        60\n",
      "              marcel-broodthaers     1.0000    1.0000    1.0000        60\n",
      "                  marcel-duchamp     0.0000    0.0000    0.0000        60\n",
      "                    marcel-janco     0.0000    0.0000    0.0000        60\n",
      "                   marcelle-cahn     0.0000    0.0000    0.0000        60\n",
      "                   marcus-larson     0.0236    0.8167    0.0458        60\n",
      "       marevna-(marie-vorobieff)     0.0000    0.0000    0.0000        60\n",
      "               margareta-sterian     0.0000    0.0000    0.0000        60\n",
      "    maria-helena-vieira-da-silva     0.2051    0.1333    0.1616        60\n",
      "               maria-primachenko     0.6667    0.0667    0.1212        60\n",
      "                  marianne-north     0.0000    0.0000    0.0000        60\n",
      "               marie-bracquemond     1.0000    0.0833    0.1538        60\n",
      "                 marie-laurencin     0.0000    0.0000    0.0000        60\n",
      "                  marin-gherasim     0.1453    1.0000    0.2537        60\n",
      "                  mario-cesariny     0.0000    0.0000    0.0000        60\n",
      "                      mario-eloy     0.7857    0.1833    0.2973        60\n",
      "                     mario-nuzzi     0.0000    0.0000    0.0000        60\n",
      "                  mario-schifano     0.0000    0.0000    0.0000        60\n",
      "                    mario-sironi     0.1010    0.8333    0.1802        60\n",
      "                    mario-zanini     0.0000    0.0000    0.0000        60\n",
      "                marjorie-strider     0.7500    0.1000    0.1765        60\n",
      "                     mark-rothko     0.0000    0.0000    0.0000        60\n",
      "                      mark-tobey     0.0000    0.0000    0.0000        60\n",
      "                  marko-pogacnik     0.1818    0.0333    0.0563        60\n",
      "                 marsden-hartley     1.0000    0.0500    0.0952        60\n",
      "                  martial-raysse     0.0000    0.0000    0.0000        60\n",
      "                    martin-barre     1.0000    0.1667    0.2857        60\n",
      "            martin-johnson-heade     0.0000    0.0000    0.0000        60\n",
      "               martin-schongauer     0.0000    0.0000    0.0000        60\n",
      "                 martiros-saryan     0.0000    0.0000    0.0000        60\n",
      "                    mary-cassatt     0.0000    0.0000    0.0000        60\n",
      "                     mary-fedden     0.6667    0.0333    0.0635        60\n",
      "                        masaccio     1.0000    0.0500    0.0952        60\n",
      "                   matej-sternen     0.0000    0.0000    0.0000        60\n",
      "                   matthias-stom     0.0000    0.0000    0.0000        60\n",
      "             maurice-de-vlaminck     1.0000    0.0500    0.0952        60\n",
      "                   maurice-denis     0.0000    0.0000    0.0000        60\n",
      "                  maurice-esteve     0.7059    0.2000    0.3117        60\n",
      "             maurice-prendergast     0.0000    0.0000    0.0000        60\n",
      "      maurice-quentin-de-la-tour     0.0083    0.4167    0.0164        60\n",
      "                 maurice-utrillo     0.1000    0.0333    0.0500        60\n",
      "                    max-beckmann     1.0000    0.0333    0.0645        60\n",
      "                        max-bill     0.3750    1.0000    0.5455        60\n",
      "                       max-ernst     1.0000    0.1333    0.2353        60\n",
      "                    max-kurzweil     0.0000    0.0000    0.0000        60\n",
      "                  max-liebermann     0.1667    0.0167    0.0303        60\n",
      "                   max-pechstein     0.5000    0.0167    0.0323        60\n",
      "                     max-slevogt     1.0000    0.0500    0.0952        60\n",
      "                       max-weber     0.0000    0.0000    0.0000        60\n",
      "                  maxim-vorobiev     0.5000    0.0333    0.0625        60\n",
      "                  maxime-lalanne     0.9444    0.2833    0.4359        60\n",
      "                   maxime-maufra     0.0000    0.0000    0.0000        60\n",
      "                      may-wilson     0.8889    0.6667    0.7619        60\n",
      "              medi-wechsler-dinu     0.7500    0.1500    0.2500        60\n",
      "                  meijer-de-haan     0.0000    0.0000    0.0000        60\n",
      "                     mel-bochner     0.5263    1.0000    0.6897        60\n",
      "                           menez     1.0000    0.0667    0.1250        60\n",
      "            micaela-eleutheriade     1.0000    0.0500    0.0952        60\n",
      "                    michael-bell     0.6250    0.0833    0.1471        60\n",
      "                  michel-carrade     0.1849    0.7333    0.2953        60\n",
      "                 michel-simonidy     0.0000    0.0000    0.0000        60\n",
      "                    michelangelo     0.0000    0.0000    0.0000        60\n",
      "         michelangelo-pistoletto     0.5714    0.0667    0.1194        60\n",
      "                 mihaly-munkacsy     0.0000    0.0000    0.0000        60\n",
      "            mikalojus-ciurlionis     0.0000    0.0000    0.0000        60\n",
      "                 mikhail-lebedev     0.0175    0.3667    0.0335        60\n",
      "                mikhail-nesterov     0.0000    0.0000    0.0000        60\n",
      "                  mikhail-vrubel     1.0000    0.0167    0.0328        60\n",
      "                  miklos-barabas     0.0000    0.0000    0.0000        60\n",
      "                    milton-avery     0.0000    0.0000    0.0000        60\n",
      "                  milton-resnick     0.0000    0.0000    0.0000        60\n",
      "                     mily-possoz     0.0000    0.0000    0.0000        60\n",
      "                   mira-schendel     0.9231    0.2000    0.3288        60\n",
      "                 miriam-schapiro     0.4167    0.0833    0.1389        60\n",
      "                   moise-kisling     0.0000    0.0000    0.0000        60\n",
      "                   morris-graves     0.0000    0.0000    0.0000        60\n",
      "                    morris-louis     0.7273    0.1333    0.2254        60\n",
      "                  mostafa-dashti     1.0000    0.0667    0.1250        60\n",
      "            mstislav-dobuzhinsky     0.0000    0.0000    0.0000        60\n",
      "                mykola-pymonenko     1.0000    0.0167    0.0328        60\n",
      "               mykola-yaroshenko     1.0000    0.0333    0.0645        60\n",
      "                     myron-stout     0.0000    0.0000    0.0000        60\n",
      "                      n.c.-wyeth     0.0000    0.0000    0.0000        60\n",
      "                  nassos-daphnis     0.2683    0.1833    0.2178        60\n",
      "              natalia-dumitresco     0.4348    1.0000    0.6061        60\n",
      "              natalia-goncharova     0.5000    0.0167    0.0323        60\n",
      "                   neil-welliver     0.0000    0.0000    0.0000        60\n",
      "             nicholas-krushenick     0.4167    0.1667    0.2381        60\n",
      "                nicholas-roerich     0.0000    0.0000    0.0000        60\n",
      "                 nicolae-darascu     1.0000    0.0333    0.0645        60\n",
      "              nicolae-grigorescu     1.0000    0.0333    0.0645        60\n",
      "                 nicolae-tonitza     0.0000    0.0000    0.0000        60\n",
      "                 nicolae-vermont     1.0000    0.0167    0.0328        60\n",
      "                nicolas-tournier     0.0000    0.0000    0.0000        60\n",
      "           niki-de-sainte-phalle     0.1250    0.1667    0.1429        60\n",
      "                  niko-pirosmani     0.4444    0.0667    0.1159        60\n",
      "                    nikola-tanev     0.7500    0.0500    0.0938        60\n",
      "                      nikolai-ge     0.0000    0.0000    0.0000        60\n",
      "                  nikolaos-gyzis     0.0000    0.0000    0.0000        60\n",
      "                 nikolaos-lytras     0.0000    0.0000    0.0000        60\n",
      "         nikolay-bogdanov-belsky     0.4000    0.0333    0.0615        60\n",
      "      nikos-hadjikyriakos-ghikas     0.0000    0.0000    0.0000        60\n",
      "                    norman-bluhm     0.0000    0.0000    0.0000        60\n",
      "                    nutzi-acontz     0.0000    0.0000    0.0000        60\n",
      "                     nzante-spee     0.0107    0.6833    0.0211        60\n",
      "                 octav-angheluta     0.0000    0.0000    0.0000        60\n",
      "                   octav-bancila     0.0000    0.0000    0.0000        60\n",
      "                    odilon-redon     0.0000    0.0000    0.0000        60\n",
      "                     ogata-gekko     1.0000    0.0500    0.0952        60\n",
      "             oleksandr-bogomazov     1.0000    0.2000    0.3333        60\n",
      "             olexandr-archipenko     1.0000    0.0500    0.0952        60\n",
      "                   olga-rozanova     0.6667    0.0667    0.1212        60\n",
      "                   olivier-debre     0.2745    0.2333    0.2523        60\n",
      "                 orest-kiprensky     0.0000    0.0000    0.0000        60\n",
      "                 oscar-dominguez     0.0000    0.0000    0.0000        60\n",
      "                     osias-beert     0.1971    0.6833    0.3060        60\n",
      "                 oskar-kokoschka     0.0000    0.0000    0.0000        60\n",
      "                     osman-hamdi     0.0000    0.0000    0.0000        60\n",
      "                   ossip-zadkine     1.0000    0.0333    0.0645        60\n",
      "              oswaldo-guayasamin     0.0349    0.1000    0.0517        60\n",
      "                    othon-friesz     1.0000    0.0333    0.0645        60\n",
      "                        otto-dix     0.0000    0.0000    0.0000        60\n",
      "                    otto-eckmann     0.8889    0.1333    0.2319        60\n",
      "                 otto-freundlich     0.1775    1.0000    0.3015        60\n",
      "            otto-gustav-carlsund     0.7857    0.1833    0.2973        60\n",
      "                   pablo-picasso     0.0000    0.0000    0.0000        60\n",
      "               panayiotis-tetsis     0.0000    0.0000    0.0000        60\n",
      "                   paolo-scheggi     0.1749    1.0000    0.2978        60\n",
      "                   paolo-uccello     1.0000    0.0667    0.1250        60\n",
      "                  paolo-veronese     0.0000    0.0000    0.0000        60\n",
      "                     park-seo-bo     1.0000    0.1000    0.1818        60\n",
      "                    parmigianino     1.0000    0.1500    0.2609        60\n",
      "                      pat-lipsky     0.0000    0.0000    0.0000        60\n",
      "               patrick-caulfield     0.3000    0.0500    0.0857        60\n",
      "                   patrick-heron     0.0000    0.0000    0.0000        60\n",
      "                patrick-procktor     0.0000    0.0000    0.0000        60\n",
      "                      paul-brach     0.4000    0.1333    0.2000        60\n",
      "                       paul-bril     0.0000    0.0000    0.0000        60\n",
      "                    paul-cezanne     0.0000    0.0000    0.0000        60\n",
      "                  paul-delaroche     0.0000    0.0000    0.0000        60\n",
      "                    paul-delvaux     0.5909    0.2167    0.3171        60\n",
      "                     paul-feeley     0.5000    0.1667    0.2500        60\n",
      "                    paul-gauguin     0.0000    0.0000    0.0000        60\n",
      "                    paul-jenkins     1.0000    0.0167    0.0328        60\n",
      "                       paul-klee     0.0000    0.0000    0.0000        60\n",
      "               paul-mathiopoulos     0.0000    0.0000    0.0000        60\n",
      "                       paul-reed     0.0000    0.0000    0.0000        60\n",
      "                   paul-serusier     0.0000    0.0000    0.0000        60\n",
      "                     paul-signac     0.0000    0.0000    0.0000        60\n",
      "          paula-modersohn-becker     0.0000    0.0000    0.0000        60\n",
      "                    pauline-boty     0.0000    0.0000    0.0000        60\n",
      "                   pavel-fedotov     1.0000    0.0500    0.0952        60\n",
      "                   pavel-filonov     0.0000    0.0000    0.0000        60\n",
      "                   pavel-svinyin     0.8750    0.1167    0.2059        60\n",
      "            peder-severin-kroyer     0.4000    0.0333    0.0615        60\n",
      "                   pedro-calapez     0.1667    0.0333    0.0556        60\n",
      "               pericles-pantazis     0.0000    0.0000    0.0000        60\n",
      "              periklis-vyzantios     0.0000    0.0000    0.0000        60\n",
      "                      perle-fine     1.0000    0.0167    0.0328        60\n",
      "                     peter-blake     0.5000    0.0167    0.0323        60\n",
      "                      peter-busa     0.4800    0.4000    0.4364        60\n",
      "                       peter-max     0.7647    0.2167    0.3377        60\n",
      "               peter-paul-rubens     0.0000    0.0000    0.0000        60\n",
      "                  peter-phillips     0.0000    0.0000    0.0000        60\n",
      "                   petre-abrudan     1.0000    0.0500    0.0952        60\n",
      "          petro-kholodny-(elder)     0.0000    0.0000    0.0000        60\n",
      "                 petrus-christus     1.0000    0.0500    0.0952        60\n",
      "                philip-de-laszlo     0.0000    0.0000    0.0000        60\n",
      "                   philip-guston     0.0000    0.0000    0.0000        60\n",
      "               philip-pearlstein     0.8571    0.1000    0.1791        60\n",
      "             philip-wilson-steer     0.0000    0.0000    0.0000        60\n",
      "                philippe-halsman     0.8000    1.0000    0.8889        60\n",
      "           piero-della-francesca     0.0000    0.0000    0.0000        60\n",
      "                 piero-di-cosimo     0.0000    0.0000    0.0000        60\n",
      "                   piero-dorazio     0.8750    0.1167    0.2059        60\n",
      "                   piero-manzoni     0.7273    0.2667    0.3902        60\n",
      "               pierre-alechinsky     0.0466    0.2667    0.0794        60\n",
      "           pierre-auguste-renoir     0.0000    0.0000    0.0000        60\n",
      "                  pierre-bonnard     0.0000    0.0000    0.0000        60\n",
      "                    pierre-daura     1.0000    0.0667    0.1250        60\n",
      "       pierre-puvis-de-chavannes     1.0000    0.0333    0.0645        60\n",
      "                 pierre-soulages     0.1250    0.1167    0.1207        60\n",
      "                 pierre-tal-coat     0.0000    0.0000    0.0000        60\n",
      "                   piet-mondrian     1.0000    0.0500    0.0952        60\n",
      "        pieter-bruegel-the-elder     0.0000    0.0000    0.0000        60\n",
      "                 pieter-de-hooch     0.0000    0.0000    0.0000        60\n",
      "                  pieter-wenning     0.3158    0.1000    0.1519        60\n",
      "               pietro-da-cortona     0.1667    0.0167    0.0303        60\n",
      "                   pietro-longhi     0.0000    0.0000    0.0000        60\n",
      "                 pietro-perugino     0.0000    0.0000    0.0000        60\n",
      "                    pino-pinelli     0.0342    0.4833    0.0638        60\n",
      "                    pinturicchio     0.8667    0.2167    0.3467        60\n",
      "                       pisanello     0.0000    0.0000    0.0000        60\n",
      "            polychronis-lembesis     0.0000    0.0000    0.0000        60\n",
      "       princess-fahrelnissa-zeid     0.2121    0.4667    0.2917        60\n",
      "              pyotr-konchalovsky     0.0000    0.0000    0.0000        60\n",
      "                     r.-b.-kitaj     1.0000    0.1000    0.1818        60\n",
      "                  radi-nedelchev     0.3939    0.2167    0.2796        60\n",
      "                     rafa-nasiri     0.4545    0.1667    0.2439        60\n",
      "                 rafael-zabaleta     0.5000    0.0500    0.0909        60\n",
      "                    ralph-goings     0.3626    0.5500    0.4371        60\n",
      "                    ralph-hotere     1.0000    0.0500    0.0952        60\n",
      "                 ralph-rosenborg     0.5000    0.1500    0.2308        60\n",
      "                    ramon-oviedo     0.0000    0.0000    0.0000        60\n",
      "                      raoul-dufy     0.0000    0.0000    0.0000        60\n",
      "                      raoul-ubac     0.0000    0.0000    0.0000        60\n",
      "                         raphael     0.0000    0.0000    0.0000        60\n",
      "                raphael-kirchner     0.0000    0.0000    0.0000        60\n",
      "                      ray-parker     0.8889    0.1333    0.2319        60\n",
      "                      red-grooms     0.5000    0.0500    0.0909        60\n",
      "                       rembrandt     0.0000    0.0000    0.0000        60\n",
      "                   remedios-varo     0.9677    1.0000    0.9836        60\n",
      "                   rene-bertholo     0.0000    0.0000    0.0000        60\n",
      "                   rene-magritte     1.0000    0.0333    0.0645        60\n",
      "             richard-artschwager     0.0000    0.0000    0.0000        60\n",
      "              richard-diebenkorn     0.0000    0.0000    0.0000        60\n",
      "                  richard-gerstl     0.0000    0.0000    0.0000        60\n",
      "                richard-hamilton     0.0000    0.0000    0.0000        60\n",
      "        richard-parkes-bonington     0.0000    0.0000    0.0000        60\n",
      "           richard-pousette-dart     0.8571    0.1000    0.1791        60\n",
      "                   richard-serra     0.1370    0.1667    0.1504        60\n",
      "                  richard-tuttle     0.0000    0.0000    0.0000        60\n",
      "                 richard-whitney     0.0000    0.0000    0.0000        60\n",
      "                     rik-wouters     1.0000    0.1000    0.1818        60\n",
      "                 robert-brackman     1.0000    0.0167    0.0328        60\n",
      "                   robert-campin     1.0000    0.1500    0.2609        60\n",
      "                 robert-delaunay     0.0000    0.0000    0.0000        60\n",
      "                robert-goodnough     0.0000    0.0000    0.0000        60\n",
      "                  robert-indiana     0.5333    0.1333    0.2133        60\n",
      "         robert-julian-onderdonk     0.0000    0.0000    0.0000        60\n",
      "                  robert-mangold     0.1000    0.0167    0.0286        60\n",
      "                   robert-morris     1.0000    0.2167    0.3562        60\n",
      "                   robert-nickle     0.0000    0.0000    0.0000        60\n",
      "                    robert-ryman     0.0000    0.0000    0.0000        60\n",
      "                  robert-silvers     0.6667    0.1333    0.2222        60\n",
      "                 robert-smithson     1.0000    0.3333    0.5000        60\n",
      "                   roberto-matta     0.8219    1.0000    0.9023        60\n",
      "                   rodolfo-arico     1.0000    0.0833    0.1538        60\n",
      "            roger-de-la-fresnaye     0.5000    0.0333    0.0625        60\n",
      "                       roger-fry     0.0000    0.0000    0.0000        60\n",
      "           rogier-van-der-weyden     0.0000    0.0000    0.0000        60\n",
      "                    roman-opalka     0.0176    1.0000    0.0346        60\n",
      "                  romare-bearden     1.0000    0.2333    0.3784        60\n",
      "                     ron-gorchov     0.1786    0.2500    0.2083        60\n",
      "                    ronald-davis     0.8182    0.1500    0.2535        60\n",
      "                       roni-horn     0.7500    0.0500    0.0938        60\n",
      "                ronnie-landfield     1.0000    0.0167    0.0328        60\n",
      "                 rosalyn-drexler     0.2708    0.2167    0.2407        60\n",
      "                rosso-fiorentino     0.7500    0.0500    0.0938        60\n",
      "                roy-lichtenstein     0.5714    0.0667    0.1194        60\n",
      "       rudolf-schweitzer-cumpana     0.0000    0.0000    0.0000        60\n",
      "                  rudolf-von-alt     0.4545    0.0833    0.1408        60\n",
      "                   rufino-tamayo     0.0690    0.1667    0.0976        60\n",
      "                    ruth-vollmer     0.6316    0.2000    0.3038        60\n",
      "                     sa-nogueira     0.2286    0.1333    0.1684        60\n",
      "                   salvador-dali     0.0000    0.0000    0.0000        60\n",
      "                     sam-francis     1.0000    0.0167    0.0328        60\n",
      "                     sam-gilliam     1.0000    0.0333    0.0645        60\n",
      "                  samuel-mutzner     0.0000    0.0000    0.0000        60\n",
      "               sandro-botticelli     1.0000    0.0167    0.0328        60\n",
      "                santiago-rusinol     0.0000    0.0000    0.0000        60\n",
      "                  saul-steinberg     0.0000    0.0000    0.0000        60\n",
      "                     sean-scully     0.8333    0.0833    0.1515        60\n",
      "               sebastien-bourdon     0.0000    0.0000    0.0000        60\n",
      "                 seraphine-louis     0.0000    0.0000    0.0000        60\n",
      "                serge-charchoune     0.0000    0.0000    0.0000        60\n",
      "                  serge-sudeikin     0.0000    0.0000    0.0000        60\n",
      "                  sergey-solomko     1.0000    0.0333    0.0645        60\n",
      "                    sever-burada     0.0000    0.0000    0.0000        60\n",
      "                    sidney-nolan     1.0000    0.2000    0.3333        60\n",
      "        sir-lawrence-alma-tadema     0.0000    0.0000    0.0000        60\n",
      "            sofonisba-anguissola     0.0000    0.0000    0.0000        60\n",
      "                      sol-lewitt     0.0000    0.0000    0.0000        60\n",
      "                  sonia-delaunay     1.0000    0.4667    0.6364        60\n",
      "                  sonya-rapoport     0.0826    1.0000    0.1527        60\n",
      "                 sorin-ilfoveanu     0.2593    0.4667    0.3333        60\n",
      "               spyros-papaloukas     1.0000    0.0333    0.0645        60\n",
      "                  stanley-pinker     0.0000    0.0000    0.0000        60\n",
      "               stefan-dimitrescu     0.0000    0.0000    0.0000        60\n",
      "                  stefan-luchian     0.0000    0.0000    0.0000        60\n",
      "                  stefan-popescu     0.0000    0.0000    0.0000        60\n",
      "                    stuart-davis     0.1860    0.1333    0.1553        60\n",
      "                 suzanne-valadon     1.0000    0.0167    0.0328        60\n",
      "                      sven-lukin     0.0000    0.0000    0.0000        60\n",
      "                    t.-c.-steele     0.0000    0.0000    0.0000        60\n",
      "                takashi-murakami     0.0469    1.0000    0.0896        60\n",
      "                      tano-festa     0.0000    0.0000    0.0000        60\n",
      "                taras-shevchenko     0.5000    0.0167    0.0323        60\n",
      "                   taro-yamamoto     0.4444    0.1333    0.2051        60\n",
      "               tarsila-do-amaral     1.0000    0.0500    0.0952        60\n",
      "                     terry-frost     0.7692    0.1667    0.2740        60\n",
      "            thalia-flora-karavia     1.0000    0.0333    0.0645        60\n",
      "               theo-van-doesburg     0.0000    0.0000    0.0000        60\n",
      "           theo-van-rysselberghe     0.5000    0.0167    0.0323        60\n",
      "            theodoor-van-thulden     0.0000    0.0000    0.0000        60\n",
      "                    theodor-aman     0.0000    0.0000    0.0000        60\n",
      "                 theodor-pallady     1.0000    0.0167    0.0328        60\n",
      "       theodor-severin-kittelsen     0.7500    0.0500    0.0938        60\n",
      "             theodore-chasseriau     0.0000    0.0000    0.0000        60\n",
      "              theodore-gericault     0.0000    0.0000    0.0000        60\n",
      "               theodore-rousseau     1.0000    0.0167    0.0328        60\n",
      "                theodoros-stamos     0.0000    0.0000    0.0000        60\n",
      "    theophrastos-triantafyllidis     1.0000    0.0333    0.0645        60\n",
      "                     thomas-cole     0.0000    0.0000    0.0000        60\n",
      "                  thomas-downing     0.1630    1.0000    0.2804        60\n",
      "                   thomas-eakins     0.0000    0.0000    0.0000        60\n",
      "             thomas-gainsborough     0.0000    0.0000    0.0000        60\n",
      "                  thomas-kinkade     0.2290    1.0000    0.3727        60\n",
      "                    thomas-moran     0.0000    0.0000    0.0000        60\n",
      "            thomas-theodor-heine     0.0000    0.0000    0.0000        60\n",
      "                       tia-peltz     0.0000    0.0000    0.0000        60\n",
      "                      tintoretto     0.0000    0.0000    0.0000        60\n",
      "                          titian     1.0000    0.0167    0.0328        60\n",
      "       tivadar-kosztka-csontvary     0.0000    0.0000    0.0000        60\n",
      "                     tom-thomson     0.0000    0.0000    0.0000        60\n",
      "                      tony-smith     1.0000    0.2833    0.4416        60\n",
      "                           toyen     0.0000    0.0000    0.0000        60\n",
      "               tsuguharu-foujita     1.0000    0.0500    0.0952        60\n",
      "             tsukioka-yoshitoshi     0.0000    0.0000    0.0000        60\n",
      "                tsuruko-yamazaki     1.0000    0.1667    0.2857        60\n",
      "                umberto-boccioni     0.0000    0.0000    0.0000        60\n",
      "                utagawa-kunisada     1.0000    0.0167    0.0328        60\n",
      "             utagawa-kunisada-ii     0.9167    0.1833    0.3056        60\n",
      "               utagawa-kuniyoshi     0.0000    0.0000    0.0000        60\n",
      "                utagawa-sadatora     0.0000    0.0000    0.0000        60\n",
      "                utagawa-toyokuni     0.7500    0.0500    0.0938        60\n",
      "             utagawa-toyokuni-ii     0.0000    0.0000    0.0000        60\n",
      "                     vajda-lajos     1.0000    0.0333    0.0645        60\n",
      "                  valentin-serov     0.0000    0.0000    0.0000        60\n",
      "                   valerio-adami     0.1667    0.0333    0.0556        60\n",
      "                    vanessa-bell     1.0000    0.0167    0.0328        60\n",
      "                vangel-naumovski     0.1307    1.0000    0.2312        60\n",
      "                  vasile-dobrian     0.0000    0.0000    0.0000        60\n",
      "                    vasile-kazar     0.0000    0.0000    0.0000        60\n",
      "                  vasile-popescu     0.0000    0.0000    0.0000        60\n",
      "                    vasily-perov     0.0000    0.0000    0.0000        60\n",
      "                  vasily-polenov     0.0000    0.0000    0.0000        60\n",
      "               vasily-sadovnikov     0.8000    0.1333    0.2286        60\n",
      "                  vasily-surikov     0.0000    0.0000    0.0000        60\n",
      "                 vasily-tropinin     0.0000    0.0000    0.0000        60\n",
      "             vasily-vereshchagin     0.0000    0.0000    0.0000        60\n",
      "                    vela-zanetti     0.0000    0.0000    0.0000        60\n",
      "               vicente-manansala     0.0000    0.0000    0.0000        60\n",
      "          victor-borisov-musatov     0.0000    0.0000    0.0000        60\n",
      "                  victor-brauner     0.4545    0.0833    0.1408        60\n",
      "                     victor-hugo     0.0000    0.0000    0.0000        60\n",
      "                victor-meirelles     1.0000    0.1167    0.2090        60\n",
      "                  victor-pasmore     1.0000    0.1167    0.2090        60\n",
      "                vieira-portuense     0.0000    0.0000    0.0000        60\n",
      "                viktor-vasnetsov     1.0000    0.0167    0.0328        60\n",
      "                vilmos-aba-novak     0.0000    0.0000    0.0000        60\n",
      "                vincent-van-gogh     0.0000    0.0000    0.0000        60\n",
      "                viorel-marginean     1.0000    0.0167    0.0328        60\n",
      "               vittore-carpaccio     0.0000    0.0000    0.0000        60\n",
      "           vladimir-borovikovsky     0.0000    0.0000    0.0000        60\n",
      "               vladimir-dimitrov     1.0000    0.1333    0.2353        60\n",
      "               vladimir-makovsky     0.0000    0.0000    0.0000        60\n",
      "                 vladimir-tatlin     0.8333    0.1667    0.2778        60\n",
      "              volodymyr-orlovsky     0.0000    0.0000    0.0000        60\n",
      "                    walasse-ting     1.0000    0.0500    0.0952        60\n",
      "                  walter-battiss     0.6667    0.0333    0.0635        60\n",
      "            walter-darby-bannard     0.1667    0.0500    0.0769        60\n",
      "                 walter-de-maria     0.4286    0.2500    0.3158        60\n",
      "                  walter-sickert     0.0000    0.0000    0.0000        60\n",
      "               wassily-kandinsky     1.0000    0.0167    0.0328        60\n",
      "                     wifredo-lam     1.0000    0.1333    0.2353        60\n",
      "             wilhelm-kotarbinski     0.0000    0.0000    0.0000        60\n",
      "                   wilhelm-leibl     0.0000    0.0000    0.0000        60\n",
      "                 willard-metcalf     0.0000    0.0000    0.0000        60\n",
      "               willem-de-kooning     1.0000    0.0167    0.0328        60\n",
      "                     willem-kalf     0.0000    0.0000    0.0000        60\n",
      "                willi-baumeister     0.0000    0.0000    0.0000        60\n",
      "      william-adolphe-bouguereau     0.0135    0.0667    0.0225        60\n",
      "                william-baziotes     0.1600    0.0667    0.0941        60\n",
      "                   william-blake     0.0000    0.0000    0.0000        60\n",
      "                 william-congdon     0.0276    0.6333    0.0528        60\n",
      "              william-h.-johnson     0.0000    0.0000    0.0000        60\n",
      "                 william-hogarth     0.0000    0.0000    0.0000        60\n",
      "             william-holman-hunt     0.0000    0.0000    0.0000        60\n",
      "          william-james-glackens     0.0000    0.0000    0.0000        60\n",
      "           william-merritt-chase     0.0000    0.0000    0.0000        60\n",
      "                   william-scott     0.0000    0.0000    0.0000        60\n",
      "                  william-shayer     0.0048    0.0667    0.0089        60\n",
      "                  william-turner     0.0000    0.0000    0.0000        60\n",
      "                   winslow-homer     0.0000    0.0000    0.0000        60\n",
      "               winston-churchill     0.0000    0.0000    0.0000        60\n",
      "                 wolfgang-paalen     0.0000    0.0000    0.0000        60\n",
      "                    wu-guanzhong     0.4545    0.1667    0.2439        60\n",
      "                      xu-beihong     0.9091    1.0000    0.9524        60\n",
      "                    yayoi-kusama     0.5789    0.1833    0.2785        60\n",
      "                 yiannis-moralis     0.0000    0.0000    0.0000        60\n",
      "              yiannis-tsaroychis     0.0000    0.0000    0.0000        60\n",
      "                yov-kondzelevych     0.0000    0.0000    0.0000        60\n",
      "                    yves-gaucher     0.6667    0.1333    0.2222        60\n",
      "                      yves-klein     0.3333    0.0167    0.0317        60\n",
      "            zinaida-serebriakova     0.0000    0.0000    0.0000        60\n",
      "\n",
      "                        accuracy                         0.0973     66240\n",
      "                       macro avg     0.3198    0.0973    0.0900     66240\n",
      "                    weighted avg     0.3198    0.0973    0.0900     66240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Precision: 0.3198\n",
      "🔄 Recall: 0.0973\n",
      "🔥 F1-score: 0.0900\n"
     ]
    }
   ],
   "source": [
    "# y_test는 실제 정답 (원-핫 인코딩된 경우 argmax로 변환)\n",
    "y_test_labels = test_y\n",
    "y_pred_labels = rf_pred_y  # 모델 예측값도 동일하게 변환\n",
    "\n",
    "# ✅ 상세한 성능 지표 출력\n",
    "print(\"🔹 Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels, digits=4))\n",
    "\n",
    "# ✅ 개별 지표 계산 (매크로 평균 사용)\n",
    "precision = precision_score(y_test_labels, y_pred_labels, average='macro')\n",
    "recall = recall_score(y_test_labels, y_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average='macro')\n",
    "\n",
    "print(f\"🎯 Precision: {precision:.4f}\")\n",
    "print(f\"🔄 Recall: {recall:.4f}\")\n",
    "print(f\"🔥 F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb2fe7cd-96f7-43a1-b50c-0d85b8e781b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12776278071493863\n",
      "0.0899808728893325\n",
      "0.08118411589340498\n"
     ]
    }
   ],
   "source": [
    "# fbeta score의 beta=1 : f1 score\n",
    "# fbeta score의 2>=beta>1 : recall의 가중치가 높게 조정된 f1 score\n",
    "# fbeta score의 0<=beta<1 : precision의 가중치가 높게 조정된 f1 score\n",
    "print(fbeta_score(test_y, rf_pred_y, beta=0.5, average='macro'))\n",
    "print(fbeta_score(test_y, rf_pred_y, beta=1, average='macro'))\n",
    "print(fbeta_score(test_y, rf_pred_y, beta=2, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b0f36-717b-4b66-9edf-cce60e2b3874",
   "metadata": {},
   "source": [
    "## MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4381d0af-e23d-4891-8de6-7a4fa8b03001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(38, 64, 32), max_iter=500, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(38, 64, 32), max_iter=500, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(38, 64, 32), max_iter=500, random_state=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다중신경망 모형\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(38, 64, 32), max_iter=500, random_state=1)\n",
    "mlp_model = mlp_model.fit(train_X, train_y)\n",
    "mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c64888-f5db-4e54-a095-83be853350de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4549969806763285"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db7232c0-02f8-444d-9124-ecbbbfabb9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <th>aaron-siskind</th>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <th>abidin-dino</th>\n",
       "      <th>abraham-manievich</th>\n",
       "      <th>ad-reinhardt</th>\n",
       "      <th>adam-baltatu</th>\n",
       "      <th>adnan-coker</th>\n",
       "      <th>adolf-fleischmann</th>\n",
       "      <th>adolf-hitler</th>\n",
       "      <th>...</th>\n",
       "      <th>wolfgang-paalen</th>\n",
       "      <th>wu-guanzhong</th>\n",
       "      <th>xu-beihong</th>\n",
       "      <th>yayoi-kusama</th>\n",
       "      <th>yiannis-moralis</th>\n",
       "      <th>yiannis-tsaroychis</th>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <th>yves-gaucher</th>\n",
       "      <th>yves-klein</th>\n",
       "      <th>zinaida-serebriakova</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-siskind</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abidin-dino</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abraham-manievich</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiannis-tsaroychis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-gaucher</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-klein</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zinaida-serebriakova</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 1084 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                  a.y.-jackson  aaron-siskind  abdullah-suriosubroto  \\\n",
       "row_0                                                                       \n",
       "a.y.-jackson                     27              0                      0   \n",
       "aaron-siskind                     0             27                      0   \n",
       "abdullah-suriosubroto             0              0                     60   \n",
       "abidin-dino                       0              0                      0   \n",
       "abraham-manievich                 0              0                      0   \n",
       "...                             ...            ...                    ...   \n",
       "yiannis-tsaroychis                0              0                      0   \n",
       "yov-kondzelevych                  0              0                      0   \n",
       "yves-gaucher                      0              0                      0   \n",
       "yves-klein                        0              0                      0   \n",
       "zinaida-serebriakova              0              0                      0   \n",
       "\n",
       "col_0                  abidin-dino  abraham-manievich  ad-reinhardt  \\\n",
       "row_0                                                                 \n",
       "a.y.-jackson                     0                  0             0   \n",
       "aaron-siskind                    0                  0             0   \n",
       "abdullah-suriosubroto            0                  0             0   \n",
       "abidin-dino                     60                  0             0   \n",
       "abraham-manievich                0                 20             0   \n",
       "...                            ...                ...           ...   \n",
       "yiannis-tsaroychis               0                  0             0   \n",
       "yov-kondzelevych                 0                  0             0   \n",
       "yves-gaucher                     0                  0             0   \n",
       "yves-klein                       0                  0             0   \n",
       "zinaida-serebriakova             0                  0             0   \n",
       "\n",
       "col_0                  adam-baltatu  adnan-coker  adolf-fleischmann  \\\n",
       "row_0                                                                 \n",
       "a.y.-jackson                      0            0                  0   \n",
       "aaron-siskind                     0            0                  0   \n",
       "abdullah-suriosubroto             0            0                  0   \n",
       "abidin-dino                       0            0                  0   \n",
       "abraham-manievich                 0            0                  0   \n",
       "...                             ...          ...                ...   \n",
       "yiannis-tsaroychis                0            0                  0   \n",
       "yov-kondzelevych                  0            0                  0   \n",
       "yves-gaucher                      0            0                  0   \n",
       "yves-klein                        0            0                  0   \n",
       "zinaida-serebriakova              0            0                  0   \n",
       "\n",
       "col_0                  adolf-hitler  ...  wolfgang-paalen  wu-guanzhong  \\\n",
       "row_0                                ...                                  \n",
       "a.y.-jackson                      0  ...                0             0   \n",
       "aaron-siskind                     0  ...                0             0   \n",
       "abdullah-suriosubroto             0  ...                0             0   \n",
       "abidin-dino                       0  ...                0             0   \n",
       "abraham-manievich                 0  ...                0             0   \n",
       "...                             ...  ...              ...           ...   \n",
       "yiannis-tsaroychis                0  ...                0             0   \n",
       "yov-kondzelevych                  0  ...                0             0   \n",
       "yves-gaucher                      0  ...                0             0   \n",
       "yves-klein                        0  ...                0             0   \n",
       "zinaida-serebriakova              0  ...                0             0   \n",
       "\n",
       "col_0                  xu-beihong  yayoi-kusama  yiannis-moralis  \\\n",
       "row_0                                                              \n",
       "a.y.-jackson                    0             0                0   \n",
       "aaron-siskind                   0             0                0   \n",
       "abdullah-suriosubroto           0             0                0   \n",
       "abidin-dino                     0             0                0   \n",
       "abraham-manievich               0             0                0   \n",
       "...                           ...           ...              ...   \n",
       "yiannis-tsaroychis              0             0                0   \n",
       "yov-kondzelevych                0             0                0   \n",
       "yves-gaucher                    0             0                0   \n",
       "yves-klein                      0             0                0   \n",
       "zinaida-serebriakova            0             0                0   \n",
       "\n",
       "col_0                  yiannis-tsaroychis  yov-kondzelevych  yves-gaucher  \\\n",
       "row_0                                                                       \n",
       "a.y.-jackson                            0                 0             0   \n",
       "aaron-siskind                           0                 0             0   \n",
       "abdullah-suriosubroto                   0                 0             0   \n",
       "abidin-dino                             0                 0             0   \n",
       "abraham-manievich                       0                 0             0   \n",
       "...                                   ...               ...           ...   \n",
       "yiannis-tsaroychis                      2                 0             0   \n",
       "yov-kondzelevych                        0                60             0   \n",
       "yves-gaucher                            0                 0            24   \n",
       "yves-klein                              0                 0             0   \n",
       "zinaida-serebriakova                    1                 0             0   \n",
       "\n",
       "col_0                  yves-klein  zinaida-serebriakova  \n",
       "row_0                                                    \n",
       "a.y.-jackson                    0                     0  \n",
       "aaron-siskind                   0                     0  \n",
       "abdullah-suriosubroto           0                     0  \n",
       "abidin-dino                     0                     0  \n",
       "abraham-manievich               0                     0  \n",
       "...                           ...                   ...  \n",
       "yiannis-tsaroychis              0                     0  \n",
       "yov-kondzelevych                0                     0  \n",
       "yves-gaucher                    2                     0  \n",
       "yves-klein                     47                     0  \n",
       "zinaida-serebriakova            0                     3  \n",
       "\n",
       "[1104 rows x 1084 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pred_y = mlp_model.predict(test_X)\n",
    "mlp_result = pd.crosstab(test_y, mlp_pred_y)\n",
    "mlp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45daa95c-5ed0-4266-ac04-c7d7dde36a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                    a.y.-jackson     0.4154    0.4500    0.4320        60\n",
      "                   aaron-siskind     0.3462    0.4500    0.3913        60\n",
      "           abdullah-suriosubroto     0.6316    1.0000    0.7742        60\n",
      "                     abidin-dino     0.6383    1.0000    0.7792        60\n",
      "               abraham-manievich     0.2857    0.3333    0.3077        60\n",
      "                    ad-reinhardt     0.1389    0.0833    0.1042        60\n",
      "                    adam-baltatu     0.0455    0.0167    0.0244        60\n",
      "                     adnan-coker     1.0000    1.0000    1.0000        60\n",
      "               adolf-fleischmann     0.9524    1.0000    0.9756        60\n",
      "                    adolf-hitler     0.2899    0.3333    0.3101        60\n",
      "adolphe-joseph-thomas-monticelli     0.2500    0.1833    0.2115        60\n",
      "                 adriaen-brouwer     0.1746    0.1833    0.1789        60\n",
      "            adriaen-van-de-velde     0.2927    0.2000    0.2376        60\n",
      "            adriaen-van-de-venne     0.0000    0.0000    0.0000        60\n",
      "              adriaen-van-ostade     0.0794    0.0833    0.0813        60\n",
      "                    aelbert-cuyp     0.6429    0.7500    0.6923        60\n",
      "                            afro     1.0000    1.0000    1.0000        60\n",
      "                    agnes-martin     0.4714    0.5500    0.5077        60\n",
      "                 agnolo-bronzino     0.0769    0.0500    0.0606        60\n",
      "               agostino-carracci     0.7407    0.3333    0.4598        60\n",
      "                      aki-kuroda     0.5490    0.4667    0.5045        60\n",
      "           akseli-gallen-kallela     0.4286    0.0500    0.0896        60\n",
      "                         al-held     0.9524    1.0000    0.9756        60\n",
      "         aladar-korosfoi-kriesch     0.1762    0.5667    0.2688        60\n",
      "                albert-bierstadt     0.1200    0.1000    0.1091        60\n",
      "                    albert-bloch     0.0741    0.0333    0.0460        60\n",
      "                  albert-gleizes     0.6667    0.2333    0.3457        60\n",
      "                     albert-huie     0.7792    1.0000    0.8759        60\n",
      "                  albert-marquet     0.8000    0.0667    0.1231        60\n",
      "            albert-pinkham-ryder     0.5941    1.0000    0.7453        60\n",
      "                   alberto-burri     0.8333    1.0000    0.9091        60\n",
      "                alberto-carneiro     0.8451    1.0000    0.9160        60\n",
      "                alberto-magnelli     0.8451    1.0000    0.9160        60\n",
      "              albrecht-altdorfer     0.0000    0.0000    0.0000        60\n",
      "                  albrecht-durer     0.0270    0.0167    0.0206        60\n",
      "                 aldemir-martins     0.3235    0.5500    0.4074        60\n",
      "                    aldo-mondino     0.4262    0.4333    0.4298        60\n",
      "              alekos-kontopoulos     0.0847    0.0833    0.0840        60\n",
      "               aleksandra-ekster     0.8451    1.0000    0.9160        60\n",
      "                aleksey-antropov     0.1678    0.4000    0.2365        60\n",
      "                aleksey-savrasov     0.1311    0.1333    0.1322        60\n",
      "                        alex-hay     0.8824    1.0000    0.9375        60\n",
      "                alexander-calder     0.7632    0.4833    0.5918        60\n",
      "                alexander-ivanov     0.1584    0.2667    0.1988        60\n",
      "              alexander-liberman     0.9677    1.0000    0.9836        60\n",
      "              alexander-orlowski     0.1848    0.2833    0.2237        60\n",
      "                alexander-shilov     0.3136    0.6167    0.4157        60\n",
      "                alexandre-benois     0.3000    0.1000    0.1500        60\n",
      "               alexey-bogolyubov     0.2308    0.2500    0.2400        60\n",
      "              alexey-venetsianov     0.0440    0.0667    0.0530        60\n",
      "                    alexey-zubov     0.5889    0.8833    0.7067        60\n",
      "                   alfred-jensen     1.0000    1.0000    1.0000        60\n",
      "                    alfred-kubin     0.1806    0.4667    0.2605        60\n",
      "                alfred-manessier     1.0000    1.0000    1.0000        60\n",
      "                   alfred-sisley     0.0667    0.1833    0.0978        60\n",
      "                  alfred-stevens     0.1429    0.2000    0.1667        60\n",
      "                     alice-baber     0.8833    0.8833    0.8833        60\n",
      "                alighiero-boetti     0.7162    0.8833    0.7910        60\n",
      "                    allan-ramsay     0.1429    0.1333    0.1379        60\n",
      "                     allen-jones     0.5781    0.6167    0.5968        60\n",
      "             alma-woodsey-thomas     0.7273    0.6667    0.6957        60\n",
      "                     alonzo-cano     0.1351    0.2500    0.1754        60\n",
      "                  alphonse-mucha     0.1324    0.1500    0.1406        60\n",
      "                     alvaro-lapa     0.8108    1.0000    0.8955        60\n",
      "         amadeo-de-souza-cardoso     0.1111    0.1167    0.1138        60\n",
      "                 amedee-ozenfant     0.5455    0.7000    0.6131        60\n",
      "               amedeo-modigliani     0.1522    0.2333    0.1842        60\n",
      "                 amrita-sher-gil     0.3222    0.4833    0.3867        60\n",
      "                     anders-zorn     0.0857    0.0500    0.0632        60\n",
      "                  andre-bauchant     0.1646    0.2167    0.1871        60\n",
      "                    andre-derain     0.6000    0.0500    0.0923        60\n",
      "       andre-dunoyer-de-segonzac     0.2151    0.3333    0.2614        60\n",
      "                   andre-lanskoy     0.9677    1.0000    0.9836        60\n",
      "                    andre-masson     0.3125    0.3333    0.3226        60\n",
      "              andre-pierre-arnal     0.9464    0.8833    0.9138        60\n",
      "             andrea-del-castagno     0.4062    0.4333    0.4194        60\n",
      "                andrea-del-sarto     0.1538    0.1000    0.1212        60\n",
      "           andrea-del-verrocchio     0.8000    1.0000    0.8889        60\n",
      "                 andrea-mantegna     0.0000    0.0000    0.0000        60\n",
      "                  andrea-solario     0.3400    0.5667    0.4250        60\n",
      "               andrei-ryabushkin     0.2121    0.1167    0.1505        60\n",
      "                     andy-warhol     0.0000    0.0000    0.0000        60\n",
      "                 angelo-de-sousa     0.9737    0.6167    0.7551        60\n",
      "                  anita-malfatti     0.2800    0.3500    0.3111        60\n",
      "        anna-ostroumova-lebedeva     0.1905    0.0667    0.0988        60\n",
      "                    anne-appleby     0.6712    0.8167    0.7368        60\n",
      "                     anne-truitt     0.4231    0.5500    0.4783        60\n",
      "               annibale-carracci     0.0000    0.0000    0.0000        60\n",
      "                anthony-van-dyck     0.3333    0.0167    0.0317        60\n",
      "               antoine-blanchard     0.6081    0.7500    0.6716        60\n",
      "                   antoine-pesne     0.0000    0.0000    0.0000        60\n",
      "                 antoine-watteau     0.0833    0.0667    0.0741        60\n",
      "                      anton-azbe     0.2788    0.7667    0.4089        60\n",
      "                    anton-melbye     0.6456    0.8500    0.7338        60\n",
      "            antonello-da-messina     0.2637    0.4000    0.3179        60\n",
      "                   antoni-tapies     1.0000    1.0000    1.0000        60\n",
      "                antonio-carneiro     0.4397    0.8500    0.5795        60\n",
      "                 antonio-ligabue     0.3750    0.6000    0.4615        60\n",
      "                  antonio-palolo     0.7910    0.8833    0.8346        60\n",
      "              aristarkh-lentulov     0.5490    0.9333    0.6914        60\n",
      "                 arkhip-kuindzhi     0.0213    0.0167    0.0187        60\n",
      "                           arman     0.9836    1.0000    0.9917        60\n",
      "                 arman-manookian     0.4800    0.2000    0.2824        60\n",
      "               armand-guillaumin     0.1857    0.2167    0.2000        60\n",
      "                   arshile-gorky     0.0000    0.0000    0.0000        60\n",
      "           artemisia-gentileschi     0.4483    0.8667    0.5909        60\n",
      "                     arthur-dove     0.9375    1.0000    0.9677        60\n",
      "                   arthur-hughes     0.1429    0.0500    0.0741        60\n",
      "                     arthur-lowe     1.0000    1.0000    1.0000        60\n",
      "                 arthur-pinajian     0.7595    1.0000    0.8633        60\n",
      "                    arthur-segal     0.2778    0.1667    0.2083        60\n",
      "                   arthur-verona     0.2353    0.5333    0.3265        60\n",
      "                    arturo-souto     0.0000    0.0000    0.0000        60\n",
      "                asgrimur-jonsson     0.1605    0.2167    0.1844        60\n",
      "                   atsuko-tanaka     0.8871    0.9167    0.9016        60\n",
      "                aubrey-beardsley     0.2632    0.1667    0.2041        60\n",
      "                    audrey-flack     0.7714    0.9000    0.8308        60\n",
      "                    august-macke     0.0000    0.0000    0.0000        60\n",
      "                  auguste-herbin     0.1667    0.0500    0.0769        60\n",
      "                   auguste-rodin     0.2821    0.1833    0.2222        60\n",
      "                   augustus-john     0.1333    0.0333    0.0533        60\n",
      "                     aurel-cojan     0.9677    1.0000    0.9836        60\n",
      "                         balthus     0.2222    0.0667    0.1026        60\n",
      "                  barnett-newman     0.4038    0.3500    0.3750        60\n",
      "               bartolome-bermejo     0.6742    1.0000    0.8054        60\n",
      "       bartolome-esteban-murillo     0.1875    0.0500    0.0789        60\n",
      "                   basil-beattie     0.6512    0.9333    0.7671        60\n",
      "                 basuki-abdullah     0.1176    0.0667    0.0851        60\n",
      "                     bela-czobel     0.2766    0.2167    0.2430        60\n",
      "                      bela-kadar     0.5464    0.8833    0.6752        60\n",
      "                   ben-nicholson     0.7465    0.8833    0.8092        60\n",
      "                   benjamin-west     0.0000    0.0000    0.0000        60\n",
      "                 benozzo-gozzoli     0.1852    0.1667    0.1754        60\n",
      "               bernardo-bellotto     0.3714    0.2167    0.2737        60\n",
      "                bernardo-strozzi     0.2609    0.2000    0.2264        60\n",
      "                bernhard-strigel     0.1597    0.3833    0.2255        60\n",
      "                    bertalan-por     0.1092    0.2167    0.1453        60\n",
      "                  berthe-morisot     0.0455    0.0167    0.0244        60\n",
      "                   betty-parsons     0.0755    0.0667    0.0708        60\n",
      "                     billy-apple     0.3191    0.2500    0.2804        60\n",
      "                  billy-childish     0.4227    0.6833    0.5223        60\n",
      "                  blinky-palermo     0.5065    0.6500    0.5693        60\n",
      "                 boris-grigoriev     0.7887    0.9333    0.8550        60\n",
      "                 boris-kustodiev     0.0000    0.0000    0.0000        60\n",
      "           bradley-walker-tomlin     0.6203    0.8167    0.7050        60\n",
      "                  brett-whiteley     0.5735    0.6500    0.6094        60\n",
      "                    brice-marden     0.1739    0.0667    0.0964        60\n",
      "                   bridget-riley     0.7703    0.9500    0.8507        60\n",
      "                    bruce-nauman     0.4878    1.0000    0.6557        60\n",
      "                   bui-xuan-phai     0.2976    0.4167    0.3472        60\n",
      "                 burhan-dogancay     0.2609    0.1000    0.1446        60\n",
      "                 camille-bombois     0.5926    0.5333    0.5614        60\n",
      "                   camille-corot     0.0000    0.0000    0.0000        60\n",
      "                camille-pissarro     0.0000    0.0000    0.0000        60\n",
      "                       canaletto     0.2542    0.2500    0.2521        60\n",
      "               candido-portinari     0.0000    0.0000    0.0000        60\n",
      "                      caravaggio     0.1500    0.1500    0.1500        60\n",
      "                      carl-bloch     1.0000    1.0000    1.0000        60\n",
      "                    carl-larsson     0.1613    0.1667    0.1639        60\n",
      "  carl-ludwig-johann-christineck     0.2917    0.5833    0.3889        60\n",
      "                  carlo-crivelli     0.1364    0.1500    0.1429        60\n",
      "                  carlos-botelho     0.6835    0.9000    0.7770        60\n",
      "                   carlos-merida     0.3333    0.2833    0.3063        60\n",
      "            carlos-orozco-romero     0.8108    1.0000    0.8955        60\n",
      "          carlos-saenz-de-tejada     0.1667    0.0500    0.0769        60\n",
      "          caspar-david-friedrich     0.1429    0.0167    0.0299        60\n",
      "                   chaim-soutine     0.1446    0.2000    0.1678        60\n",
      "                  charles-cottet     0.2053    0.6500    0.3120        60\n",
      "                  charles-demuth     0.4245    0.7500    0.5422        60\n",
      "       charles-francois-daubigny     0.1867    0.2333    0.2074        60\n",
      "                 charles-hermans     0.4783    0.7333    0.5789        60\n",
      "                  charles-hinman     1.0000    0.9167    0.9565        60\n",
      "                charles-lapicque     0.5312    0.2833    0.3696        60\n",
      "                 charles-reiffel     0.3590    0.7000    0.4746        60\n",
      "                 charles-sheeler     1.0000    0.8333    0.9091        60\n",
      "                   childe-hassam     0.0000    0.0000    0.0000        60\n",
      "                  christen-kobke     0.2766    0.2167    0.2430        60\n",
      "                 christian-schad     1.0000    1.0000    1.0000        60\n",
      "                 christo-coetzee     0.9231    1.0000    0.9600        60\n",
      "  christoffer-wilhelm-eckersberg     0.9375    1.0000    0.9677        60\n",
      "               chronis-botsoglou     0.1154    0.0500    0.0698        60\n",
      "                     chuck-close     0.8696    1.0000    0.9302        60\n",
      "              cima-da-conegliano     0.2982    0.2833    0.2906        60\n",
      "                 claes-oldenburg     0.5789    0.3667    0.4490        60\n",
      "                  claude-lorrain     0.4390    0.6000    0.5070        60\n",
      "                    claude-monet     0.0000    0.0000    0.0000        60\n",
      "               claude-tousignant     1.0000    1.0000    1.0000        60\n",
      "                  claude-viallat     0.9412    0.8000    0.8649        60\n",
      "                   claudio-tozzi     0.8000    1.0000    0.8889        60\n",
      "                  clyfford-still     0.3810    0.4000    0.3902        60\n",
      "      columbano-bordalo-pinheiro     0.5270    0.6500    0.5821        60\n",
      "              conrad-marca-relli     0.2113    0.2500    0.2290        60\n",
      "                 constant-troyon     0.1513    0.3833    0.2170        60\n",
      "            constantin-artachino     0.1930    0.1833    0.1880        60\n",
      "              constantin-blendea     0.9375    1.0000    0.9677        60\n",
      "             constantin-brancusi     0.4000    0.8667    0.5474        60\n",
      "     constantin-daniel-rosenthal     0.4719    0.7000    0.5638        60\n",
      "              constantin-flondor     0.9375    1.0000    0.9677        60\n",
      "                 constantin-guys     0.1811    0.3833    0.2460        60\n",
      "              constantin-piliuta     0.3056    0.1833    0.2292        60\n",
      "                constantin-stahi     0.4744    0.6167    0.5362        60\n",
      "              constantine-maleas     0.1333    0.0333    0.0533        60\n",
      "                       corneille     1.0000    1.0000    1.0000        60\n",
      "                 cornelis-de-vos     0.3429    0.4000    0.3692        60\n",
      "   cornelis-norbertus-gysbrechts     0.7692    1.0000    0.8696        60\n",
      "               cornelis-springer     0.3571    0.4167    0.3846        60\n",
      "           cornelis-vreedenburgh     0.2632    0.1667    0.2041        60\n",
      "                   corneliu-baba     0.2000    0.0667    0.1000        60\n",
      "            corneliu-michailescu     0.2477    0.4500    0.3195        60\n",
      "                       correggio     0.2000    0.0500    0.0800        60\n",
      "                 costas-niarchos     0.1417    0.2833    0.1889        60\n",
      "                  craig-kauffman     0.9091    1.0000    0.9524        60\n",
      "         cristovao-de-figueiredo     0.5647    0.8000    0.6621        60\n",
      "                      cy-twombly     0.2453    0.2167    0.2301        60\n",
      "                       dadamaino     0.9677    1.0000    0.9836        60\n",
      "                    damien-hirst     0.9836    1.0000    0.9917        60\n",
      "                 dan-christensen     0.7647    0.2167    0.3377        60\n",
      "                      dan-flavin     0.8421    0.8000    0.8205        60\n",
      "                    daniel-buren     0.6111    0.9167    0.7333        60\n",
      "                  daniel-dezeuze     0.8980    0.7333    0.8073        60\n",
      "          dante-gabriel-rossetti     0.0000    0.0000    0.0000        60\n",
      "          david-alfaro-siqueiros     0.4940    0.6833    0.5734        60\n",
      "                 david-batchelor     0.7612    0.8500    0.8031        60\n",
      "                   david-bomberg     0.2895    0.3667    0.3235        60\n",
      "                   david-burliuk     0.0000    0.0000    0.0000        60\n",
      "                     david-smith     0.8696    1.0000    0.9302        60\n",
      "       david-teniers-the-younger     0.1163    0.0833    0.0971        60\n",
      "                    david-wilkie     0.1296    0.1167    0.1228        60\n",
      "                    denise-green     0.5735    0.6500    0.6094        60\n",
      "                   derek-boshier     0.1961    0.3333    0.2469        60\n",
      "                    diego-rivera     0.2308    0.2000    0.2143        60\n",
      "                 diego-velazquez     0.0000    0.0000    0.0000        60\n",
      "               dimitrie-paciurea     0.8824    1.0000    0.9375        60\n",
      "                dimitris-mytaras     0.6000    0.0500    0.0923        60\n",
      "                    ding-yanyong     0.6316    1.0000    0.7742        60\n",
      "                      dirk-bouts     0.1087    0.2500    0.1515        60\n",
      "                 dmitry-levitzky     0.2083    0.2500    0.2273        60\n",
      "            domenico-ghirlandaio     0.1702    0.1333    0.1495        60\n",
      "              domenico-veneziano     0.3654    0.6333    0.4634        60\n",
      "               domingos-sequeira     0.4245    0.7500    0.5422        60\n",
      "                   donald-sultan     0.9524    1.0000    0.9756        60\n",
      "                       donatello     0.6742    1.0000    0.8054        60\n",
      "                     dosso-dossi     0.0000    0.0000    0.0000        60\n",
      "                     doug-ohlson     0.6548    0.9167    0.7639        60\n",
      "                    doug-wheeler     1.0000    1.0000    1.0000        60\n",
      "                 douglas-huebler     0.9677    1.0000    0.9836        60\n",
      "                 dumitru-ghiatza     0.3482    0.6500    0.4535        60\n",
      "                     edgar-degas     0.0000    0.0000    0.0000        60\n",
      "                  edith-vonnegut     0.6905    0.9667    0.8056        60\n",
      "          edmund-charles-tarbell     0.3214    0.3000    0.3103        60\n",
      "                    edmund-dulac     0.1923    0.0833    0.1163        60\n",
      "                  edouard-cortes     0.3960    0.6667    0.4969        60\n",
      "                   edouard-manet     0.0000    0.0000    0.0000        60\n",
      "                edouard-vuillard     0.1250    0.0167    0.0294        60\n",
      "             eduard-von-gebhardt     0.2727    0.5000    0.3529        60\n",
      "                eduardo-paolozzi     0.7564    0.9833    0.8551        60\n",
      "                   eduardo-viana     0.6829    0.9333    0.7887        60\n",
      "                    edvard-munch     0.2143    0.0500    0.0811        60\n",
      "                edward-avedisian     0.8219    1.0000    0.9023        60\n",
      "              edward-burne-jones     0.6667    0.0333    0.0635        60\n",
      "                  edward-corbett     0.5714    0.4667    0.5138        60\n",
      "                   edward-hopper     0.2632    0.0833    0.1266        60\n",
      "                   edward-ruscha     0.4839    0.7500    0.5882        60\n",
      "            edwin-henry-landseer     0.2778    0.0833    0.1282        60\n",
      "                     efim-volkov     0.1567    0.5667    0.2455        60\n",
      "                    egon-schiele     0.1509    0.1333    0.1416        60\n",
      "                     eileen-agar     0.9091    1.0000    0.9524        60\n",
      "                        el-greco     0.0000    0.0000    0.0000        60\n",
      "                    el-lissitzky     0.8108    1.0000    0.8955        60\n",
      "               elaine-de-kooning     0.2692    0.2333    0.2500        60\n",
      "                 eliseu-visconti     0.0000    0.0000    0.0000        60\n",
      "                 ellsworth-kelly     0.3333    0.1500    0.2069        60\n",
      "                      emil-nolde     0.2727    0.1000    0.1463        60\n",
      "                     emile-claus     0.2117    0.4833    0.2944        60\n",
      "                      emily-carr     0.4512    0.6167    0.5211        60\n",
      "                 emmanuel-zairis     0.3379    0.8167    0.4780        60\n",
      "                    endre-balint     0.1333    0.3333    0.1905        60\n",
      "                    endre-bartos     0.3889    0.3500    0.3684        60\n",
      "               enrico-castellani     0.9524    1.0000    0.9756        60\n",
      "                     eric-fischl     0.2075    0.1833    0.1947        60\n",
      "           ernst-ludwig-kirchner     0.0000    0.0000    0.0000        60\n",
      "                            erro     0.4194    0.8667    0.5652        60\n",
      "             esaias-van-de-velde     0.1091    0.2000    0.1412        60\n",
      "                 esteban-vicente     0.3492    0.3667    0.3577        60\n",
      "                   eugene-boudin     0.1892    0.1167    0.1443        60\n",
      "                eugene-delacroix     0.0000    0.0000    0.0000        60\n",
      "               eustache-le-sueur     0.4886    0.7167    0.5811        60\n",
      "                    eva-gonzales     0.0588    0.0500    0.0541        60\n",
      "                       eva-hesse     0.3692    0.4000    0.3840        60\n",
      "                   evelyne-axell     0.3889    0.4667    0.4242        60\n",
      "                fairfield-porter     0.3544    0.4667    0.4029        60\n",
      "           federico-zandomeneghi     0.0714    0.0833    0.0769        60\n",
      "                   felicien-rops     0.0000    0.0000    0.0000        60\n",
      "           felix-gonzalez-torres     0.9655    0.9333    0.9492        60\n",
      "                 felix-vallotton     0.4583    0.1833    0.2619        60\n",
      "                ferdinand-hodler     0.0000    0.0000    0.0000        60\n",
      "              ferdynand-ruszczyc     0.2766    0.2167    0.2430        60\n",
      "                   fern-coppedge     0.6522    1.0000    0.7895        60\n",
      "                 fernand-khnopff     0.1471    0.0833    0.1064        60\n",
      "                   fernand-leger     0.3750    0.0500    0.0882        60\n",
      "                 fernando-botero     0.3784    0.2333    0.2887        60\n",
      "                 fernando-calhau     0.8333    1.0000    0.9091        60\n",
      "                 fernando-lanhas     1.0000    1.0000    1.0000        60\n",
      "             fikret-mualla-saygi     0.7059    0.4000    0.5106        60\n",
      "                 filipp-malyavin     0.2128    0.3333    0.2597        60\n",
      "            filippo-brunelleschi     0.8696    1.0000    0.9302        60\n",
      "                   filippo-lippi     0.6667    0.0333    0.0635        60\n",
      "                ford-madox-brown     0.1690    0.2000    0.1832        60\n",
      "                    forrest-bess     0.4545    0.4167    0.4348        60\n",
      "                    fra-angelico     0.1111    0.0167    0.0290        60\n",
      "                francesco-guardi     0.1649    0.2667    0.2038        60\n",
      "                 francesco-hayez     0.2308    0.0500    0.0822        60\n",
      "              francesco-solimena     0.2014    0.4833    0.2843        60\n",
      "                 francis-picabia     0.0857    0.0500    0.0632        60\n",
      "                 francisc-sirato     0.1296    0.1167    0.1228        60\n",
      "        francisco-bayeu-y-subias     0.6067    0.9000    0.7248        60\n",
      "           francisco-de-zurbaran     0.1273    0.1167    0.1217        60\n",
      "                  francisco-goya     0.1429    0.0500    0.0741        60\n",
      "                francois-boucher     0.2051    0.2667    0.2319        60\n",
      "                  frank-auerbach     0.2826    0.2167    0.2453        60\n",
      "                   frank-bowling     0.7229    1.0000    0.8392        60\n",
      "                  frank-johnston     0.6628    0.9500    0.7808        60\n",
      "                   frank-lobdell     0.5000    0.7333    0.5946        60\n",
      "                    frank-stella     0.0413    0.0833    0.0552        60\n",
      "             franklin-carmichael     0.3333    0.3833    0.3566        60\n",
      "                      frans-hals     0.0000    0.0000    0.0000        60\n",
      "                   frans-snyders     0.1753    0.2833    0.2166        60\n",
      "                 frantisek-kupka     0.0000    0.0000    0.0000        60\n",
      "                     franz-kline     0.3824    0.4333    0.4062        60\n",
      "                      franz-marc     0.0833    0.0167    0.0278        60\n",
      "       franz-richard-unterberger     0.6000    0.3500    0.4421        60\n",
      "                     franz-stuck     0.0000    0.0000    0.0000        60\n",
      "        franz-xaver-winterhalter     0.4182    0.7667    0.5412        60\n",
      "                   fred-sandback     0.7342    0.9667    0.8345        60\n",
      "                   fred-williams     0.6522    1.0000    0.7895        60\n",
      "                      fred-yates     0.6744    0.9667    0.7945        60\n",
      "                frederic-bazille     0.1000    0.0333    0.0500        60\n",
      "           frederic-edwin-church     0.7059    1.0000    0.8276        60\n",
      "              frederic-remington     0.2500    0.4167    0.3125        60\n",
      "            frederick-hammersley     0.9524    1.0000    0.9756        60\n",
      "                     frida-kahlo     1.0000    1.0000    1.0000        60\n",
      "                  friedel-dzubas     0.2917    0.1167    0.1667        60\n",
      "                   frits-thaulow     0.1333    0.1000    0.1143        60\n",
      "                fyodor-bronnikov     0.1081    0.1333    0.1194        60\n",
      "                  fyodor-rokotov     0.2595    0.5667    0.3560        60\n",
      "                 fyodor-solntsev     0.7733    0.9667    0.8593        60\n",
      "                 fyodor-vasilyev     0.2609    0.1000    0.1446        60\n",
      "                   gabriel-metsu     0.1744    0.2500    0.2055        60\n",
      "                      gene-davis     0.3667    0.1833    0.2444        60\n",
      "                  genevieve-asse     0.6308    0.6833    0.6560        60\n",
      "                george-bouzianis     0.1339    0.2500    0.1744        60\n",
      "                   george-catlin     0.1857    0.4333    0.2600        60\n",
      "          george-frederick-watts     0.0769    0.0667    0.0714        60\n",
      "                   george-inness     1.0000    1.0000    1.0000        60\n",
      "                     george-luks     0.0000    0.0000    0.0000        60\n",
      "                george-mavroides     0.3496    0.7167    0.4699        60\n",
      "                  george-morland     0.0385    0.0500    0.0435        60\n",
      "                    george-pemba     0.5085    1.0000    0.6742        60\n",
      "                    george-segal     0.5319    0.4167    0.4673        60\n",
      "                   george-stubbs     0.4286    0.2500    0.3158        60\n",
      "                  georges-braque     0.0000    0.0000    0.0000        60\n",
      "                  georges-lemmen     0.2381    0.3333    0.2778        60\n",
      "     georges-ribemont-dessaignes     0.8108    1.0000    0.8955        60\n",
      "                 georges-rouault     0.5053    0.8000    0.6194        60\n",
      "                  georges-seurat     0.2143    0.1500    0.1765        60\n",
      "            georges-vantongerloo     0.7595    1.0000    0.8633        60\n",
      "              georgios-jakobides     0.1429    0.0833    0.1053        60\n",
      "                    gerard-david     0.0000    0.0000    0.0000        60\n",
      "                gerard-fromanger     0.2500    0.0667    0.1053        60\n",
      "                   gerard-sekoto     0.3857    0.4500    0.4154        60\n",
      "                 gerard-terborch     0.5294    0.3000    0.3830        60\n",
      "                 gerardo-dottori     1.0000    1.0000    1.0000        60\n",
      "                 gerhard-richter     0.4937    0.6500    0.5612        60\n",
      "                      gerrit-dou     0.2353    0.2667    0.2500        60\n",
      "                   geta-bratescu     0.8772    0.8333    0.8547        60\n",
      "             gheorghe-tattarescu     0.2000    0.3667    0.2588        60\n",
      "                   giacomo-balla     0.8824    1.0000    0.9375        60\n",
      "            gian-lorenzo-bernini     0.4082    0.3333    0.3670        60\n",
      "           gianfranco-baruchello     0.9375    1.0000    0.9677        60\n",
      "              gil-teixeira-lopes     1.0000    1.0000    1.0000        60\n",
      "                  gilles-aillaud     0.5816    0.9500    0.7215        60\n",
      "                   gino-severini     0.3600    0.6000    0.4500        60\n",
      "                 giorgio-morandi     0.7595    1.0000    0.8633        60\n",
      "                  giorgio-vasari     0.1200    0.0500    0.0706        60\n",
      "                       giorgione     0.0000    0.0000    0.0000        60\n",
      "                giovanni-anselmo     0.9836    1.0000    0.9917        60\n",
      "     giovanni-antonio-boltraffio     0.4649    0.8833    0.6092        60\n",
      "       giovanni-battista-tiepolo     0.0000    0.0000    0.0000        60\n",
      "                giovanni-bellini     0.1765    0.0500    0.0779        60\n",
      "                giovanni-boldini     0.5000    0.0167    0.0323        60\n",
      "       giovanni-domenico-tiepolo     0.0556    0.0167    0.0256        60\n",
      "                giovanni-fattori     0.1588    0.4500    0.2348        60\n",
      "           giovanni-paolo-panini     0.5273    0.9667    0.6824        60\n",
      "             giuseppe-arcimboldo     0.3909    0.7167    0.5059        60\n",
      "              giuseppe-de-nittis     0.4167    0.0833    0.1389        60\n",
      "            gosta-adrian-nilsson     0.2500    0.2333    0.2414        60\n",
      "               gotthard-graubner     0.7966    0.7833    0.7899        60\n",
      "          grace-cossington-smith     0.2083    0.1667    0.1852        60\n",
      "                   grandma-moses     0.3636    0.4667    0.4088        60\n",
      "              gregoire-boonzaier     0.1250    0.0167    0.0294        60\n",
      "                  gregorio-lopes     0.5000    0.8833    0.6386        60\n",
      "             grigoriy-myasoyedov     0.1875    0.1000    0.1304        60\n",
      "                  guido-molinari     0.8000    1.0000    0.8889        60\n",
      "                      guido-reni     0.1111    0.1167    0.1138        60\n",
      "                   gulacsy-lajos     0.2703    0.5000    0.3509        60\n",
      "                guntis-strupulis     1.0000    1.0000    1.0000        60\n",
      "                    gustav-klimt     0.5000    0.0333    0.0625        60\n",
      "             gustave-caillebotte     0.0000    0.0000    0.0000        60\n",
      "                 gustave-courbet     0.0000    0.0000    0.0000        60\n",
      "                    gustave-dore     0.1944    0.2333    0.2121        60\n",
      "                 gustave-loiseau     0.0547    0.1167    0.0745        60\n",
      "                  gustave-moreau     0.0172    0.0167    0.0169        60\n",
      "                        guy-rose     0.0370    0.0167    0.0230        60\n",
      "                       gwen-john     0.1895    0.4833    0.2723        60\n",
      "                    hans-baldung     0.0000    0.0000    0.0000        60\n",
      "                    hans-bellmer     0.9524    1.0000    0.9756        60\n",
      "                    hans-hofmann     0.5385    0.1167    0.1918        60\n",
      "        hans-holbein-the-younger     0.0000    0.0000    0.0000        60\n",
      "                    hans-memling     0.0833    0.0167    0.0278        60\n",
      "                    hans-richter     0.8333    1.0000    0.9091        60\n",
      "                 hans-von-aachen     0.1343    0.1500    0.1417        60\n",
      "                    harry-clarke     0.2529    0.3667    0.2993        60\n",
      "                    hassel-smith     0.6977    1.0000    0.8219        60\n",
      "                    hedda-sterne     0.4333    0.6500    0.5200        60\n",
      "             helen-frankenthaler     0.3636    0.0667    0.1127        60\n",
      "              helene-schjerfbeck     0.0735    0.1667    0.1020        60\n",
      "                  helio-oiticica     0.9836    1.0000    0.9917        60\n",
      "        hendrick-cornelisz-vroom     0.5050    0.8500    0.6335        60\n",
      "            hendrick-terbrugghen     0.6047    0.8667    0.7123        60\n",
      "                    henk-peeters     0.8824    1.0000    0.9375        60\n",
      "                   henri-catargi     0.2500    0.1333    0.1739        60\n",
      "       henri-de-toulouse-lautrec     0.0000    0.0000    0.0000        60\n",
      "              henri-edmond-cross     0.0789    0.1000    0.0882        60\n",
      "             henri-fantin-latour     0.5102    0.4167    0.4587        60\n",
      "                   henri-laurens     0.5098    0.4333    0.4685        60\n",
      "             henri-le-fauconnier     0.1818    0.1667    0.1739        60\n",
      "                    henri-martin     0.3333    0.0167    0.0317        60\n",
      "                   henri-matisse     0.0000    0.0000    0.0000        60\n",
      "                   henri-michaux     0.8824    1.0000    0.9375        60\n",
      "                  henri-rousseau     0.1268    0.1500    0.1374        60\n",
      "                 henrique-pousao     0.3306    0.6833    0.4457        60\n",
      "        henry-herbert-la-thangue     0.3333    0.0333    0.0606        60\n",
      "                     henry-moore     1.0000    1.0000    1.0000        60\n",
      "                   henry-raeburn     0.0500    0.0333    0.0400        60\n",
      "              henryk-siemiradzki     0.1000    0.0333    0.0500        60\n",
      "                  heorhiy-narbut     0.2500    0.0500    0.0833        60\n",
      "                hercules-seghers     0.2917    0.3500    0.3182        60\n",
      "                hieronymus-bosch     0.1111    0.0167    0.0290        60\n",
      "                  hilma-af-klint     1.0000    1.0000    1.0000        60\n",
      "                   hiro-yamagata     0.6000    0.3000    0.4000        60\n",
      "                       hiroshige     0.1000    0.0667    0.0800        60\n",
      "                   hoca-ali-riza     0.2679    0.5000    0.3488        60\n",
      "                    homer-watson     0.2090    0.4667    0.2887        60\n",
      "                   horace-pippin     0.1918    0.2333    0.2105        60\n",
      "                    horia-bernea     0.1429    0.0167    0.0299        60\n",
      "                    horia-damian     0.9524    1.0000    0.9756        60\n",
      "                  howard-finster     0.9524    1.0000    0.9756        60\n",
      "                  howard-hodgkin     0.5784    0.9833    0.7284        60\n",
      "                  howard-mehring     0.9344    0.9500    0.9421        60\n",
      "                   hubert-robert     0.2090    0.2333    0.2205        60\n",
      "                    hugo-simberg     0.4528    0.4000    0.4248        60\n",
      "               hugo-van-der-goes     0.0714    0.0500    0.0588        60\n",
      "                   ian-davenport     0.9231    1.0000    0.9600        60\n",
      "                       ilka-gedo     0.2414    0.3500    0.2857        60\n",
      "                    ilya-mashkov     0.1111    0.0667    0.0833        60\n",
      "                      ilya-repin     0.2500    0.0167    0.0312        60\n",
      "                     imi-knoebel     0.8864    0.6500    0.7500        60\n",
      "              ioannis-altamouras     0.0746    0.0833    0.0787        60\n",
      "                   ion-andreescu     0.2000    0.2000    0.2000        60\n",
      "                     ion-nicodim     0.8108    1.0000    0.8955        60\n",
      "                       ion-pacea     0.3250    0.2167    0.2600        60\n",
      "            ion-theodorescu-sion     0.0000    0.0000    0.0000        60\n",
      "                   ion-tuculescu     0.3548    0.5500    0.4314        60\n",
      "                      iosif-iser     0.1429    0.0333    0.0541        60\n",
      "                  ipolit-strambu     0.2000    0.4833    0.2829        60\n",
      "                      irma-stern     0.1765    0.1000    0.1277        60\n",
      "                     isa-genzken     1.0000    1.0000    1.0000        60\n",
      "                   isaac-levitan     0.0000    0.0000    0.0000        60\n",
      "                isaac-van-ostade     0.3765    0.5333    0.4414        60\n",
      "                   istvan-farkas     0.2525    0.4167    0.3145        60\n",
      "            istvan-ilosvai-varga     0.3125    0.1667    0.2174        60\n",
      "                     istvan-nagy     0.1515    0.0833    0.1075        60\n",
      "                     ito-jakuchu     0.3333    0.5000    0.4000        60\n",
      "                     ito-shinsui     0.9677    1.0000    0.9836        60\n",
      "                 ivan-aivazovsky     0.4375    0.1167    0.1842        60\n",
      "                   ivan-albright     0.3229    0.5167    0.3974        60\n",
      "                    ivan-bilibin     0.2941    0.0833    0.1299        60\n",
      "                  ivan-generalic     0.7250    0.9667    0.8286        60\n",
      "                     ivan-grohar     0.2696    0.5167    0.3543        60\n",
      "                   ivan-kramskoy     0.0000    0.0000    0.0000        60\n",
      "                      ivan-milev     0.4146    0.8500    0.5574        60\n",
      "                    ivan-nikitin     0.5000    0.8333    0.6250        60\n",
      "                  ivan-rutkovych     0.7108    0.9833    0.8252        60\n",
      "                   ivan-shishkin     0.1923    0.0833    0.1163        60\n",
      "                 ivan-vladimirov     0.2353    0.0667    0.1039        60\n",
      "              j.-e.-h.-macdonald     0.1721    0.3500    0.2308        60\n",
      "                jacek-malczewski     0.1333    0.0667    0.0889        60\n",
      "                       jack-bush     0.0000    0.0000    0.0000        60\n",
      "                    jack-tworkov     0.3091    0.2833    0.2957        60\n",
      "                 jack-youngerman     0.8451    1.0000    0.9160        60\n",
      "                 jackson-pollock     0.5532    0.4333    0.4860        60\n",
      "   jacob-isaakszoon-van-ruisdael     0.7460    0.7833    0.7642        60\n",
      "                  jacob-jordaens     0.2069    0.1000    0.1348        60\n",
      "           jacoba-van-heemskerck     0.7746    0.9167    0.8397        60\n",
      "                  jacopo-bellini     0.1304    0.0500    0.0723        60\n",
      "                 jacopo-pontormo     0.1429    0.0167    0.0299        60\n",
      "                  jacques-stella     0.6977    1.0000    0.8219        60\n",
      "                  jacques-villon     0.4444    0.2000    0.2759        60\n",
      "                     james-ensor     0.2500    0.0833    0.1250        60\n",
      "          james-mcneill-whistler     0.0000    0.0000    0.0000        60\n",
      "                    james-tissot     0.1667    0.0167    0.0303        60\n",
      "                   james-turrell     1.0000    1.0000    1.0000        60\n",
      "                     jamie-wyeth     0.2222    0.0333    0.0580        60\n",
      "                     jan-matejko     0.0000    0.0000    0.0000        60\n",
      "                    jan-provoost     0.4815    0.8667    0.6190        60\n",
      "                  jan-siberechts     0.3500    0.5833    0.4375        60\n",
      "                    jan-sluyters     0.1538    0.0333    0.0548        60\n",
      "                       jan-steen     0.0526    0.0167    0.0253        60\n",
      "                      jan-toorop     0.1667    0.2667    0.2051        60\n",
      "                    jan-van-eyck     0.4444    0.0667    0.1159        60\n",
      "                jan-van-hemessen     0.1132    0.1000    0.1062        60\n",
      "                      jane-frank     0.6061    1.0000    0.7547        60\n",
      "                      janet-fish     0.6267    0.7833    0.6963        60\n",
      "            janos-mattis-teutsch     0.4825    0.9167    0.6322        60\n",
      "                   janos-tornyai     0.3119    0.5667    0.4024        60\n",
      "                       jay-defeo     0.4691    0.6333    0.5390        60\n",
      "         jean-alexandru-steriadi     0.1760    0.3667    0.2378        60\n",
      "             jean-baptiste-oudry     0.2014    0.4667    0.2814        60\n",
      "    jean-baptiste-simeon-chardin     0.2444    0.1833    0.2095        60\n",
      "                      jean-david     0.0800    0.0667    0.0727        60\n",
      "                   jean-degottex     0.4872    0.6333    0.5507        60\n",
      "                   jean-dubuffet     0.7692    1.0000    0.8696        60\n",
      "                   jean-fautrier     0.4800    1.0000    0.6486        60\n",
      "                    jean-fouquet     0.1282    0.0833    0.1010        60\n",
      "            jean-francois-millet     0.0000    0.0000    0.0000        60\n",
      "                     jean-helion     0.6316    0.6000    0.6154        60\n",
      "                        jean-hey     0.3196    0.5167    0.3949        60\n",
      "           jean-honore-fragonard     0.0870    0.0667    0.0755        60\n",
      "                       jean-hugo     0.4545    0.0833    0.1408        60\n",
      "                jean-leon-gerome     0.0000    0.0000    0.0000        60\n",
      "               jean-marc-nattier     0.7222    0.8667    0.7879        60\n",
      "                  jean-metzinger     0.2632    0.0833    0.1266        60\n",
      "               jean-paul-lemieux     0.4815    0.6500    0.5532        60\n",
      "              jean-paul-riopelle     0.3115    0.3167    0.3140        60\n",
      "               jean-rene-bazaine     1.0000    1.0000    1.0000        60\n",
      "            jehan-georges-vibert     0.4717    0.8333    0.6024        60\n",
      "                        jim-dine     0.6197    0.7333    0.6718        60\n",
      "                      jim-lambie     0.9167    0.9167    0.9167        60\n",
      "                     jimmy-ernst     0.2619    0.1833    0.2157        60\n",
      "                  jiro-yoshihara     0.6897    1.0000    0.8163        60\n",
      "                         jo-baer     0.8077    0.7000    0.7500        60\n",
      "                 joachim-patinir     0.2857    0.7333    0.4112        60\n",
      "                 joachim-wtewael     0.2606    0.6167    0.3663        60\n",
      "           joan-hernandez-pijuan     0.2169    0.3000    0.2517        60\n",
      "                       joan-miro     0.1316    0.0833    0.1020        60\n",
      "                   joan-mitchell     0.3176    0.4500    0.3724        60\n",
      "                       joan-ponc     0.9836    1.0000    0.9917        60\n",
      "                     joan-snyder     0.5577    0.4833    0.5179        60\n",
      "                     joao-vieira     1.0000    1.0000    1.0000        60\n",
      "                  jock-macdonald     0.9836    1.0000    0.9917        60\n",
      "                       joe-goode     0.4699    0.6500    0.5455        60\n",
      "            johan-christian-dahl     0.6892    0.8500    0.7612        60\n",
      "      johan-hendrik-weissenbruch     0.3529    0.2000    0.2553        60\n",
      "                    johann-koler     0.2716    0.3667    0.3121        60\n",
      "                  johannes-itten     0.9000    0.7500    0.8182        60\n",
      "      johannes-sveinsson-kjarval     0.6020    0.9833    0.7468        60\n",
      "                johannes-vermeer     0.1714    0.2000    0.1846        60\n",
      "          john-atkinson-grimshaw     0.2198    0.3333    0.2649        60\n",
      "                john-chamberlain     0.8571    1.0000    0.9231        60\n",
      "                    john-collier     0.3636    0.0667    0.1127        60\n",
      "                  john-constable     0.0926    0.0833    0.0877        60\n",
      "                      john-crome     0.1233    0.3000    0.1748        60\n",
      "           john-duncan-fergusson     0.6071    0.8500    0.7083        60\n",
      "            john-everett-millais     0.0000    0.0000    0.0000        60\n",
      "                     john-ferren     0.1765    0.5000    0.2609        60\n",
      "               john-french-sloan     0.0000    0.0000    0.0000        60\n",
      "            john-henry-twachtman     0.0306    0.0500    0.0380        60\n",
      "                    john-hoppner     0.3421    0.2167    0.2653        60\n",
      "                    john-hoyland     0.3214    0.1500    0.2045        60\n",
      "                     john-lavery     0.2308    0.2000    0.2143        60\n",
      "              john-lewis-krimmel     0.3411    0.7333    0.4656        60\n",
      "                      john-marin     0.3358    0.7500    0.4639        60\n",
      "                  john-mccracken     0.8378    0.5167    0.6392        60\n",
      "                 john-mclaughlin     0.4810    0.6333    0.5468        60\n",
      "                     john-miller     0.5909    0.4333    0.5000        60\n",
      "    john-roddam-spencer-stanhope     0.1442    0.2500    0.1829        60\n",
      "                    john-russell     0.0714    0.0333    0.0455        60\n",
      "             john-singer-sargent     0.0000    0.0000    0.0000        60\n",
      "           john-singleton-copley     0.9836    1.0000    0.9917        60\n",
      "                   john-trumbull     0.6349    0.6667    0.6504        60\n",
      "         john-william-waterhouse     0.1212    0.0667    0.0860        60\n",
      "                   jorge-martins     0.7600    0.9500    0.8444        60\n",
      "            jose-clemente-orozco     0.9836    1.0000    0.9917        60\n",
      "        jose-de-almada-negreiros     0.3597    0.8333    0.5025        60\n",
      "               jose-de-guimaraes     0.7237    0.9167    0.8088        60\n",
      "                   jose-guerrero     0.5000    0.5333    0.5161        60\n",
      "           jose-gutierrez-solana     0.1364    0.1500    0.1429        60\n",
      "                     jose-malhoa     0.8955    1.0000    0.9449        60\n",
      "                    josef-albers     0.8451    1.0000    0.9160        60\n",
      "                    josef-herman     0.6883    0.8833    0.7737        60\n",
      "                josefa-de-obidos     0.3333    0.7000    0.4516        60\n",
      "                   joseph-wright     0.2169    0.3000    0.2517        60\n",
      "                 joshua-reynolds     0.2143    0.0500    0.0811        60\n",
      "              jozsef-rippl-ronai     0.0000    0.0000    0.0000        60\n",
      "             juan-de-valdes-leal     0.3472    0.8333    0.4902        60\n",
      "                       juan-gris     0.2048    0.2833    0.2378        60\n",
      "                  judith-leyster     0.3218    0.4667    0.3810        60\n",
      "                    judy-chicago     0.9655    0.9333    0.9492        60\n",
      "                    jules-cheret     0.8696    1.0000    0.9302        60\n",
      "           jules-joseph-lefebvre     0.9375    1.0000    0.9677        60\n",
      "                   jules-lefranc     0.4407    0.4333    0.4370        60\n",
      "                    jules-pascin     0.5234    0.9333    0.6707        60\n",
      "                   jules-perahim     1.0000    1.0000    1.0000        60\n",
      "               julian-alden-weir     0.1724    0.0833    0.1124        60\n",
      "                  julio-gonzalez     0.7049    0.7167    0.7107        60\n",
      "                     julio-pomar     0.4022    0.6167    0.4868        60\n",
      "          julius-leblanc-stewart     0.0000    0.0000    0.0000        60\n",
      "                   jury-annenkov     0.1923    0.0833    0.1163        60\n",
      "                jusepe-de-ribera     0.4545    0.8333    0.5882        60\n",
      "                   karl-benjamin     1.0000    1.0000    1.0000        60\n",
      "                     karl-bodmer     0.4400    0.1833    0.2588        60\n",
      "                   karl-bryullov     0.0000    0.0000    0.0000        60\n",
      "                     karl-schrag     0.1026    0.1333    0.1159        60\n",
      "                 karoly-ferenczy     0.3889    0.1167    0.1795        60\n",
      "                kateryna-bilokur     0.4430    0.5833    0.5036        60\n",
      "                  kathe-kollwitz     0.3037    0.6833    0.4205        60\n",
      "              katsushika-hokusai     0.0588    0.0167    0.0260        60\n",
      "                     kay-nielsen     0.0698    0.0500    0.0583        60\n",
      "                kazimir-malevich     0.0000    0.0000    0.0000        60\n",
      "                  kazuo-nakamura     0.4545    0.5000    0.4762        60\n",
      "                 kees-van-dongen     0.5455    0.1000    0.1690        60\n",
      "                    keisai-eisen     0.1786    0.1667    0.1724        60\n",
      "                    keith-haring     0.6533    0.8167    0.7259        60\n",
      "                   keith-sonnier     1.0000    0.8000    0.8889        60\n",
      "                     kenzo-okada     0.6892    0.8500    0.7612        60\n",
      "                     kimon-loghi     0.1395    0.1000    0.1165        60\n",
      "                kitagawa-utamaro     0.4848    0.2667    0.3441        60\n",
      "                  klavdy-lebedev     0.4351    0.9500    0.5969        60\n",
      "                   koloman-moser     0.0000    0.0000    0.0000        60\n",
      "                     konrad-witz     0.3592    0.6167    0.4540        60\n",
      "            konstantin-bogaevsky     0.0435    0.0333    0.0377        60\n",
      "              konstantin-korovin     0.0000    0.0000    0.0000        60\n",
      "             konstantin-makovsky     0.1250    0.0167    0.0294        60\n",
      "                konstantin-somov     0.0000    0.0000    0.0000        60\n",
      "             konstantin-vasilyev     0.2000    0.0167    0.0308        60\n",
      "          konstantinos-parthenis     0.0769    0.0167    0.0274        60\n",
      "          konstantinos-volanakis     0.1650    0.2833    0.2086        60\n",
      "                 kurt-schwitters     1.0000    1.0000    1.0000        60\n",
      "             kuzma-petrov-vodkin     0.0000    0.0000    0.0000        60\n",
      "                   lajos-tihanyi     0.3488    0.2500    0.2913        60\n",
      "                      larry-bell     0.8333    0.5833    0.6863        60\n",
      "                       larry-zox     0.7595    1.0000    0.8633        60\n",
      "                    lasar-segall     0.7143    1.0000    0.8333        60\n",
      "              laszlo-mednyanszky     0.1905    0.1333    0.1569        60\n",
      "              laszlo-moholy-nagy     0.9836    1.0000    0.9917        60\n",
      "                 lavinia-fontana     0.1684    0.5333    0.2560        60\n",
      "                   lawren-harris     0.6795    0.8833    0.7681        60\n",
      "                    le-corbusier     0.5463    0.9833    0.7024        60\n",
      "                le-nain-brothers     0.3276    0.3167    0.3220        60\n",
      "                          le-pho     0.4200    0.3500    0.3818        60\n",
      "                     lee-krasner     0.5946    0.3667    0.4536        60\n",
      "                        lee-ufan     0.6765    0.7667    0.7188        60\n",
      "                   lennart-rodhe     0.9836    1.0000    0.9917        60\n",
      "                   leo-villareal     0.8333    1.0000    0.9091        60\n",
      "                      leon-bakst     0.2424    0.1333    0.1720        60\n",
      "                  leon-berkowitz     0.9231    0.6000    0.7273        60\n",
      "                     leon-bonnat     0.7424    0.8167    0.7778        60\n",
      "                       leon-dabo     0.7895    1.0000    0.8824        60\n",
      "                 leon-spilliaert     0.2982    0.5667    0.3908        60\n",
      "               leonardo-da-vinci     0.0000    0.0000    0.0000        60\n",
      "                 leopold-survage     0.4651    0.6667    0.5479        60\n",
      "                    leroy-neiman     0.4444    0.9333    0.6022        60\n",
      "                     lev-lagorio     0.1750    0.2333    0.2000        60\n",
      "                    li-yuan-chia     1.0000    1.0000    1.0000        60\n",
      "                   ligia-macovei     0.1500    0.1000    0.1200        60\n",
      "                   lorenzo-lotto     0.0000    0.0000    0.0000        60\n",
      "                lorser-feitelson     0.9836    1.0000    0.9917        60\n",
      "                   louay-kayyali     0.2671    0.6500    0.3786        60\n",
      "                      louis-cane     0.9111    0.6833    0.7810        60\n",
      "                    louis-janmot     0.2957    0.5667    0.3886        60\n",
      "                louis-marcoussis     0.2121    0.2333    0.2222        60\n",
      "                  louis-schanker     0.1212    0.0667    0.0860        60\n",
      "                    louis-valtat     0.2164    0.6167    0.3203        60\n",
      "                     louis-vivin     0.7397    0.9000    0.8120        60\n",
      "  louise-elisabeth-vigee-le-brun     0.1522    0.1167    0.1321        60\n",
      "                 louise-nevelson     0.6324    0.7167    0.6719        60\n",
      "                  lourdes-castro     0.3333    0.4000    0.3636        60\n",
      "                   lovis-corinth     0.0882    0.1000    0.0938        60\n",
      "                 luca-signorelli     0.2727    0.0500    0.0845        60\n",
      "         lucas-cranach-the-elder     0.0000    0.0000    0.0000        60\n",
      "      lucia-demetriade-balacescu     0.2105    0.0667    0.1013        60\n",
      "                    lucian-freud     0.1132    0.1000    0.1062        60\n",
      "               lucian-grigorescu     0.2159    0.3167    0.2568        60\n",
      "               luciano-bartolini     0.9524    1.0000    0.9756        60\n",
      "                   luigi-russolo     0.9677    1.0000    0.9836        60\n",
      "                      luis-feito     0.9091    1.0000    0.9524        60\n",
      "                      lygia-pape     1.0000    1.0000    1.0000        60\n",
      "                       lynd-ward     0.4528    0.4000    0.4248        60\n",
      "                lyonel-feininger     0.0000    0.0000    0.0000        60\n",
      "                   lyubov-popova     0.7895    1.0000    0.8824        60\n",
      "                      m.-h.-maxy     0.2609    0.1000    0.1446        60\n",
      "                     m.c.-escher     0.5417    0.2167    0.3095        60\n",
      "                          mabuse     0.0000    0.0000    0.0000        60\n",
      "          maerten-van-heemskerck     0.0000    0.0000    0.0000        60\n",
      "                         man-ray     0.4872    0.6333    0.5507        60\n",
      "                     manabu-mabe     0.4947    0.7833    0.6065        60\n",
      "                     manuel-neri     1.0000    1.0000    1.0000        60\n",
      "                    marc-chagall     0.0000    0.0000    0.0000        60\n",
      "                  marcel-barbeau     1.0000    1.0000    1.0000        60\n",
      "              marcel-broodthaers     1.0000    1.0000    1.0000        60\n",
      "                  marcel-duchamp     0.0000    0.0000    0.0000        60\n",
      "                    marcel-janco     0.2500    0.1833    0.2115        60\n",
      "                   marcelle-cahn     0.9524    1.0000    0.9756        60\n",
      "                   marcus-larson     0.6867    0.9500    0.7972        60\n",
      "       marevna-(marie-vorobieff)     0.2462    0.2667    0.2560        60\n",
      "               margareta-sterian     0.6061    0.6667    0.6349        60\n",
      "    maria-helena-vieira-da-silva     0.5505    1.0000    0.7101        60\n",
      "               maria-primachenko     0.7429    0.4333    0.5474        60\n",
      "                  marianne-north     0.3857    0.4500    0.4154        60\n",
      "               marie-bracquemond     0.4180    0.8500    0.5604        60\n",
      "                 marie-laurencin     0.4127    0.8667    0.5591        60\n",
      "                  marin-gherasim     0.9836    1.0000    0.9917        60\n",
      "                  mario-cesariny     0.7042    0.8333    0.7634        60\n",
      "                      mario-eloy     0.7792    1.0000    0.8759        60\n",
      "                     mario-nuzzi     0.6957    0.8000    0.7442        60\n",
      "                  mario-schifano     0.1935    0.1000    0.1319        60\n",
      "                    mario-sironi     0.8696    1.0000    0.9302        60\n",
      "                    mario-zanini     0.4512    0.6167    0.5211        60\n",
      "                marjorie-strider     0.4023    0.5833    0.4762        60\n",
      "                     mark-rothko     0.3387    0.3500    0.3443        60\n",
      "                      mark-tobey     0.3261    0.2500    0.2830        60\n",
      "                  marko-pogacnik     0.8333    1.0000    0.9091        60\n",
      "                 marsden-hartley     0.4404    0.8000    0.5680        60\n",
      "                  martial-raysse     0.4306    0.5167    0.4697        60\n",
      "                    martin-barre     0.6575    0.8000    0.7218        60\n",
      "            martin-johnson-heade     0.8955    1.0000    0.9449        60\n",
      "               martin-schongauer     0.2778    0.0833    0.1282        60\n",
      "                 martiros-saryan     0.0000    0.0000    0.0000        60\n",
      "                    mary-cassatt     0.0000    0.0000    0.0000        60\n",
      "                     mary-fedden     0.3750    0.2000    0.2609        60\n",
      "                        masaccio     0.2162    0.2667    0.2388        60\n",
      "                   matej-sternen     0.3507    0.7833    0.4845        60\n",
      "                   matthias-stom     0.9231    1.0000    0.9600        60\n",
      "             maurice-de-vlaminck     0.1200    0.0500    0.0706        60\n",
      "                   maurice-denis     0.0000    0.0000    0.0000        60\n",
      "                  maurice-esteve     0.9231    1.0000    0.9600        60\n",
      "             maurice-prendergast     0.2500    0.2167    0.2321        60\n",
      "      maurice-quentin-de-la-tour     0.1212    0.0667    0.0860        60\n",
      "                 maurice-utrillo     0.1654    0.3500    0.2246        60\n",
      "                    max-beckmann     0.1630    0.2500    0.1974        60\n",
      "                        max-bill     1.0000    1.0000    1.0000        60\n",
      "                       max-ernst     0.2086    0.4833    0.2915        60\n",
      "                    max-kurzweil     0.1529    0.2167    0.1793        60\n",
      "                  max-liebermann     0.0000    0.0000    0.0000        60\n",
      "                   max-pechstein     0.0500    0.0167    0.0250        60\n",
      "                     max-slevogt     0.1875    0.0500    0.0789        60\n",
      "                       max-weber     0.0952    0.0333    0.0494        60\n",
      "                  maxim-vorobiev     0.4103    0.5333    0.4638        60\n",
      "                  maxime-lalanne     0.8125    0.8667    0.8387        60\n",
      "                   maxime-maufra     0.2778    0.0833    0.1282        60\n",
      "                      may-wilson     0.9375    1.0000    0.9677        60\n",
      "              medi-wechsler-dinu     0.1812    0.4833    0.2636        60\n",
      "                  meijer-de-haan     0.6279    0.9000    0.7397        60\n",
      "                     mel-bochner     1.0000    1.0000    1.0000        60\n",
      "                           menez     0.6897    1.0000    0.8163        60\n",
      "            micaela-eleutheriade     0.3043    0.1167    0.1687        60\n",
      "                    michael-bell     0.7407    1.0000    0.8511        60\n",
      "                  michel-carrade     1.0000    1.0000    1.0000        60\n",
      "                 michel-simonidy     0.2973    0.3667    0.3284        60\n",
      "                    michelangelo     0.2353    0.1333    0.1702        60\n",
      "         michelangelo-pistoletto     1.0000    1.0000    1.0000        60\n",
      "                 mihaly-munkacsy     0.1167    0.1167    0.1167        60\n",
      "            mikalojus-ciurlionis     0.1053    0.0333    0.0506        60\n",
      "                 mikhail-lebedev     0.7455    0.6833    0.7130        60\n",
      "                mikhail-nesterov     0.0938    0.0500    0.0652        60\n",
      "                  mikhail-vrubel     0.0000    0.0000    0.0000        60\n",
      "                  miklos-barabas     0.0000    0.0000    0.0000        60\n",
      "                    milton-avery     0.3000    0.3500    0.3231        60\n",
      "                  milton-resnick     0.3273    0.9000    0.4800        60\n",
      "                     mily-possoz     0.3284    0.7333    0.4536        60\n",
      "                   mira-schendel     0.8696    1.0000    0.9302        60\n",
      "                 miriam-schapiro     0.4600    0.3833    0.4182        60\n",
      "                   moise-kisling     0.0000    0.0000    0.0000        60\n",
      "                   morris-graves     0.2115    0.1833    0.1964        60\n",
      "                    morris-louis     0.7045    0.5167    0.5962        60\n",
      "                  mostafa-dashti     0.5714    0.4667    0.5138        60\n",
      "            mstislav-dobuzhinsky     0.0000    0.0000    0.0000        60\n",
      "                mykola-pymonenko     0.2091    0.3833    0.2706        60\n",
      "               mykola-yaroshenko     0.3077    0.2000    0.2424        60\n",
      "                     myron-stout     0.9677    1.0000    0.9836        60\n",
      "                      n.c.-wyeth     0.0920    0.1333    0.1088        60\n",
      "                  nassos-daphnis     0.9836    1.0000    0.9917        60\n",
      "              natalia-dumitresco     1.0000    1.0000    1.0000        60\n",
      "              natalia-goncharova     0.0000    0.0000    0.0000        60\n",
      "                   neil-welliver     0.1667    0.1000    0.1250        60\n",
      "             nicholas-krushenick     0.6522    1.0000    0.7895        60\n",
      "                nicholas-roerich     0.0000    0.0000    0.0000        60\n",
      "                 nicolae-darascu     0.0741    0.0667    0.0702        60\n",
      "              nicolae-grigorescu     0.0000    0.0000    0.0000        60\n",
      "                 nicolae-tonitza     0.1667    0.1500    0.1579        60\n",
      "                 nicolae-vermont     0.0000    0.0000    0.0000        60\n",
      "                nicolas-tournier     0.9677    1.0000    0.9836        60\n",
      "           niki-de-sainte-phalle     0.7671    0.9333    0.8421        60\n",
      "                  niko-pirosmani     0.1556    0.1167    0.1333        60\n",
      "                    nikola-tanev     0.5875    0.7833    0.6714        60\n",
      "                      nikolai-ge     0.1250    0.0333    0.0526        60\n",
      "                  nikolaos-gyzis     0.1579    0.1000    0.1224        60\n",
      "                 nikolaos-lytras     0.0508    0.0500    0.0504        60\n",
      "         nikolay-bogdanov-belsky     0.5000    0.0167    0.0323        60\n",
      "      nikos-hadjikyriakos-ghikas     0.3594    0.7667    0.4894        60\n",
      "                    norman-bluhm     0.5825    1.0000    0.7362        60\n",
      "                    nutzi-acontz     0.3644    0.7167    0.4831        60\n",
      "                     nzante-spee     0.7833    0.7833    0.7833        60\n",
      "                 octav-angheluta     0.2683    0.3667    0.3099        60\n",
      "                   octav-bancila     0.1064    0.2500    0.1493        60\n",
      "                    odilon-redon     0.1429    0.0667    0.0909        60\n",
      "                     ogata-gekko     0.5152    0.8500    0.6415        60\n",
      "             oleksandr-bogomazov     0.5294    0.9000    0.6667        60\n",
      "             olexandr-archipenko     0.3919    0.4833    0.4328        60\n",
      "                   olga-rozanova     0.2719    0.5167    0.3563        60\n",
      "                   olivier-debre     0.8776    0.7167    0.7890        60\n",
      "                 orest-kiprensky     0.1667    0.1000    0.1250        60\n",
      "                 oscar-dominguez     0.7761    0.8667    0.8189        60\n",
      "                     osias-beert     0.9836    1.0000    0.9917        60\n",
      "                 oskar-kokoschka     0.0000    0.0000    0.0000        60\n",
      "                     osman-hamdi     0.5510    0.9000    0.6835        60\n",
      "                   ossip-zadkine     0.1786    0.0833    0.1136        60\n",
      "              oswaldo-guayasamin     0.5086    0.9833    0.6705        60\n",
      "                    othon-friesz     0.3704    0.1667    0.2299        60\n",
      "                        otto-dix     0.0385    0.0167    0.0233        60\n",
      "                    otto-eckmann     0.4625    0.6167    0.5286        60\n",
      "                 otto-freundlich     1.0000    1.0000    1.0000        60\n",
      "            otto-gustav-carlsund     0.4844    0.5167    0.5000        60\n",
      "                   pablo-picasso     0.0000    0.0000    0.0000        60\n",
      "               panayiotis-tetsis     0.3043    0.1167    0.1687        60\n",
      "                   paolo-scheggi     1.0000    1.0000    1.0000        60\n",
      "                   paolo-uccello     0.1667    0.0667    0.0952        60\n",
      "                  paolo-veronese     0.0000    0.0000    0.0000        60\n",
      "                     park-seo-bo     0.5104    0.8167    0.6282        60\n",
      "                    parmigianino     0.2414    0.1167    0.1573        60\n",
      "                      pat-lipsky     0.9836    1.0000    0.9917        60\n",
      "               patrick-caulfield     0.3409    0.2500    0.2885        60\n",
      "                   patrick-heron     1.0000    1.0000    1.0000        60\n",
      "                patrick-procktor     0.3415    0.2333    0.2772        60\n",
      "                      paul-brach     0.7059    1.0000    0.8276        60\n",
      "                       paul-bril     0.1863    0.3167    0.2346        60\n",
      "                    paul-cezanne     0.0000    0.0000    0.0000        60\n",
      "                  paul-delaroche     0.3070    0.5833    0.4023        60\n",
      "                    paul-delvaux     0.3153    0.5833    0.4094        60\n",
      "                     paul-feeley     0.7500    0.1500    0.2500        60\n",
      "                    paul-gauguin     0.0000    0.0000    0.0000        60\n",
      "                    paul-jenkins     0.4833    0.4833    0.4833        60\n",
      "                       paul-klee     0.0000    0.0000    0.0000        60\n",
      "               paul-mathiopoulos     0.3118    0.4833    0.3791        60\n",
      "                       paul-reed     0.7353    0.4167    0.5319        60\n",
      "                   paul-serusier     0.2143    0.1000    0.1364        60\n",
      "                     paul-signac     0.2083    0.0833    0.1190        60\n",
      "          paula-modersohn-becker     0.1429    0.0667    0.0909        60\n",
      "                    pauline-boty     0.7015    0.7833    0.7402        60\n",
      "                   pavel-fedotov     0.0000    0.0000    0.0000        60\n",
      "                   pavel-filonov     0.7465    0.8833    0.8092        60\n",
      "                   pavel-svinyin     0.1579    0.1000    0.1224        60\n",
      "            peder-severin-kroyer     0.0000    0.0000    0.0000        60\n",
      "                   pedro-calapez     0.9231    1.0000    0.9600        60\n",
      "               pericles-pantazis     0.1125    0.1500    0.1286        60\n",
      "              periklis-vyzantios     0.2500    0.1333    0.1739        60\n",
      "                      perle-fine     0.1852    0.0833    0.1149        60\n",
      "                     peter-blake     0.7792    1.0000    0.8759        60\n",
      "                      peter-busa     0.8108    1.0000    0.8955        60\n",
      "                       peter-max     0.5556    0.5833    0.5691        60\n",
      "               peter-paul-rubens     0.0000    0.0000    0.0000        60\n",
      "                  peter-phillips     0.6892    0.8500    0.7612        60\n",
      "                   petre-abrudan     0.7059    1.0000    0.8276        60\n",
      "          petro-kholodny-(elder)     0.9524    1.0000    0.9756        60\n",
      "                 petrus-christus     0.1034    0.1500    0.1224        60\n",
      "                philip-de-laszlo     0.5175    0.9833    0.6782        60\n",
      "                   philip-guston     0.2660    0.4167    0.3247        60\n",
      "               philip-pearlstein     0.8955    1.0000    0.9449        60\n",
      "             philip-wilson-steer     0.1818    0.0667    0.0976        60\n",
      "                philippe-halsman     1.0000    1.0000    1.0000        60\n",
      "           piero-della-francesca     0.0000    0.0000    0.0000        60\n",
      "                 piero-di-cosimo     0.1913    0.3667    0.2514        60\n",
      "                   piero-dorazio     0.6000    0.9500    0.7355        60\n",
      "                   piero-manzoni     1.0000    1.0000    1.0000        60\n",
      "               pierre-alechinsky     0.3719    0.7500    0.4972        60\n",
      "           pierre-auguste-renoir     0.0833    0.0333    0.0476        60\n",
      "                  pierre-bonnard     0.1250    0.0167    0.0294        60\n",
      "                    pierre-daura     0.6316    0.8000    0.7059        60\n",
      "       pierre-puvis-de-chavannes     0.1148    0.1167    0.1157        60\n",
      "                 pierre-soulages     0.9184    0.7500    0.8257        60\n",
      "                 pierre-tal-coat     0.7344    0.7833    0.7581        60\n",
      "                   piet-mondrian     0.0000    0.0000    0.0000        60\n",
      "        pieter-bruegel-the-elder     0.1765    0.1000    0.1277        60\n",
      "                 pieter-de-hooch     0.2133    0.2667    0.2370        60\n",
      "                  pieter-wenning     0.2883    0.7833    0.4215        60\n",
      "               pietro-da-cortona     0.3077    0.4000    0.3478        60\n",
      "                   pietro-longhi     0.3118    0.4833    0.3791        60\n",
      "                 pietro-perugino     0.0000    0.0000    0.0000        60\n",
      "                    pino-pinelli     0.9000    0.7500    0.8182        60\n",
      "                    pinturicchio     0.3590    0.4667    0.4058        60\n",
      "                       pisanello     0.3922    0.6667    0.4938        60\n",
      "            polychronis-lembesis     0.3077    0.2000    0.2424        60\n",
      "       princess-fahrelnissa-zeid     0.8571    0.9000    0.8780        60\n",
      "              pyotr-konchalovsky     0.0000    0.0000    0.0000        60\n",
      "                     r.-b.-kitaj     0.4831    0.7167    0.5772        60\n",
      "                  radi-nedelchev     0.6235    0.8833    0.7310        60\n",
      "                     rafa-nasiri     0.6667    0.6333    0.6496        60\n",
      "                 rafael-zabaleta     0.1897    0.1833    0.1864        60\n",
      "                    ralph-goings     0.9524    1.0000    0.9756        60\n",
      "                    ralph-hotere     0.3514    0.4333    0.3881        60\n",
      "                 ralph-rosenborg     0.5238    0.7333    0.6111        60\n",
      "                    ramon-oviedo     0.1569    0.2667    0.1975        60\n",
      "                      raoul-dufy     0.1111    0.0167    0.0290        60\n",
      "                      raoul-ubac     0.6800    0.5667    0.6182        60\n",
      "                         raphael     0.0000    0.0000    0.0000        60\n",
      "                raphael-kirchner     0.0625    0.0167    0.0263        60\n",
      "                      ray-parker     0.5114    0.7500    0.6081        60\n",
      "                      red-grooms     0.5500    0.7333    0.6286        60\n",
      "                       rembrandt     0.0556    0.0167    0.0256        60\n",
      "                   remedios-varo     1.0000    1.0000    1.0000        60\n",
      "                   rene-bertholo     0.1803    0.1833    0.1818        60\n",
      "                   rene-magritte     0.5600    0.7000    0.6222        60\n",
      "             richard-artschwager     0.6265    0.8667    0.7273        60\n",
      "              richard-diebenkorn     0.2800    0.2333    0.2545        60\n",
      "                  richard-gerstl     0.0000    0.0000    0.0000        60\n",
      "                richard-hamilton     0.7143    0.1667    0.2703        60\n",
      "        richard-parkes-bonington     0.2308    0.2500    0.2400        60\n",
      "           richard-pousette-dart     0.2771    0.3833    0.3217        60\n",
      "                   richard-serra     0.6575    0.8000    0.7218        60\n",
      "                  richard-tuttle     0.3182    0.3500    0.3333        60\n",
      "                 richard-whitney     0.1304    0.1000    0.1132        60\n",
      "                     rik-wouters     0.2899    0.3333    0.3101        60\n",
      "                 robert-brackman     0.2549    0.2167    0.2342        60\n",
      "                   robert-campin     0.1468    0.2667    0.1893        60\n",
      "                 robert-delaunay     0.9677    1.0000    0.9836        60\n",
      "                robert-goodnough     0.3036    0.2833    0.2931        60\n",
      "                  robert-indiana     0.7647    0.6500    0.7027        60\n",
      "         robert-julian-onderdonk     0.3889    0.1167    0.1795        60\n",
      "                  robert-mangold     0.4091    0.3000    0.3462        60\n",
      "                   robert-morris     0.9677    1.0000    0.9836        60\n",
      "                   robert-nickle     0.5769    1.0000    0.7317        60\n",
      "                    robert-ryman     0.1959    0.3167    0.2420        60\n",
      "                  robert-silvers     0.3111    0.4667    0.3733        60\n",
      "                 robert-smithson     0.9524    1.0000    0.9756        60\n",
      "                   roberto-matta     1.0000    1.0000    1.0000        60\n",
      "                   rodolfo-arico     0.5806    0.3000    0.3956        60\n",
      "            roger-de-la-fresnaye     0.3200    0.1333    0.1882        60\n",
      "                       roger-fry     0.3659    0.2500    0.2970        60\n",
      "           rogier-van-der-weyden     0.0556    0.0167    0.0256        60\n",
      "                    roman-opalka     1.0000    1.0000    1.0000        60\n",
      "                  romare-bearden     0.7059    1.0000    0.8276        60\n",
      "                     ron-gorchov     0.8000    1.0000    0.8889        60\n",
      "                    ronald-davis     0.9643    0.9000    0.9310        60\n",
      "                       roni-horn     1.0000    0.8500    0.9189        60\n",
      "                ronnie-landfield     0.1000    0.0500    0.0667        60\n",
      "                 rosalyn-drexler     0.6897    0.6667    0.6780        60\n",
      "                rosso-fiorentino     0.4107    0.3833    0.3966        60\n",
      "                roy-lichtenstein     0.4667    0.1167    0.1867        60\n",
      "       rudolf-schweitzer-cumpana     0.1196    0.1833    0.1447        60\n",
      "                  rudolf-von-alt     0.3421    0.2167    0.2653        60\n",
      "                   rufino-tamayo     0.6585    0.9000    0.7606        60\n",
      "                    ruth-vollmer     0.5652    0.4333    0.4906        60\n",
      "                     sa-nogueira     0.8451    1.0000    0.9160        60\n",
      "                   salvador-dali     0.0000    0.0000    0.0000        60\n",
      "                     sam-francis     0.1333    0.0667    0.0889        60\n",
      "                     sam-gilliam     0.3830    0.3000    0.3364        60\n",
      "                  samuel-mutzner     0.0000    0.0000    0.0000        60\n",
      "               sandro-botticelli     0.0000    0.0000    0.0000        60\n",
      "                santiago-rusinol     0.1964    0.1833    0.1897        60\n",
      "                  saul-steinberg     0.3333    0.1333    0.1905        60\n",
      "                     sean-scully     0.7792    1.0000    0.8759        60\n",
      "               sebastien-bourdon     0.4045    0.6000    0.4832        60\n",
      "                 seraphine-louis     0.8696    1.0000    0.9302        60\n",
      "                serge-charchoune     0.9677    1.0000    0.9836        60\n",
      "                  serge-sudeikin     0.0000    0.0000    0.0000        60\n",
      "                  sergey-solomko     0.2000    0.0667    0.1000        60\n",
      "                    sever-burada     0.6395    0.9167    0.7534        60\n",
      "                    sidney-nolan     0.8500    0.8500    0.8500        60\n",
      "        sir-lawrence-alma-tadema     0.0000    0.0000    0.0000        60\n",
      "            sofonisba-anguissola     0.2917    0.2333    0.2593        60\n",
      "                      sol-lewitt     0.3500    0.3500    0.3500        60\n",
      "                  sonia-delaunay     0.9524    1.0000    0.9756        60\n",
      "                  sonya-rapoport     1.0000    1.0000    1.0000        60\n",
      "                 sorin-ilfoveanu     1.0000    1.0000    1.0000        60\n",
      "               spyros-papaloukas     0.1111    0.1333    0.1212        60\n",
      "                  stanley-pinker     0.4118    0.7000    0.5185        60\n",
      "               stefan-dimitrescu     0.0794    0.0833    0.0813        60\n",
      "                  stefan-luchian     0.1111    0.0167    0.0290        60\n",
      "                  stefan-popescu     0.2647    0.1500    0.1915        60\n",
      "                    stuart-davis     0.4194    0.6500    0.5098        60\n",
      "                 suzanne-valadon     0.0465    0.0333    0.0388        60\n",
      "                      sven-lukin     0.9464    0.8833    0.9138        60\n",
      "                    t.-c.-steele     0.1136    0.0833    0.0962        60\n",
      "                takashi-murakami     1.0000    1.0000    1.0000        60\n",
      "                      tano-festa     0.3621    0.3500    0.3559        60\n",
      "                taras-shevchenko     0.3333    0.4833    0.3946        60\n",
      "                   taro-yamamoto     0.7015    0.7833    0.7402        60\n",
      "               tarsila-do-amaral     0.2969    0.3167    0.3065        60\n",
      "                     terry-frost     0.8148    0.7333    0.7719        60\n",
      "            thalia-flora-karavia     0.1899    0.2500    0.2158        60\n",
      "               theo-van-doesburg     0.0000    0.0000    0.0000        60\n",
      "           theo-van-rysselberghe     0.0000    0.0000    0.0000        60\n",
      "            theodoor-van-thulden     0.1481    0.1333    0.1404        60\n",
      "                    theodor-aman     0.3750    0.5000    0.4286        60\n",
      "                 theodor-pallady     0.0000    0.0000    0.0000        60\n",
      "       theodor-severin-kittelsen     0.0714    0.0333    0.0455        60\n",
      "             theodore-chasseriau     0.1818    0.2000    0.1905        60\n",
      "              theodore-gericault     0.0625    0.0167    0.0263        60\n",
      "               theodore-rousseau     0.0561    0.1000    0.0719        60\n",
      "                theodoros-stamos     0.2069    0.1000    0.1348        60\n",
      "    theophrastos-triantafyllidis     0.0656    0.0667    0.0661        60\n",
      "                     thomas-cole     0.2400    0.2000    0.2182        60\n",
      "                  thomas-downing     1.0000    1.0000    1.0000        60\n",
      "                   thomas-eakins     0.0000    0.0000    0.0000        60\n",
      "             thomas-gainsborough     0.0455    0.0333    0.0385        60\n",
      "                  thomas-kinkade     0.8955    1.0000    0.9449        60\n",
      "                    thomas-moran     0.1970    0.2167    0.2063        60\n",
      "            thomas-theodor-heine     0.2636    0.4833    0.3412        60\n",
      "                       tia-peltz     0.0698    0.0500    0.0583        60\n",
      "                      tintoretto     0.5000    0.0500    0.0909        60\n",
      "                          titian     0.0000    0.0000    0.0000        60\n",
      "       tivadar-kosztka-csontvary     0.3556    0.2667    0.3048        60\n",
      "                     tom-thomson     0.3284    0.3667    0.3465        60\n",
      "                      tony-smith     0.5890    0.7167    0.6466        60\n",
      "                           toyen     0.3048    0.5333    0.3879        60\n",
      "               tsuguharu-foujita     0.1340    0.2167    0.1656        60\n",
      "             tsukioka-yoshitoshi     0.2927    0.2000    0.2376        60\n",
      "                tsuruko-yamazaki     0.8219    1.0000    0.9023        60\n",
      "                umberto-boccioni     0.1579    0.0500    0.0759        60\n",
      "                utagawa-kunisada     0.1163    0.0833    0.0971        60\n",
      "             utagawa-kunisada-ii     0.9231    1.0000    0.9600        60\n",
      "               utagawa-kuniyoshi     0.1429    0.0333    0.0541        60\n",
      "                utagawa-sadatora     0.6974    0.8833    0.7794        60\n",
      "                utagawa-toyokuni     0.5238    0.7333    0.6111        60\n",
      "             utagawa-toyokuni-ii     0.5204    0.8500    0.6456        60\n",
      "                     vajda-lajos     0.3725    0.9500    0.5352        60\n",
      "                  valentin-serov     0.0000    0.0000    0.0000        60\n",
      "                   valerio-adami     0.8219    1.0000    0.9023        60\n",
      "                    vanessa-bell     0.2597    0.3333    0.2920        60\n",
      "                vangel-naumovski     1.0000    1.0000    1.0000        60\n",
      "                  vasile-dobrian     0.2632    0.3333    0.2941        60\n",
      "                    vasile-kazar     0.5176    0.7333    0.6069        60\n",
      "                  vasile-popescu     0.1921    0.5667    0.2869        60\n",
      "                    vasily-perov     0.0000    0.0000    0.0000        60\n",
      "                  vasily-polenov     0.0000    0.0000    0.0000        60\n",
      "               vasily-sadovnikov     0.4286    0.4500    0.4390        60\n",
      "                  vasily-surikov     0.0000    0.0000    0.0000        60\n",
      "                 vasily-tropinin     0.2500    0.0500    0.0833        60\n",
      "             vasily-vereshchagin     0.2000    0.0167    0.0308        60\n",
      "                    vela-zanetti     0.9524    1.0000    0.9756        60\n",
      "               vicente-manansala     0.4462    0.4833    0.4640        60\n",
      "          victor-borisov-musatov     0.2174    0.0833    0.1205        60\n",
      "                  victor-brauner     0.4724    1.0000    0.6417        60\n",
      "                     victor-hugo     0.2462    0.2667    0.2560        60\n",
      "                victor-meirelles     0.7895    1.0000    0.8824        60\n",
      "                  victor-pasmore     0.4762    0.1667    0.2469        60\n",
      "                vieira-portuense     0.6897    1.0000    0.8163        60\n",
      "                viktor-vasnetsov     0.0000    0.0000    0.0000        60\n",
      "                vilmos-aba-novak     0.3504    0.8000    0.4873        60\n",
      "                vincent-van-gogh     0.0000    0.0000    0.0000        60\n",
      "                viorel-marginean     0.2459    0.2500    0.2479        60\n",
      "               vittore-carpaccio     0.1111    0.0500    0.0690        60\n",
      "           vladimir-borovikovsky     0.1613    0.3333    0.2174        60\n",
      "               vladimir-dimitrov     0.5000    0.5667    0.5312        60\n",
      "               vladimir-makovsky     0.0000    0.0000    0.0000        60\n",
      "                 vladimir-tatlin     0.7229    1.0000    0.8392        60\n",
      "              volodymyr-orlovsky     0.3182    0.1167    0.1707        60\n",
      "                    walasse-ting     0.6585    0.4500    0.5347        60\n",
      "                  walter-battiss     0.5000    0.0167    0.0323        60\n",
      "            walter-darby-bannard     0.4896    0.7833    0.6026        60\n",
      "                 walter-de-maria     0.8955    1.0000    0.9449        60\n",
      "                  walter-sickert     0.0000    0.0000    0.0000        60\n",
      "               wassily-kandinsky     0.1481    0.0667    0.0920        60\n",
      "                     wifredo-lam     0.6593    1.0000    0.7947        60\n",
      "             wilhelm-kotarbinski     0.0000    0.0000    0.0000        60\n",
      "                   wilhelm-leibl     0.2336    0.5333    0.3249        60\n",
      "                 willard-metcalf     0.1143    0.0667    0.0842        60\n",
      "               willem-de-kooning     0.0769    0.0167    0.0274        60\n",
      "                     willem-kalf     0.3866    0.7667    0.5140        60\n",
      "                willi-baumeister     0.8525    0.8667    0.8595        60\n",
      "      william-adolphe-bouguereau     0.2778    0.2500    0.2632        60\n",
      "                william-baziotes     0.3182    0.7000    0.4375        60\n",
      "                   william-blake     0.1594    0.1833    0.1705        60\n",
      "                 william-congdon     0.6250    1.0000    0.7692        60\n",
      "              william-h.-johnson     0.2414    0.1167    0.1573        60\n",
      "                 william-hogarth     0.3158    0.1000    0.1519        60\n",
      "             william-holman-hunt     0.0000    0.0000    0.0000        60\n",
      "          william-james-glackens     0.1220    0.0833    0.0990        60\n",
      "           william-merritt-chase     0.1000    0.0333    0.0500        60\n",
      "                   william-scott     0.5667    0.5667    0.5667        60\n",
      "                  william-shayer     0.4079    0.5167    0.4559        60\n",
      "                  william-turner     0.2083    0.0833    0.1190        60\n",
      "                   winslow-homer     0.0465    0.0333    0.0388        60\n",
      "               winston-churchill     0.2414    0.2333    0.2373        60\n",
      "                 wolfgang-paalen     0.9836    1.0000    0.9917        60\n",
      "                    wu-guanzhong     0.2644    0.3833    0.3129        60\n",
      "                      xu-beihong     0.9836    1.0000    0.9917        60\n",
      "                    yayoi-kusama     0.8088    0.9167    0.8594        60\n",
      "                 yiannis-moralis     0.2195    0.3000    0.2535        60\n",
      "              yiannis-tsaroychis     0.1333    0.0333    0.0533        60\n",
      "                yov-kondzelevych     0.4478    1.0000    0.6186        60\n",
      "                    yves-gaucher     0.4898    0.4000    0.4404        60\n",
      "                      yves-klein     0.4273    0.7833    0.5529        60\n",
      "            zinaida-serebriakova     0.0698    0.0500    0.0583        60\n",
      "\n",
      "                        accuracy                         0.4550     66240\n",
      "                       macro avg     0.4085    0.4550    0.4113     66240\n",
      "                    weighted avg     0.4085    0.4550    0.4113     66240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Precision: 0.4085\n",
      "🔄 Recall: 0.4550\n",
      "🔥 F1-score: 0.4113\n"
     ]
    }
   ],
   "source": [
    "# y_test는 실제 정답 (원-핫 인코딩된 경우 argmax로 변환)\n",
    "y_test_labels = test_y\n",
    "y_pred_labels = mlp_pred_y  # 모델 예측값도 동일하게 변환\n",
    "\n",
    "# ✅ 상세한 성능 지표 출력\n",
    "print(\"🔹 Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels, digits=4))\n",
    "\n",
    "# ✅ 개별 지표 계산 (매크로 평균 사용)\n",
    "precision = precision_score(y_test_labels, y_pred_labels, average='macro')\n",
    "recall = recall_score(y_test_labels, y_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average='macro')\n",
    "\n",
    "print(f\"🎯 Precision: {precision:.4f}\")\n",
    "print(f\"🔄 Recall: {recall:.4f}\")\n",
    "print(f\"🔥 F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0726bda-2ec0-4b65-a393-3d2fcb099f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40160019696647176\n",
      "0.4113096079426252\n",
      "0.43268640462063523\n"
     ]
    }
   ],
   "source": [
    "# fbeta score의 beta=1 : f1 score\n",
    "# fbeta score의 2>=beta>1 : recall의 가중치가 높게 조정된 f1 score\n",
    "# fbeta score의 0<=beta<1 : precision의 가중치가 높게 조정된 f1 score\n",
    "print(fbeta_score(test_y, mlp_pred_y, beta=0.5, average='macro'))\n",
    "print(fbeta_score(test_y, mlp_pred_y, beta=1, average='macro'))\n",
    "print(fbeta_score(test_y, mlp_pred_y, beta=2, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6176a-4ade-4790-bf9a-77df5523911a",
   "metadata": {},
   "source": [
    "# 군집분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc7e5bfd-5c7d-4fc0-95c3-de76c8907f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d05be",
   "metadata": {},
   "source": [
    "## KMeans 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95c224fc-7de0-4298-b4d8-3d1101f980fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c260d9d6-2d9e-4e16-9784-509b153a7c61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=1104, n_init=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=1104, n_init=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(init='random', n_clusters=1104, n_init=5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model = KMeans(n_clusters=len(set(y)),  # 클러스터 갯수:2개의 그룹으로 나눔\n",
    "               init='random', # random(중심초기점이 random), k-means++(멀리 떨어진 초기점)\n",
    "               n_init=5,\n",
    "               max_iter=300)  # 300번 중심점 이동\n",
    "kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94a33692-cf32-4aaa-949d-da5fa0cbc195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56.548378  ,   3.9741023 ,  -3.7660558 , ...,  11.927933  ,\n",
       "         -3.3473277 ,  -6.7931695 ],\n",
       "       [ 33.657715  ,  13.974932  , -28.182302  , ...,  -4.753823  ,\n",
       "         -2.0227087 ,   2.9349484 ],\n",
       "       [ 63.113914  ,  13.542155  ,   1.168807  , ...,  -0.24177289,\n",
       "         -0.3052293 ,  -1.807678  ],\n",
       "       ...,\n",
       "       [-30.363344  ,   6.509787  , -10.315461  , ...,  -0.7998073 ,\n",
       "          0.50404805,   1.0504757 ],\n",
       "       [-18.69838   , -16.232683  , -28.903456  , ...,  -1.2520957 ,\n",
       "          3.0923548 ,   0.6265442 ],\n",
       "       [-20.526297  ,  39.26541   ,  11.346867  , ...,   2.201813  ,\n",
       "         -6.982558  ,   1.6055441 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.cluster_centers_  # 최종 두 클러스터의 중심점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d0301a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans_model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b0d6894-9c16-4050-8a0d-d12a4b138a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 예 측 값 :  [373 373 211 ... 376 591 168]\n",
      "modelLabel:  [373 373 211 ... 376 591 168]\n",
      " 실제 y값 :  ['a.y.-jackson' 'a.y.-jackson' 'a.y.-jackson' ... 'zinaida-serebriakova'\n",
      " 'zinaida-serebriakova' 'zinaida-serebriakova']\n"
     ]
    }
   ],
   "source": [
    "pred = kmeans_model.predict(X)\n",
    "print(' 예 측 값 : ', pred)\n",
    "print('modelLabel: ', kmeans_model.labels_)\n",
    "print(' 실제 y값 : ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d58d7b1f-512e-4907-afa3-0e72f54ff34e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>k-means값</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1094</th>\n",
       "      <th>1095</th>\n",
       "      <th>1096</th>\n",
       "      <th>1097</th>\n",
       "      <th>1098</th>\n",
       "      <th>1099</th>\n",
       "      <th>1100</th>\n",
       "      <th>1101</th>\n",
       "      <th>1102</th>\n",
       "      <th>1103</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>실제값</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-siskind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abidin-dino</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abraham-manievich</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiannis-tsaroychis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-gaucher</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-klein</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zinaida-serebriakova</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 1104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "k-means값               0     1     2     3     4     5     6     7     8     \\\n",
       "실제값                                                                           \n",
       "a.y.-jackson              0     0     0     0     0     0     0     0     0   \n",
       "aaron-siskind             0     0     0     0     0    18    13     0     0   \n",
       "abdullah-suriosubroto     0     0     0     0     0     0     0     0     0   \n",
       "abidin-dino               0     0     0     0     0     0     0     0     0   \n",
       "abraham-manievich         0     0     0     0     0     0     0     0     0   \n",
       "...                     ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "yiannis-tsaroychis        0     0     0     0     0     0     0     0     2   \n",
       "yov-kondzelevych          0     0     0     0     0     0     0     0     0   \n",
       "yves-gaucher              0     0     0     0     0     0     0     0     0   \n",
       "yves-klein                0     0     0     0     0     0     0     0     0   \n",
       "zinaida-serebriakova      0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "k-means값               9     ...  1094  1095  1096  1097  1098  1099  1100  \\\n",
       "실제값                          ...                                             \n",
       "a.y.-jackson              0  ...     0     0     0     8     0     0     0   \n",
       "aaron-siskind             0  ...     9     0     0     0     0    14     0   \n",
       "abdullah-suriosubroto     0  ...     0     0     0     0     0     0     0   \n",
       "abidin-dino               0  ...     0     0     0     0     0     0     0   \n",
       "abraham-manievich         0  ...     0     0     0     0     0     0     0   \n",
       "...                     ...  ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "yiannis-tsaroychis        0  ...     0     0     0     0     0     0     0   \n",
       "yov-kondzelevych          0  ...     0     0     0     0     0     0     0   \n",
       "yves-gaucher              0  ...     0     0     0     0     0     0     0   \n",
       "yves-klein                0  ...     0     0     0     0     0     0     0   \n",
       "zinaida-serebriakova      0  ...     0     0     0     0     0     0     0   \n",
       "\n",
       "k-means값               1101  1102  1103  \n",
       "실제값                                      \n",
       "a.y.-jackson              0     0     0  \n",
       "aaron-siskind             0     0     0  \n",
       "abdullah-suriosubroto     0     0     0  \n",
       "abidin-dino               0     0     0  \n",
       "abraham-manievich         0     0     0  \n",
       "...                     ...   ...   ...  \n",
       "yiannis-tsaroychis        0     0     0  \n",
       "yov-kondzelevych          0     0     0  \n",
       "yves-gaucher              0     0     0  \n",
       "yves-klein                0     0     0  \n",
       "zinaida-serebriakova      0     6     0  \n",
       "\n",
       "[1104 rows x 1104 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y, kmeans_model.labels_, rownames=['실제값'], colnames=['k-means값'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e645e1-6fd6-494b-b31a-496e31678d95",
   "metadata": {},
   "source": [
    "## 군집모형 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14af976d-bc80-471a-b465-3e4673dfb643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # 문자열 라벨을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f831045a-252e-49a7-91e8-3336c2388c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fd5e5c2-68cf-48e3-a1af-a9de00c5e664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=1104, n_init=10, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=1104, n_init=10, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=1104, n_init=10, random_state=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KMeans(n_clusters=len(le.classes_), random_state=1, n_init=10)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f4206b-d97a-4900-a4bd-7d5ebf35e2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X)\n",
    "all(pred == model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "884ffd73-b76d-4568-80de-0d874193f435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>예측값</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>...</th>\n",
       "      <th>1078</th>\n",
       "      <th>1079</th>\n",
       "      <th>1080</th>\n",
       "      <th>1082</th>\n",
       "      <th>1090</th>\n",
       "      <th>1094</th>\n",
       "      <th>1096</th>\n",
       "      <th>1097</th>\n",
       "      <th>1100</th>\n",
       "      <th>1102</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>실제값</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "예측값   0     2     4     7     8     13    15    16    19    20    ...  1078  \\\n",
       "실제값                                                               ...         \n",
       "0       68     0     0     5     0     0     0     0     0     0  ...     0   \n",
       "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2        0    28     0    27     0     0     0     0     0     0  ...     0   \n",
       "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4        0     0    19     0     0     0     0     0     0     0  ...     0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "1099     4     2     0     0     0     0     0     0     0     0  ...     0   \n",
       "1100     0     0     0     0    14     0     0     0     0     0  ...     0   \n",
       "1101     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1102     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1103     0     1     0     1     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "예측값   1079  1080  1082  1090  1094  1096  1097  1100  1102  \n",
       "실제값                                                         \n",
       "0        0     0     0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     4     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "1099     2     0     0     0     0     0     0     0     0  \n",
       "1100     0     0    14     0     0     0     0    21     0  \n",
       "1101     0     0     0     0     0     0     0     0     0  \n",
       "1102     0     0     0     0     0     0     0     0    36  \n",
       "1103     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[1104 rows x 564 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측된 클러스터 라벨\n",
    "pred = model.labels_\n",
    "\n",
    "# 각 클러스터에서 가장 많이 등장하는 실제 라벨 찾기\n",
    "mapping = {}\n",
    "for cluster in range(len(le.classes_)):\n",
    "    mask = (pred == cluster)\n",
    "    if np.any(mask):  # 해당 클러스터에 속한 샘플이 존재하는 경우만 실행\n",
    "        most_common_label = np.bincount(y_encoded[mask]).argmax()\n",
    "        mapping[cluster] = most_common_label\n",
    "\n",
    "# 클러스터 인덱스를 실제 라벨과 매핑\n",
    "adjusted_pred = np.vectorize(mapping.get)(pred)\n",
    "\n",
    "# 교차표 생성\n",
    "cross_tab = pd.crosstab(y_encoded, adjusted_pred, rownames=['실제값'], colnames=['예측값'])\n",
    "\n",
    "# 결과 출력\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15ff03eb-85c8-4392-a427-3656e3428d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <th>abraham-manievich</th>\n",
       "      <th>adnan-coker</th>\n",
       "      <th>adolf-fleischmann</th>\n",
       "      <th>adriaen-van-de-venne</th>\n",
       "      <th>aelbert-cuyp</th>\n",
       "      <th>afro</th>\n",
       "      <th>agostino-carracci</th>\n",
       "      <th>aki-kuroda</th>\n",
       "      <th>...</th>\n",
       "      <th>willem-kalf</th>\n",
       "      <th>willi-baumeister</th>\n",
       "      <th>william-adolphe-bouguereau</th>\n",
       "      <th>william-blake</th>\n",
       "      <th>william-shayer</th>\n",
       "      <th>wolfgang-paalen</th>\n",
       "      <th>xu-beihong</th>\n",
       "      <th>yayoi-kusama</th>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <th>yves-klein</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.y.-jackson</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron-siskind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdullah-suriosubroto</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abidin-dino</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abraham-manievich</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiannis-tsaroychis</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yov-kondzelevych</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-gaucher</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yves-klein</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zinaida-serebriakova</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                  a.y.-jackson  abdullah-suriosubroto  abraham-manievich  \\\n",
       "row_0                                                                           \n",
       "a.y.-jackson                     68                      0                  0   \n",
       "aaron-siskind                     0                      0                  0   \n",
       "abdullah-suriosubroto             0                     28                  0   \n",
       "abidin-dino                       0                      0                  0   \n",
       "abraham-manievich                 0                      0                 19   \n",
       "...                             ...                    ...                ...   \n",
       "yiannis-tsaroychis                4                      2                  0   \n",
       "yov-kondzelevych                  0                      0                  0   \n",
       "yves-gaucher                      0                      0                  0   \n",
       "yves-klein                        0                      0                  0   \n",
       "zinaida-serebriakova              0                      1                  0   \n",
       "\n",
       "col_0                  adnan-coker  adolf-fleischmann  adriaen-van-de-venne  \\\n",
       "row_0                                                                         \n",
       "a.y.-jackson                     5                  0                     0   \n",
       "aaron-siskind                    0                  0                     0   \n",
       "abdullah-suriosubroto           27                  0                     0   \n",
       "abidin-dino                      0                  0                     0   \n",
       "abraham-manievich                0                  0                     0   \n",
       "...                            ...                ...                   ...   \n",
       "yiannis-tsaroychis               0                  0                     0   \n",
       "yov-kondzelevych                 0                 14                     0   \n",
       "yves-gaucher                     0                  0                     0   \n",
       "yves-klein                       0                  0                     0   \n",
       "zinaida-serebriakova             1                  0                     0   \n",
       "\n",
       "col_0                  aelbert-cuyp  afro  agostino-carracci  aki-kuroda  ...  \\\n",
       "row_0                                                                     ...   \n",
       "a.y.-jackson                      0     0                  0           0  ...   \n",
       "aaron-siskind                     0     0                  0           0  ...   \n",
       "abdullah-suriosubroto             0     0                  0           0  ...   \n",
       "abidin-dino                       0     0                  0           0  ...   \n",
       "abraham-manievich                 0     0                  0           0  ...   \n",
       "...                             ...   ...                ...         ...  ...   \n",
       "yiannis-tsaroychis                0     0                  0           0  ...   \n",
       "yov-kondzelevych                  0     0                  0           0  ...   \n",
       "yves-gaucher                      0     0                  0           0  ...   \n",
       "yves-klein                        0     0                  0           0  ...   \n",
       "zinaida-serebriakova              0     0                  0           0  ...   \n",
       "\n",
       "col_0                  willem-kalf  willi-baumeister  \\\n",
       "row_0                                                  \n",
       "a.y.-jackson                     0                 0   \n",
       "aaron-siskind                    0                 0   \n",
       "abdullah-suriosubroto            0                 0   \n",
       "abidin-dino                      0                 0   \n",
       "abraham-manievich                0                 0   \n",
       "...                            ...               ...   \n",
       "yiannis-tsaroychis               0                 2   \n",
       "yov-kondzelevych                 0                 0   \n",
       "yves-gaucher                     0                 0   \n",
       "yves-klein                       0                 0   \n",
       "zinaida-serebriakova             0                 0   \n",
       "\n",
       "col_0                  william-adolphe-bouguereau  william-blake  \\\n",
       "row_0                                                              \n",
       "a.y.-jackson                                    0              0   \n",
       "aaron-siskind                                   0              0   \n",
       "abdullah-suriosubroto                           0              0   \n",
       "abidin-dino                                     0              0   \n",
       "abraham-manievich                               0              0   \n",
       "...                                           ...            ...   \n",
       "yiannis-tsaroychis                              0              0   \n",
       "yov-kondzelevych                                0             14   \n",
       "yves-gaucher                                    0              0   \n",
       "yves-klein                                      0              0   \n",
       "zinaida-serebriakova                            0              0   \n",
       "\n",
       "col_0                  william-shayer  wolfgang-paalen  xu-beihong  \\\n",
       "row_0                                                                \n",
       "a.y.-jackson                        0                0           0   \n",
       "aaron-siskind                       0                0           0   \n",
       "abdullah-suriosubroto               0                0           0   \n",
       "abidin-dino                         0                0           0   \n",
       "abraham-manievich                   0                4           0   \n",
       "...                               ...              ...         ...   \n",
       "yiannis-tsaroychis                  0                0           0   \n",
       "yov-kondzelevych                    0                0           0   \n",
       "yves-gaucher                        0                0           0   \n",
       "yves-klein                          0                0           0   \n",
       "zinaida-serebriakova                0                0           0   \n",
       "\n",
       "col_0                  yayoi-kusama  yov-kondzelevych  yves-klein  \n",
       "row_0                                                              \n",
       "a.y.-jackson                      0                 0           0  \n",
       "aaron-siskind                     0                 0           0  \n",
       "abdullah-suriosubroto             0                 0           0  \n",
       "abidin-dino                       0                 0           0  \n",
       "abraham-manievich                 0                 0           0  \n",
       "...                             ...               ...         ...  \n",
       "yiannis-tsaroychis                0                 0           0  \n",
       "yov-kondzelevych                  0                21           0  \n",
       "yves-gaucher                      0                 0           0  \n",
       "yves-klein                        0                 0          36  \n",
       "zinaida-serebriakova              0                 0           0  \n",
       "\n",
       "[1104 rows x 564 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_str = le.inverse_transform(adjusted_pred)\n",
    "pd.crosstab(y, pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d73892-01c9-4d79-b0dd-8bac65917111",
   "metadata": {},
   "source": [
    "## 조정된 rand지수 외 성능평가 기준 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "723d6e9d-e921-4929-8211-496819ff5d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07772341337665915"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(labels_true=y, labels_pred=pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d80d5ec-cfc8-418e-a0c4-f7765dc34101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07723746587727041"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(labels_true=y_encoded, labels_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "952537e1-4ead-4976-bad4-7320d72cb7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47860058193020094"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneity_score(y, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9243825a-fecf-4bd1-9f52-17313ebc73c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5539896148243851"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness_score(y, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bd4fed4-764d-4732-a7e9-f7e3009d763d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5135430355073454"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(y, pred_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddcc3d86-4e55-40a9-984f-37c67c0c6b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3534084129717696"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_score(y, pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf46f4-e014-43f0-a258-fa296f4bba71",
   "metadata": {},
   "source": [
    "# 앙상블모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c3c03d-c8dc-4df5-9dc0-eb66611e7c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154560, 512), (154560,), (66240, 512), (66240,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab01e51-08da-445d-8d20-61f104baaf30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y)\n",
    "test_y = le.fit_transform(test_y)\n",
    "\n",
    "def model_measure(model, test_X=test_X, test_y=test_y):\n",
    "    pred = model.predict(test_X)\n",
    "    accuracy  = model.score(test_X, test_y)\n",
    "    precision = precision_score(test_y, pred, average=\"macro\")\n",
    "    recall    = recall_score(test_y, pred, average=\"macro\")\n",
    "    f1score  = f1_score(test_y, pred, average=\"macro\")\n",
    "    return '정확도:{:.3f}, 정밀도:{:.3f}, 재현율:{:.3f}, f1_score:{:.3f}'.format(accuracy, precision, recall, f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78617bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\ai\\\\Downloads\\\\Data\\\\label_encoder.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder 객체 저장\n",
    "joblib.dump(le, os.path.join(filepath,'label_encoder.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb556b0-9a37-4324-b4a7-df68c1f44a7d",
   "metadata": {},
   "source": [
    "# 의사결정 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1505c",
   "metadata": {},
   "source": [
    "## 배깅 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366151e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ✅ 경량화된 랜덤포레스트 모델\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50,      # 트리 개수 줄이기\n",
    "    max_depth=10,          # 트리 깊이 제한\n",
    "    max_features='sqrt',  # 최적의 특징 개수 자동 선택\n",
    "    random_state=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(train_X,train_y)\n",
    "\n",
    "print(model_measure(rf_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f195e",
   "metadata": {},
   "source": [
    "## 부스팅 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a6edb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.907, 정밀도:0.902, 재현율:0.907, f1_score:0.901\n"
     ]
    }
   ],
   "source": [
    "# ✅ 경량화된 XGBoost 모델\n",
    "xgb_model = XGBClassifier(\n",
    "    max_depth=10,          # 트리 깊이 제한\n",
    "    n_estimators=50,      # 트리 개수 줄이기\n",
    "    learning_rate=0.1,    # 학습 속도 증가\n",
    "    subsample=0.8,        # 데이터 일부 샘플링\n",
    "    colsample_bytree=0.8, # 일부 특성만 사용\n",
    "    tree_method='hist',   # 히스토그램 기반 트리 (메모리 절약)\n",
    "    eval_metric='logloss',\n",
    "#     use_label_encoder=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(train_X,train_y)\n",
    "\n",
    "print(model_measure(xgb_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cece5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ✅ 경량화된 LightGBM 모델\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=50,      # 트리 개수 줄이기\n",
    "    max_depth=10,          # 트리 깊이 제한\n",
    "    num_leaves=16,        # 리프 개수 줄이기\n",
    "    subsample=0.8,        # 데이터 일부 샘플링\n",
    "    colsample_bytree=0.8, # 일부 특성만 사용\n",
    "    verbose=-1,           # 불필요한 출력 제거\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(train_X,train_y)\n",
    "\n",
    "print(model_measure(lgb_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1b469d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\ai\\\\Downloads\\\\Data\\\\xgb_artist_model.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_model, os.path.join(filepath,'xgb_artist_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45ade609",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_xgb = joblib.load(os.path.join(filepath,'xgb_artist_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6769a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.907, 정밀도:0.902, 재현율:0.907, f1_score:0.901\n"
     ]
    }
   ],
   "source": [
    "print(model_measure(loaded_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207f169-fc79-4be1-8cf3-3bfa1de4c6cc",
   "metadata": {},
   "source": [
    "## 투표를 이용한 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9806c946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.load(os.path.join(filepath,'X.npy'))\n",
    "y = np.load(os.path.join(filepath,'y.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a40f5f-fcad-414a-84ec-b05c960244ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;rfm&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective=&#x27;multi:softprob&#x27;, ...)),\n",
       "                             (&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;rfm&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective=&#x27;multi:softprob&#x27;, ...)),\n",
       "                             (&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rfm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "              n_jobs=-1, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "               num_leaves=16, random_state=42, subsample=0.8, verbose=-1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('rfm',\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric='logloss',\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective='multi:softprob', ...)),\n",
       "                             ('lgb',\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model_hard = VotingClassifier(estimators=[('rfm', rf_model),\n",
    "                                                 ('xgb', xgb_model),\n",
    "                                                 ('lgb', lgb_model)],\n",
    "                                     voting='hard') # voting='hard' 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3434b339-47b2-4d89-bb1d-06d33d0e7ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pablo-picasso'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model_hard.predict(test_X[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841fc6cc-51aa-4cd4-8fd7-9a947a7ad854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;rfm&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective=&#x27;multi:softprob&#x27;, ...)),\n",
       "                             (&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;rfm&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             (&#x27;xgb&#x27;,\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=&#x27;logloss&#x27;,\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective=&#x27;multi:softprob&#x27;, ...)),\n",
       "                             (&#x27;lgb&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rfm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "              n_jobs=-1, num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "               num_leaves=16, random_state=42, subsample=0.8, verbose=-1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('rfm',\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50, n_jobs=-1,\n",
       "                                                     random_state=1)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=0.8, device=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric='logloss',\n",
       "                                            feature_types=None...\n",
       "                                            max_delta_step=None, max_depth=10,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=50, n_jobs=-1,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            objective='multi:softprob', ...)),\n",
       "                             ('lgb',\n",
       "                              LGBMClassifier(colsample_bytree=0.8, max_depth=10,\n",
       "                                             n_estimators=50, n_jobs=-1,\n",
       "                                             num_leaves=16, random_state=42,\n",
       "                                             subsample=0.8, verbose=-1))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voting 알고리즘 - soft 방식\n",
    "voting_model_soft = VotingClassifier(estimators=[('rfm', rf_model),\n",
    "                                                 ('xgb', xgb_model),\n",
    "                                                 ('lgb', lgb_model)],\n",
    "                                     voting='soft') # voting='hard' 기본값\n",
    "voting_model_soft.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb244a7-f824-43de-86f6-3fb2783afa32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pablo-picasso'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model_soft.predict(test_X[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b01d70d-efb9-492f-81e1-e606df4ef047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82c7d79-f993-4565-abc6-79e050c20fc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ramon-oviedo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_test_y = le.inverse_transform(test_y)\n",
    "inverse_test_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47316ee0-c90b-4003-9b77-e24b9989f6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정확도:0.400, 정밀도:0.677, 재현율:0.400, f1_score:0.411'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_measure(voting_model_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "584c7910-5d14-47f9-bd76-bd12cfecd995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정확도:0.102, 정밀도:0.177, 재현율:0.102, f1_score:0.100'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_measure(voting_model_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0832ef9-0450-4b1d-8bfb-3731cbc12c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfm': RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1),\n",
       " 'xgb': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='logloss',\n",
       "               feature_types=None, gamma=None, grow_policy=None,\n",
       "               importance_type=None, interaction_constraints=None,\n",
       "               learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "               max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "               max_leaves=None, min_child_weight=None, missing=nan,\n",
       "               monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "               n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       " 'lgb': LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                num_leaves=16, random_state=42, subsample=0.8, verbose=-1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voting_model_hard의 개별 모델들 딕셔너리 형태로 반환\n",
    "voting_model_hard.named_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d06d40e-4f40-4df4-9dda-a2ad9bd08e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfm': RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1),\n",
       " 'xgb': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='logloss',\n",
       "               feature_types=None, gamma=None, grow_policy=None,\n",
       "               importance_type=None, interaction_constraints=None,\n",
       "               learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "               max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "               max_leaves=None, min_child_weight=None, missing=nan,\n",
       "               monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "               n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       " 'lgb': LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                num_leaves=16, random_state=42, subsample=0.8, verbose=-1)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# voting_model_soft의 개별 모델들 딕셔너리 형태로 반환\n",
    "voting_model_soft.named_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17c4e732-ca5c-48d6-9a9d-14b364ee40c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:0.145, 정밀도:0.380, 재현율:0.145, f1_score:0.132\n",
      "정확도:0.907, 정밀도:0.902, 재현율:0.907, f1_score:0.901\n",
      "정확도:0.042, 정밀도:0.081, 재현율:0.042, f1_score:0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(model_measure(voting_model_hard.named_estimators_['rfm']))  # 단일모델로 개별모델 access 가능\n",
    "print(model_measure(voting_model_hard.named_estimators_['xgb']))\n",
    "print(model_measure(voting_model_hard.named_estimators_['lgb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30785869-c41b-445f-8b50-c3aec4f75ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('rfm',\n",
       "   RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)),\n",
       "  ('xgb',\n",
       "   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bynode=None,\n",
       "                 colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                 enable_categorical=False, eval_metric='logloss',\n",
       "                 feature_types=None, gamma=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "                 n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...)),\n",
       "  ('lgb',\n",
       "   LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                  num_leaves=16, random_state=42, subsample=0.8, verbose=-1))],\n",
       " 'flatten_transform': True,\n",
       " 'n_jobs': None,\n",
       " 'verbose': False,\n",
       " 'voting': 'hard',\n",
       " 'weights': None,\n",
       " 'rfm': RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1),\n",
       " 'xgb': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='logloss',\n",
       "               feature_types=None, gamma=None, grow_policy=None,\n",
       "               importance_type=None, interaction_constraints=None,\n",
       "               learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "               max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "               max_leaves=None, min_child_weight=None, missing=nan,\n",
       "               monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "               n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       " 'lgb': LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                num_leaves=16, random_state=42, subsample=0.8, verbose=-1),\n",
       " 'rfm__bootstrap': True,\n",
       " 'rfm__ccp_alpha': 0.0,\n",
       " 'rfm__class_weight': None,\n",
       " 'rfm__criterion': 'gini',\n",
       " 'rfm__max_depth': 10,\n",
       " 'rfm__max_features': 'sqrt',\n",
       " 'rfm__max_leaf_nodes': None,\n",
       " 'rfm__max_samples': None,\n",
       " 'rfm__min_impurity_decrease': 0.0,\n",
       " 'rfm__min_samples_leaf': 1,\n",
       " 'rfm__min_samples_split': 2,\n",
       " 'rfm__min_weight_fraction_leaf': 0.0,\n",
       " 'rfm__n_estimators': 50,\n",
       " 'rfm__n_jobs': -1,\n",
       " 'rfm__oob_score': False,\n",
       " 'rfm__random_state': 1,\n",
       " 'rfm__verbose': 0,\n",
       " 'rfm__warm_start': False,\n",
       " 'xgb__objective': 'multi:softprob',\n",
       " 'xgb__base_score': None,\n",
       " 'xgb__booster': None,\n",
       " 'xgb__callbacks': None,\n",
       " 'xgb__colsample_bylevel': None,\n",
       " 'xgb__colsample_bynode': None,\n",
       " 'xgb__colsample_bytree': 0.8,\n",
       " 'xgb__device': None,\n",
       " 'xgb__early_stopping_rounds': None,\n",
       " 'xgb__enable_categorical': False,\n",
       " 'xgb__eval_metric': 'logloss',\n",
       " 'xgb__feature_types': None,\n",
       " 'xgb__gamma': None,\n",
       " 'xgb__grow_policy': None,\n",
       " 'xgb__importance_type': None,\n",
       " 'xgb__interaction_constraints': None,\n",
       " 'xgb__learning_rate': 0.1,\n",
       " 'xgb__max_bin': None,\n",
       " 'xgb__max_cat_threshold': None,\n",
       " 'xgb__max_cat_to_onehot': None,\n",
       " 'xgb__max_delta_step': None,\n",
       " 'xgb__max_depth': 10,\n",
       " 'xgb__max_leaves': None,\n",
       " 'xgb__min_child_weight': None,\n",
       " 'xgb__missing': nan,\n",
       " 'xgb__monotone_constraints': None,\n",
       " 'xgb__multi_strategy': None,\n",
       " 'xgb__n_estimators': 50,\n",
       " 'xgb__n_jobs': -1,\n",
       " 'xgb__num_parallel_tree': None,\n",
       " 'xgb__random_state': 42,\n",
       " 'xgb__reg_alpha': None,\n",
       " 'xgb__reg_lambda': None,\n",
       " 'xgb__sampling_method': None,\n",
       " 'xgb__scale_pos_weight': None,\n",
       " 'xgb__subsample': 0.8,\n",
       " 'xgb__tree_method': 'hist',\n",
       " 'xgb__validate_parameters': None,\n",
       " 'xgb__verbosity': None,\n",
       " 'lgb__boosting_type': 'gbdt',\n",
       " 'lgb__class_weight': None,\n",
       " 'lgb__colsample_bytree': 0.8,\n",
       " 'lgb__importance_type': 'split',\n",
       " 'lgb__learning_rate': 0.1,\n",
       " 'lgb__max_depth': 10,\n",
       " 'lgb__min_child_samples': 20,\n",
       " 'lgb__min_child_weight': 0.001,\n",
       " 'lgb__min_split_gain': 0.0,\n",
       " 'lgb__n_estimators': 50,\n",
       " 'lgb__n_jobs': -1,\n",
       " 'lgb__num_leaves': 16,\n",
       " 'lgb__objective': None,\n",
       " 'lgb__random_state': 42,\n",
       " 'lgb__reg_alpha': 0.0,\n",
       " 'lgb__reg_lambda': 0.0,\n",
       " 'lgb__subsample': 0.8,\n",
       " 'lgb__subsample_for_bin': 200000,\n",
       " 'lgb__subsample_freq': 0,\n",
       " 'lgb__verbose': -1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model_hard.get_params()  # 모델 내의 모든 파라미터(하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9fac7ec-5ab5-47d4-b78f-704cb2a98bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('rfm',\n",
       "   RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1)),\n",
       "  ('xgb',\n",
       "   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bynode=None,\n",
       "                 colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                 enable_categorical=False, eval_metric='logloss',\n",
       "                 feature_types=None, gamma=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None,\n",
       "                 learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "                 n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...)),\n",
       "  ('lgb',\n",
       "   LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                  num_leaves=16, random_state=42, subsample=0.8, verbose=-1))],\n",
       " 'flatten_transform': True,\n",
       " 'n_jobs': None,\n",
       " 'verbose': False,\n",
       " 'voting': 'soft',\n",
       " 'weights': None,\n",
       " 'rfm': RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, random_state=1),\n",
       " 'xgb': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='logloss',\n",
       "               feature_types=None, gamma=None, grow_policy=None,\n",
       "               importance_type=None, interaction_constraints=None,\n",
       "               learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "               max_cat_to_onehot=None, max_delta_step=None, max_depth=10,\n",
       "               max_leaves=None, min_child_weight=None, missing=nan,\n",
       "               monotone_constraints=None, multi_strategy=None, n_estimators=50,\n",
       "               n_jobs=-1, num_parallel_tree=None, objective='multi:softprob', ...),\n",
       " 'lgb': LGBMClassifier(colsample_bytree=0.8, max_depth=10, n_estimators=50, n_jobs=-1,\n",
       "                num_leaves=16, random_state=42, subsample=0.8, verbose=-1),\n",
       " 'rfm__bootstrap': True,\n",
       " 'rfm__ccp_alpha': 0.0,\n",
       " 'rfm__class_weight': None,\n",
       " 'rfm__criterion': 'gini',\n",
       " 'rfm__max_depth': 10,\n",
       " 'rfm__max_features': 'sqrt',\n",
       " 'rfm__max_leaf_nodes': None,\n",
       " 'rfm__max_samples': None,\n",
       " 'rfm__min_impurity_decrease': 0.0,\n",
       " 'rfm__min_samples_leaf': 1,\n",
       " 'rfm__min_samples_split': 2,\n",
       " 'rfm__min_weight_fraction_leaf': 0.0,\n",
       " 'rfm__n_estimators': 50,\n",
       " 'rfm__n_jobs': -1,\n",
       " 'rfm__oob_score': False,\n",
       " 'rfm__random_state': 1,\n",
       " 'rfm__verbose': 0,\n",
       " 'rfm__warm_start': False,\n",
       " 'xgb__objective': 'multi:softprob',\n",
       " 'xgb__base_score': None,\n",
       " 'xgb__booster': None,\n",
       " 'xgb__callbacks': None,\n",
       " 'xgb__colsample_bylevel': None,\n",
       " 'xgb__colsample_bynode': None,\n",
       " 'xgb__colsample_bytree': 0.8,\n",
       " 'xgb__device': None,\n",
       " 'xgb__early_stopping_rounds': None,\n",
       " 'xgb__enable_categorical': False,\n",
       " 'xgb__eval_metric': 'logloss',\n",
       " 'xgb__feature_types': None,\n",
       " 'xgb__gamma': None,\n",
       " 'xgb__grow_policy': None,\n",
       " 'xgb__importance_type': None,\n",
       " 'xgb__interaction_constraints': None,\n",
       " 'xgb__learning_rate': 0.1,\n",
       " 'xgb__max_bin': None,\n",
       " 'xgb__max_cat_threshold': None,\n",
       " 'xgb__max_cat_to_onehot': None,\n",
       " 'xgb__max_delta_step': None,\n",
       " 'xgb__max_depth': 10,\n",
       " 'xgb__max_leaves': None,\n",
       " 'xgb__min_child_weight': None,\n",
       " 'xgb__missing': nan,\n",
       " 'xgb__monotone_constraints': None,\n",
       " 'xgb__multi_strategy': None,\n",
       " 'xgb__n_estimators': 50,\n",
       " 'xgb__n_jobs': -1,\n",
       " 'xgb__num_parallel_tree': None,\n",
       " 'xgb__random_state': 42,\n",
       " 'xgb__reg_alpha': None,\n",
       " 'xgb__reg_lambda': None,\n",
       " 'xgb__sampling_method': None,\n",
       " 'xgb__scale_pos_weight': None,\n",
       " 'xgb__subsample': 0.8,\n",
       " 'xgb__tree_method': 'hist',\n",
       " 'xgb__validate_parameters': None,\n",
       " 'xgb__verbosity': None,\n",
       " 'lgb__boosting_type': 'gbdt',\n",
       " 'lgb__class_weight': None,\n",
       " 'lgb__colsample_bytree': 0.8,\n",
       " 'lgb__importance_type': 'split',\n",
       " 'lgb__learning_rate': 0.1,\n",
       " 'lgb__max_depth': 10,\n",
       " 'lgb__min_child_samples': 20,\n",
       " 'lgb__min_child_weight': 0.001,\n",
       " 'lgb__min_split_gain': 0.0,\n",
       " 'lgb__n_estimators': 50,\n",
       " 'lgb__n_jobs': -1,\n",
       " 'lgb__num_leaves': 16,\n",
       " 'lgb__objective': None,\n",
       " 'lgb__random_state': 42,\n",
       " 'lgb__reg_alpha': 0.0,\n",
       " 'lgb__reg_lambda': 0.0,\n",
       " 'lgb__subsample': 0.8,\n",
       " 'lgb__subsample_for_bin': 200000,\n",
       " 'lgb__subsample_freq': 0,\n",
       " 'lgb__verbose': -1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model_soft.get_params()  # 모델 내의 모든 파라미터(하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a1339-4b91-4a71-88e0-1cbbd8d7afbc",
   "metadata": {},
   "source": [
    "## 머신러닝 모형 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d40ffac-9757-436f-bbcf-cf8509c0a2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\ai\\\\Downloads\\\\Data\\\\voting_model_hard.joblib']\n",
      "['E:\\\\ai\\\\Downloads\\\\Data\\\\voting_model_soft.joblib']\n"
     ]
    }
   ],
   "source": [
    "# 대용량 모형일 때 : joblib 파일로 저장 (joblib 라이브러리 사용)\n",
    "print(joblib.dump(voting_model_hard, os.path.join(filepath,'voting_model_hard.joblib')))\n",
    "print(joblib.dump(voting_model_soft, os.path.join(filepath,'voting_model_soft.joblib')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc72f3a4-e2df-43e4-853a-3c4011aece6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziz0n\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정확도:0.400, 정밀도:0.677, 재현율:0.400, f1_score:0.411'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 load\n",
    "loaded_voting_model_hard = joblib.load(os.path.join(filepath,'voting_model_hard.joblib'))\n",
    "model_measure(loaded_voting_model_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4adb69-778e-4e60-9e05-4b31e06d954f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 load\u001b[39;00m\n\u001b[0;32m      2\u001b[0m loaded_voting_model_soft \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(filepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoting_model_soft.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_voting_model_soft\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36mmodel_measure\u001b[1;34m(model, test_X, test_y)\u001b[0m\n\u001b[0;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_X)\n\u001b[0;32m      3\u001b[0m accuracy  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(test_X, test_y)\n\u001b[1;32m----> 4\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m recall    \u001b[38;5;241m=\u001b[39m recall_score(test_y, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m f1score  \u001b[38;5;241m=\u001b[39m f1_score(test_y, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1377\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:117\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "# 모델 load\n",
    "loaded_voting_model_soft = joblib.load(os.path.join(filepath,'voting_model_soft.joblib'))\n",
    "model_measure(loaded_voting_model_soft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182.326px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
